[
["index.html", "Intro to GIS and Spatial Analysis Preface", " Intro to GIS and Spatial Analysis Manuel Gimond Last edited on 2016-10-03 Preface These pages are a compilation of lecture notes for my Introduction to GIS and Spatial Analysis course (ES214). They are ordered in such a way to follow the course outline, but most pages can be read in any desirable order. The course (and this book) is split into two parts: data manipulation &amp; visualization and exploratory spatial data analysis. The first part of this book is usually conducted using ArcGIS Desktop whereas the latter part of the book is conducted in R. ArcGIS was chosen as the GIS data manipulation environment because of its “desirability” in job applications for undergraduates in the Unites States. But other GIS software environments, such as the open source software QGIS, could easily be adopted in lieu of ArcGIS–even R can be used to perform many spatial data manipulations such as clipping, buffering and projecting. Even though some of the chapters of this book make direct reference to ArcGIS techniques, most chapters can be studied without access to the software. The latter part of this book (and the course) make heavy use of R because of a) its broad appeal in the world of data analysis b) its rich (if not richest) array of spatial analysis and spatial statistics packages c) its scripting environment (which facilitates reproducibility) d) and its very cheap cost (it’s completely free and open source!). But R can be used for many traditional “GIS” application that involve most data manipulation operations–the only benefit in using a full-blown GIS environment like ArcGIS or QGIS is in creating/editing spatial data, rendering complex maps and manipulating spatial data. The Appendix covers various aspects of spatial data manipulation and analysis using R. The course only focuses on point pattern analysis and spatial autocorrelation using R, but I’ve added other R resources for students wishing to expand their GIS skills using R. This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. "],
["introGIS.html", "Chapter 1 Introduction to GIS 1.1 What is a GIS? 1.2 What is Spatial Analysis? 1.3 What’s in an Acronym?", " Chapter 1 Introduction to GIS 1.1 What is a GIS? A Geographic Information System is a multi-component environment used to create, manage, visualize and analyze data and its spatial counterpart. It’s important to note that most datasets you will encounter in your lifetime can all be assigned a spatial location whether on the earth’s surface or within some arbitrary coordinate system (such as a soccer field or a gridded petri dish). So in essence, any dataset can be represented in a GIS: the question then becomes “does it need to be analyzed in a GIS environment?” The answer to this question depends on the purpose of the analysis. If, for example, we are interested in identifying the ten African countries with the highest conflict index score for the 1966-78 period, a simple table listing those scores by country is all that is needed. Table 1.1: Index of total African conflict for the 1966-78 period (Anselin and O’Loughlin 1992). Country Conflicts Country Conflicts EGYPT 5246 LIBERIA 980 SUDAN 4751 SENEGAL 933 UGANDA 3134 CHAD 895 ZAIRE 3087 TOGO 848 TANZANIA 2881 GABON 824 LIBYA 2355 MAURITANIA 811 KENYA 2273 ZIMBABWE 795 SOMALIA 2122 MOZAMBIQUE 792 ETHIOPIA 1878 IVORY COAST 758 SOUTH AFRICA 1875 MALAWI 629 MOROCCO 1861 CENTRAL AFRICAN REPUBLIC 618 ZAMBIA 1554 CAMEROON 604 ANGOLA 1528 BURUNDI 604 ALGERIA 1421 RWANDA 487 TUNISIA 1363 SIERRA LEONE 423 BOTSWANA 1266 LESOTHO 363 CONGO 1142 NIGER 358 NIGERIA 1130 BURKINA FASO 347 GHANA 1090 MALI 299 GUINEA 1015 THE GAMBIA 241 BENIN 998 SWAZILAND 147 Data source: Anselin, L. and John O’Loughlin. 1992. Geography of international conflict and cooperation: spatial dependence and regional context in Africa. In The New Geopolitics, ed. M. Ward, pp. 39-75. A simple sort on the Conflict column reveals that EGYPT, SUDAN, UGANDA, ZAIRE, TANZANIA, LIBYA, KENYA, SOMALIA, ETHIOPIA, SOUTH AFRICA are the top ten countries. What if we are interested in knowing whether countries with a high conflict index score are geographically clustered, does the above table provide us with enough information to help answer this question? The answer, of course, is no. We need additional data pertaining to the geographic location and shape of each country. A map of the countries would be helpful. Figure 1.1: Choropleth representation of African conflict index scores. Countries for which a score was not available are not mapped. Maps are ubiquitous: available online and in various print medium. But we seldom ask how the boundaries of the map features are encoded in a computing environment? After all, if we expect software to assist us in the analysis, the spatial elements of our data should be readily accessible in a digital form. Spending a few minutes thinking through this question will make you realize that simple tables or spreadsheets are not up to this task. A more complex data storage mechanism is required. This is the core of a GIS environment: a spatial database that facilitates the storage and retrieval of data that define the spatial boundaries, lines or points of the entities we are studying. This may seem trivial, but without a spatial database, most spatial data exploration and analysis would not be possible! 1.1.1 GIS software Many GIS software applications are available–both commercial and open source. Two popular applications are ArcGIS and QGIS. 1.1.1.1 ArcGIS A popular commercial GIS software is ArcGIS developed by ESRI (ESRI, pronounced ez-ree),was once a small land-use consulting firm which did not start developing GIS software until the mid 1970s. The ArcGIS desktop environment encompasses a suite of applications which include ArcMap, ArcCatalog, ArcScene and ArcGlobe. ArcGIS comes in three different license levels (basic, standard and advanced) and can be purchased with additional add-on packages. As such, a single license can range from a few thousand dollars to well over ten thousand dollars. In addition to software licensing costs, ArcGIS is only available for Windows operating systems; so if your workplace is a Mac only environment, the purchase of a Windows PC would add to the expense. 1.1.2 QGIS A very capable open source (free) GIS software is QGIS. It encompasses most of the functionality included in ArcGIS. If you are looking for a GIS application for your Mac or Linux environment, QGIS is a wonderful choice given its multi-platform support. Built into the current versions of QGIS are functions from another open source software: GRASS. GRASS has been around since the 1980’s and has many advanced GIS data manipulation functions however, its use is not as intuitive as that of QGIS or ArcGIS (hence the preferred QGIS alternative). 1.2 What is Spatial Analysis? A distinction is made in this course between GIS and spatial analysis. In the context of mainstream GIS software, the term analysis refers to data manipulation and basic data description. In the context of spatial analysis, the analysis focuses on the statistical analysis of patterns and underlying processes or more generally, spatial analysis addresses the question “what could have been the genesis of the observed spatial pattern?” It’s an exploratory process whereby we attempt to quantify the observed pattern then explore the processes that may have generated the pattern. For example, you record the location of each tree in a well defined study area. You then map the location of each tree (a GIS task). At this point, you might be inclined to make inferences about the observed pattern. Are the trees clustered or dispersed? Is the tree density constant across the study area? Could soil type or slope have led to the observed pattern? Those are questions that are addressed in spatial analysis using quantitative and statistical techniques. Figure 1.2: Distribution of Maple trees in a 1,000 x 1,000 ft study area. What you will learn in this course is that popular GIS software like ArcGIS are great tools to create and manipulate spatial data, but if one wishes to go beyond the data manipulation and analyze patterns and processes that may have led to these patterns, other quantitative tools are needed. One such tool we will use in this class is R: an open source (freeware) data analysis environment. R has one, if not the richest set of spatial data analysis and statistics tools available today. Learning the R programming environment will prove to be quite beneficial given that many of the operations learnt are transferable across many other (non-spatial) quantitative analysis projects. R can be installed on both Windows and Mac operating systems. Another related piece of software that you might find useful is RStudio which offers a nice interface to R. To learn more about data analysis in R, visit the ES218 course website. 1.3 What’s in an Acronym? Figure 1.3: A highly recommended book for anyone serious about exploring spatial data is O’Sullivan and Unwin’s Geographic Information Analysis (O’Sullivan and Unwin 2010). The book tackles general concepts and theory behind spatial analysis. GIS is a ubiquitous technology. Many of you are taking this course in part because you have seen GIS listed as a “desirable”&quot; or “required” skill in job postings. Many of you will think of GIS as a “map making” environment as do many ancillary users of GIS in the workforce. While “visualizing” data is an important feature of a GIS, one must not lose sight of what data is being visualized and for what purpose. O’Sullivan and Unwin (2010) use the term accidental geographer to refer to those “whose understanding of geographic science is based on the operations made possible by GIS software”. We can expand on this idea and define accidental data analyst as one whose understanding of data and its analysis is limited to the point-and-click environment of popular pieces of software such as spreadsheet environments, statistical packages and GIS software. The aggressive marketing of GIS technology has the undesirable effect of placing the technology before purpose and theory. This is not unique to GIS, however. Such concerns were shared decades ago when personal computers made it easier for researchers and employees to graph non-spatial data as well as perform many statistical procedures. The different purposes of mapping spatial data have strong parallels to that of graphing (or plotting) non-spatial data. John Tukey (Tukey 1972) offers three broad classes of the latter: “Graphs from which numbers are to be read off- substitutes for tables. Graphs intended to show the reader what has already been learned (by some other technique)–these we shall sometimes impolitely call propaganda graphs. Graphs intended to let us see what may be happening over and above what we have already described- these are the analytical graphs that are our main topic.&quot; A GIS world analogy is proposed here: Reference maps (USGS maps, hiking maps, road maps). Such maps are used to navigate landscapes or identify locations of points-of-interest. Presentation maps presented in the press such as the NY Times and the Wall Street Journal, but also maps presented in journals. Such maps are designed to convey a very specific narrative of the author’s choosing. (Here we’ll avoid Tukey’s harsh description of such visual displays, but the idea that maps can be used as propaganda is not farfetched). Statistical maps whose purpose it is to manipulate the raw data in such a way to tease out patterns otherwise not discernable the original data. This usually requires multiple data manipulation operations and visualization and can sometimes benefit being explored outside of a spatial context. This course will focus on the last two spatial data visualization purposes with a strong emphasis on the latter (Statistical maps). References "],
["feature-representation.html", "Chapter 2 Feature Representation 2.1 Vector vs. Raster 2.2 Object vs. Field 2.3 Scale 2.4 Attribute Tables", " Chapter 2 Feature Representation 2.1 Vector vs. Raster To work in a GIS environment, real world observations (objects or events that can be recorded in 2D or 3D space) need to be reduced to spatial entities. These spatial entities can be represented in a GIS as a vector data model or a raster data model. Figure 2.1: Vector and raster representations of a river feature. 2.1.1 Vector Vector features can be decomposed into three different geometric primitives: points, polylines and polygons. 2.1.1.1 Point Figure 2.2: Three point objects defined by their X and Y coordinate values. A point is composed of one coordinate pair representing a specific location in a coordinate system. Points are the most basic geometric primitives having no length or area. By definition a point can’t be “seen” since it has no area; but this is not practical if such primitives are to be mapped. So points on a map are represented using symbols that have both area and shape (e.g. circle, square, plus signs). We seem capable of interpreting such symbols as points, but there may be instances when such interpretation may be ambiguous (e.g. is a round symbol delineating the area of a round feature on the ground such as a large oil storage tank or is it representing the point location of that tank?). 2.1.1.2 Polyline Figure 1.2: A simple polyline object defined by connected vertices. A polyline is composed of a sequence of two or more coordinate pairs called vertices. A vertex is defined by coordinate pairs just like a point, but what differentiates a vertex from a point is its explicitly defined relationship with neighboring vertices. A vertex is connected to at least one other vertex. Like a point, a true line can’t be seen since it has no area. And like a point, a line is symbolized using shapes that have a color, width and style (e.g. solid, dashed, dotted, etc…). Roads and rivers are commonly stored as polylines in a GIS. Like a point, a true line can’t be seen since it has no area. And like a point, a line is symbolized using shapes that have a color, width and style (e.g. solid, dashed, dotted, etc…). Roads and rivers are commonly stored as polylines in a GIS. 2.1.1.3 Polygon Figure 1.3: A simple polygon object defined by an area enclosed by connected vertices. A polygon is composed of one or more lines whose starting and ending coordinate pairs are the same. Sometimes you will see words lattice or area used in lieu of ‘polygon’. Polygons represent both length (i.e. the perimeter of the area) and area. They also embody the idea of an inside and outside; in fact, the area that a polygon encloses is explicitly defined in a GIS environment. If it isn’t, then you are working with a polyline feature. If this does not seem intuitive, think of three connected lines defining a triangle: they can represent three connected road segments (thus polyline features), or the grassy strip enclosed by the connected roads (in which case an ‘inside’ is implied thus defining a polygon). 2.1.2 Raster Figure 2.3: A simple raster object defined by a 10x10 array of cells or pixels. A raster data model uses an array of cells, or pixels, to represent real-world objects. Raster datasets are commonly used for representing and managing imagery, surface temperatures, digital elevation models, and numerous other entities. A raster can be thought of as a special case of an area object where the area is divided into a regular grid of cells. But a regularly spaced array of marked points may be a better analogy since rasters are stored as an array of values where each cell is defined by a single coordinate pair inside of most GIS environments. Implicit in a raster data model is a value associated with each cell or pixel. This is in contrast to a vector model that may or may not have a value associated with the geometric primitive. 2.2 Object vs. Field The traditional vector/raster perspective of our world is one that has been driven by software and data storage environments. But this perspective is not particularly helpful if one is interested in analyzing the pattern. In fact, it can mask some important properties of the entity being studied. An object vs. field view of the world proves to be more insightful even though it may seem more abstract. 2.2.1 Object View An object view of the world treats entities as discrete objects; they need not occur at every location within a study area. Point locations of cities would be an example of an object. So would be polygonal representations of urban areas which may be non-contiguous. 2.2.2 Field View A field view of the world treats entities as a scalar field. This is a mathematical concept in which a scalar is a quantity having a magnitude. It is measurable at every location within the study region. Two popular examples of a scalar field are surface elevation and surface temperature. Each represents a property that can be measured at any location. Another example of a scalar field is the presence and absence of a building. This is a binary scalar where a value of 0 is assigned to a location devoid of buildings and a value of 1 is assigned to locations having one or more buildings. A field representation of buildings may not seem intuitive, in fact, given the definition of an object view of the world in the last section, it would seem only fitting to view buildings as objects. In fact, buildings can be viewed as both field or objects. The context of the analysis is ultimately what will dictate which view to adopt. If we’re interested in studying the distribution of buildings over a study area, then an object view of the features makes sense. If, on the other hand, we are interested in identifying all locations where buildings don’t exist, then a binary field view of these entities would make sense. 2.3 Scale How one chooses to represent a real-world entity will be in large part dictated by the scale of the analysis. In a GIS, scale has a specific meaning: it’s the ratio of distance on the map to that in the real world. So a large scale map implies a relatively large ratio and thus a small extent. This is counter to the layperson’s interpretation of large scale which focuses on the scope or extent of a study; so a large scale analysis would imply one that covers a large area. The following two maps represent the same entity: the Boston region. At a small scale (e.g. 1:10,000,000), Boston and other cities may be best represented as points. At a large scale (e.g. 1:34,000), Boston may be best represented as a polygon. Note that at this large scale roads may also be represented as polygon features instead of polylines. Figure 2.4: Map of the Boston area at a 1:10,000,000 scale. Note that in geography, this is considered small scale whereas in layperson terms, this extent is often referred to as a large scale (i.e. covering a large area). Figure 2.5: Map of the Boston area at a 1:34,000 scale. Note that in geography, this is considered large scale whereas in layperson terms, this extent is often referred to as a small scale (i.e. covering a small area). 2.4 Attribute Tables Non-spatial information associated with a spatial feature is referred to as an attribute. A feature on a GIS map is linked to its record in the attribute table by a unique numerical identifier (ID). Every feature in a layer has an identifier. It is important to understand the one-to-one or many-to-one relationship between feature, and attribute record. Because features on the map are linked to their records in the table, you can click a feature on the map and see its attributes in the table. When you select a record in the table, the linked feature on the map is automatically selected as well. Raster data can also have attributes only if pixels are represented using a small set of integer values. Raster datasets that contain attribute tables typically have cell values that represent or define a class, group, category, or membership. NOTE: not all GIS raster data formats can store attribute information; in fact most raster datasets you will work with in this course will not have attribute tables. 2.4.1 Measurement Levels Data can be broken down into four measurement levels: Nominal data which have no implied order, size or quantitative information (e.g. paved and unpaved roads) Ordinal data may be descriptive such as “high/low” or numeric (ranking from 1 to 10), however, we cannot quantify the difference since a linear scale is not implied (e.g. great, average, poor). Interval data are numeric and have a linear scale, however they do not have a true zero and can therefore not be used to measure relative magnitudes. For example, one cannot say that 60°F is twice as warm as 30°F since when presented in degrees °C the temperature values are 15.5°C and -1.1°C respectively (and 15.5 is clearly not twice as big as -1.1). Ratio scale data are interval data with a true zero such as monetary value (e.g. $1, $20, $100). 2.4.2 Data type Another way to categorize an attribute is by its data type. ArcGIS supports several data types such as integer, float, double and text. Knowing your data type and measurement level should dictate how they are stored in a GIS environment. The following table lists popular data types available in most GIS applications. Type Stored values Note Short integer -32,768 to 32,768 Whole numbers Long integer -2,147,483,648 to 2,147,483,648 Whole numbers Float -3.4 * E-38 to 1.2 E38 Real numbers Double -2.2E-308 to 1.8 E308 Real numbers Text Up to 64,000 characters Letters and words While whole numbers can be stored as a float or double (i.e. we can store the number 2 as 2.0) doing so comes at a cost: an increase in storage space. This may not be a big deal if the dataset is small, but if it consists of tens of thousands of records the increase in file size and processing time may become an issue. While storing an integer value as a float may not have dire consequences, the same cannot be said of storing a float as an integer. For example, if your values consist of 0.2, 0.01, 0.34, 0.1 and 0.876, their integer counterpart would be 0, 0, 0, and 1 (i.e. values rounded to the nearest whole number). This can have a significant impact on a map as shown in the following example. Figure 2.6: Map of data represented as decimal (float) values. Figure 2.7: Map of same data represented as integers instead of float. "],
["gis-data-management.html", "Chapter 3 GIS Data Management 3.1 GIS File Data Formats 3.2 Managing GIS Files 3.3 Managing a Map Project in ArcGIS", " Chapter 3 GIS Data Management 3.1 GIS File Data Formats In the GIS world, you will encounter many different GIS file formats. Some file formats are unique to specific GIS applications, others are universal. For this course, we will focus on a subset of spatial data file formats: shapefiles for vector data, imagine and GeoTiff files for rasters and file geodatabases for both vector and raster data. 3.1.1 Vector Data File Formats 3.1.1.1 Shapefile A shapefile is a file-based data format native to ArcView 3.x software (a much older version of ArcMap). Conceptually, a shapefile is a feature class–it stores a collection of features that have the same geometry type (point, line, or polygon), the same attributes, and a common spatial extent. Despite what its name may imply, a “single” shapefile is actually composed of at least three files, and as many as eight. Each file that makes up a “shapefile” has a common filename but different extension type. The list of files that define a “shapefile” are shown in the following table. Note that each file has a specific role in defining a shapefile. File extension Content .dbf Attribute information .shp Feature geometry .shx Feature geometry index .aih Attribute index .ain Attribute index .prj Coordinate system information .sbn Spatial index file .sbx Spatial index file 3.1.1.2 File Geodatabase A file geodatabase is a relational database storage format. It’s a far more complex data structure than the shapefile and consists of a .gdb folder housing dozens of files. Its complexity renders it more versatile allowing it to store multiple feature classes and enabling topological definitions (i.e. allowing the user to define rules that govern the way different feature classes relate to one another). An example of the contents of a geodatabase is shown in the following figure. Figure 2.2: Sample content of an ArcGIS file geodatabase. 3.1.1.3 GeoPackage This is a relatively new data format that follows (open format standards)[https://en.wikipedia.org/wiki/Open_format] (it is non-proprietary). It’s built on top of SQLite (a self-contained relational database). Its one big advantage over many other vector formats is its compactness–coordinate value, metadata, attribute table, projection information, etc…, are all stored in a single file which facilitates portability. Its filename usually ends in .gpkg. Applications such as QGIS (2.12 and up), R and ArcGIS will recognize this format (ArcGIS version 10.2.2 and above will read the file from ArcCatalog but requires a script to create a GeoPackage). 3.1.2 Raster Data File Formats Rasters are in part defined by their pixel depth. Pixel depth defines the range of distinct values the raster can store. For example, a 1-bit raster can only store 2 distinct values: 0 and 1. 3.1.2.1 Imagine The Imagine file format was originally created by an image processing software company called ERDAS. This file format consists of a single .img file. This is a simpler file format than the shapefile. It is sometimes accompanied by an .xml file which usually stores metadata information about the raster layer. 3.1.2.2 GeoTiff A popular public domain raster data format is the GeoTIFF format. If maximum portability and platform independence is important, this file format may be a good choice. 3.1.2.3 File Geodatabase A raster file can also be stored in a file geodatabase alongside vector files. Geodatabases have the benefit of defining image mosaic structures thus allowing the user to create “stitched” images from multiple image files stored in the geodatabase. Also, processing very large raster files can be computationally more efficient when stored in a file geodatabase as opposed to an Imagine file format. 3.2 Managing GIS Files Unless you are intimately familiar with the file structure of a GIS file, it is best to copy/move/delete GIS files from within the software environment. ArcCatalog (part of the ArcGIS suite) acts like a Windows file management environment with the added benefit that it recognizes GIS files. Figure 1.3: Windows Explorer view vs. ArcCatalog view. Note how the many files that make up the Parks shapefile (as viewed in a Windows Explorer environment) appears as a single entry in the ArcCatalog view. This makes it easier to rename the shapefile since it needs to be done only for a single file in ArcCatalog (as opposed to renaming the Parks files seven times in the Windows Explorer environment). 3.3 Managing a Map Project in ArcGIS Unlike many other software environments such as word processors and spreadsheets, a GIS map is not self-contained in a single file. A GIS map consists of many files: ArcMap’s .mxd file and the various vector and/or raster files used in the map project. The .mxd file only stores information about how the different layers are to be symbolized and the GIS file locations these layers point to. Because of the complex data structure associated with GIS maps, it’s usually best to store the .mxd and all associated GIS files under a single project directory. Then, when you are ready to share your map project with someone else, just pass along that project folder as is or compressed in a zip or tar file. Because .mxd map files read data from GIS files, it must know where to find these files on your computer or across the network. There are two ways in which a map document can store the location to the GIS files: as a relative pathname or a full pathname. A relative pathname defines the location of the GIS files relative to the location of the .mxd file on your computer. For example, let’s say that you created a project folder called HW05 under D:/Username/. In that folder, you have a map document, Map.mxd, that displays two layers stored in the GIS files Roads.shp and Cities.shp. In this scenario, the .mxd document and shapefiles are in the same project folder. If you set the Pathnames parameter to “Store relative pathnames to data sources” (accessed from ArcMap’s File &gt;&gt; Map Document Properties menu) ArcMap will not need to know the entire directory structure above the HW05/ folder to find the two shapefiles as illustrated below. If the “Store relative pathnames to data sources” is not checked in the map’s document properties, then ArcMap will need to know the entire directory structure leading to the HW05/ folder as illustrated below. Exclamation marks in your map document indicate that the GIS files are missing or that the directory structure has changed. Your choice of full vs relative pathnames matters if you find yourself having to move or copy your project folder to another directory structure. For example, if you share you HW05/ project folder with another user and that user places the project folder under a different directory structure such as C:/User/Jdoe/GIS/, ArcMap will not find the shapefiles if the pathnames is set to full (i.e. the Store relative pathnames is not checked). This will result in exclamation marks in your map document TOC. This problem can be avoided by making sure that the map document is set to use relative pathnames and by placing all GIS files (raster and vector) in a common project folder. "],
["symbolizing-features.html", "Chapter 4 Symbolizing features 4.1 Color 4.2 Color Space 4.3 Classification 4.4 So how do I find a proper color scheme for my data? 4.5 Classification Intervals", " Chapter 4 Symbolizing features 4.1 Color Each color is a combination of three perceptual dimensions: hue, lightness and saturation. 4.1.1 Hue Hue is the perceptual dimension associated with color names. Figure 2.2: An example of eight different hues. Hues are associated with color names such as green, red or blue. Note that magentas and purples are not part of the natural visible light spectrum; instead they are a mix of reds and blues (or violets) from the spectrum’s tail ends. Typically, we use different hues to represent different categories of data. 4.1.2 Lightness Lightness (sometimes referred to as value) describes how much light reflects (or is emitted) off of a surface. Lightness is an important dimension for representing ordinal/interval/ratio data. Figure 1.2: Eight different hues with decreasing lightness values. 4.1.3 Saturation Saturation (sometimes referred to as chroma) is a measure of a color’s vividness. You can use saturated colors to help distinguish map symbols. But be careful when manipulating saturation, its property should be modified sparingly in most maps. Figure 1.3: Eight different hues with decreasing saturation values. 4.2 Color Space The three perceptual dimensions of color can be used to construct a 3D color space. This 3D space need not be a cube (as one would expect given that we are combining three dimensions) but a cone where lightness, saturation and hue are the cone’s height, radius and circumference respectively. Figure 2.3: This is how the software defines the color space. But does this match our perception of color space? The cone shape reflects the fact that as one decreases saturation, the distinction between different hues disappears leading to a grayscale color (the central axis of the cone). So if one sets the saturation value of a color to 0, the hue ends up being some shade of grey. The color space implemented in most software is symmetrical about the value/lightness axis. However, this is not how we “perceive” color space: our perceptual view of the color space is not perfectly symmetrical. Let’s examine a slice of the symmetrical color space along the blue/yellow hue axis at a lightness value of about 90%. Figure 2.4: A cross section of the color space with constant hues and lightness values and decreasing saturation values where the two hues merge. Now, how many distinct yellows can you make out? How many distinct blues can you make out? Do the numbers match? Unless you have incredible color perception, you will probably observe that the number of distinct colors do not match when in fact they do! There are exactly 30 distinct blues and 30 distinct yellows. Let’s add a border to each color to convince ourselves that the software did indeed generate the same number of distinct colors. Figure 2.5: A cross section of the color space with each color distinctly outlined. It should be clear by now that a symmetrical color space does not reflect the way we “perceive” colors. There are more rigorously designed color spaces such as CIELAB and Munsell that depict the color space as a non-symmetrical object as perceived by humans. For example, in a Munsell color space, a vertical slice of the cone along the blue/yellow axis looks like this. Figure 2.6: A slice of the Munsell color space. Note that based on the Munsell color space, we can make out fewer yellows than blues across all lightness values. In fact, for these two hues, we can make out only 29 different shades of yellow (we do not include the gray levels where saturation = 0) vs 36 shades of blue. So how do we leverage our understanding of color spaces when choosing colors for our map features? The next section highlights three different color schemes: qualitative, sequential and divergent. 4.3 Classification 4.3.1 Qualitative color scheme Qualitative schemes are used to symbolize data having no inherent order (i.e. categorical data). Different hues are normally used to distinguish different categorical values. When possible, use commonly associated hues for land features such as vegetation and water. Figure 2.7: Example of four different qualitative color schemes. Color hex numbers are superimposed on each palette. Election results can be displayed using a qualitative color scheme. But be careful in your choice of hues if a cultural bias exists (i.e. it may not make sense to assign “blue” to republican or “red”&quot; to democratic regions). Figure 4.1: Map of 2012 election results shown in a qualitative color scheme. Note the use of three hues (red, blue and gray) of equal lightness and saturation. 4.3.2 Sequential color scheme Sequential color schemes are used to highlight ordered data such as income, temperature, elevation or infection rates. A well designed sequential color scheme ranges from a light color (representing low attribute values) to a dark color (representing high attribute values). Such color schemes are typically composed of a single hue, but may include two hues as shown in the last two color schemes of the following figure. Figure 4.2: Example of four different sequential color schemes. Color hex numbers are superimposed on each palette. Distribution of income is a good example of a sequential map. Income values are interval/ratio data which have an implied order. Figure 4.3: Map of household income shown in a sequential color scheme. Note the use of a single hue (green) and 7 different lightness levels. 4.3.3 Divergent color scheme Divergent color schemes apply to ordered data as well. However, there is an implied central value about which all values are compared. Typically, a divergent color scheme is composed of two hues–one for each side of the central value. Each hue’s lightness/saturation value is then adjusted symmetrically about the central value. Examples of such a color scheme follows: Figure 4.4: Example of four different divergent color schemes. Color hex numbers are superimposed onto each palette. Continuing with the last example, we now focus on the divergence of income values about the median value of $36,641. We use a brown hue for income values below the median and a green/blue hue for values above the median. Figure 4.5: This map of household income uses a divergent color scheme where two different hues (brown and blue-green) are used for two sets of values separated by the median income of 36,641 dollars. Each hue is then split into three separate colors using decreasing lightness values away from the median. 4.4 So how do I find a proper color scheme for my data? Fortunately, there is a wonderful online resource that will guide you through the process of picking a proper set of color swatches given the nature of your data (i.e. sequential, diverging, and qualitative) and the number of intervals (aka classes). The website is http://colorbrewer2.org/ and was developed by Cynthia Brewer et. al at the Pennsylvania State University. You’ll note that the ColorBrewer website limits the total number of color swatches to 12 or less. There is a good reason for this in that our eyes can only associate so many different colors with value ranges/bins. Try matching 9 different shades of green in a map to the legend box swatches! Additional features available on that website include choosing colorblind safe colors and color schemes that translate well into grayscale colors (useful if your work is to be published in journals that do not offer color prints). 4.5 Classification Intervals You may have noticed the use of different classification breaks in the last two maps. For the sequential color scheme map, an equal interval classification scheme was used where the full range of values in the map are split equally into 7 intervals so that each color swatch covers an equal range of values. The divergent color scheme map adopts a quantile interval classification where each color swatch is represented an equal number of times across each polygon. Using different classification intervals will result in different looking maps. In the following figure, three maps of household income (aggregated at the census tract level) are presented using different classification intervals: quantile, equal and Jenks. Note the different range of values covered by each color swatch. The quantile interval scheme ensures that each color swatch is represented an equal number of times. If we have 20 polygons and 5 classes, the interval breaks will be such that each color is assigned to 4 different polygons. The equal interval scheme breaks up the range of values into equal interval widths. If the polygon values range from 10,000 to 25,000 and we have 5 classes, the intervals will be [10,000 ; 13,000], [13,000 ; 16,000], …, [22,000 ; 25,000]. The Jenks interval scheme (aka natural breaks) uses an algorithm that identifies clusters in the dataset. The number of clusters is defined by the desired number of intervals. It may help to view the breaks when superimposed on top of a distribution of the attribute data. In the following graphics the three classification intervals are superimposed on a histogram of the per-household income data. The histogram shows the distribution of values as “bins” where each bin represents a range of income values. The y-axis shows the frequency (or number of occurrences) for values in each bin. Figure 4.6: Three different classification intervals used in the three maps. Note how each interval scheme encompasses different ranges of values (hence the reason all three maps look so different). "],
["pitfalls-to-avoid.html", "Chapter 5 Pitfalls to avoid 5.1 Representing Count 5.2 MAUP 5.3 Ecological Fallacy 5.4 Mapping rates 5.5 Coping with Unstable Rates", " Chapter 5 Pitfalls to avoid 5.1 Representing Count Let’s define a 5km x 5km area and map the location of each individual inside the study area. Let’s assume, for sake of argument, that individuals are laid out in a perfect grid pattern. Now let’s define two different zoning schemes: one which follows a uniform grid pattern and another that does not. The layout of individuals relative to both zonal schemes are shown in Figure 5.1. Figure 5.1: Figure shows the layout of individuals inside two different zonal unit configurations. If we sum the number of individuals in each polygon, we get two maps that appear to be giving us two completely different population distribution patterns: Figure 5.2: Count of individuals in each zonal unit. Note how an underlying point distribution can generate vastly different looking choropleth maps given different aggregation schemes. The maps highlight how non-uniform aerial units can fool us into thinking a pattern exists when in fact this is just an artifact of the aggregation scheme. A solution to this problem is to represent counts as ratios such as number of deaths per number of people or number of people per square kilometer. In Figure 5.3, we opt for the latter ratio (number of people per square kilometer). Figure 5.3: Point density choropleth maps. The sample study extent is 20x20 units which generates a uniform point density of 1. The slight discrepancy in values for the map on the right is to be expected given that the zonal boundaries do not split the distance between points exactly. 5.2 MAUP Continuing with the uniform point distribution from the last section, let’s assume that as part of the survey, two variables (v1 and v2) were recorded for each point (symbolized as varying shades of green and reds in the two left-hand maps of Figure 5.4). We might be interested in assessing if the variables v1 and v2 are correlated (i.e. as variable v1 increases in value, does this trigger a monotonic increase or decrease in variable v2?). One way to visualize the relationship between two variables is to generate a bivariate scatter plot (left plot of Figure 5.4). Figure 5.4: Plots of variables v1 and v2 for each individual in the survey. The color scheme is sequential with darker colors depicting higher values and lighter colors depicting lower values. It’s obvious from the adjoining scatter plot that there is little to no correlation between variables v1 and v2 at the individual level; both the slope and coefficient of determination, \\(R^2\\), are close to \\(0\\). But many datasets (such as the US census data) are provided to us not at the individual level but at various levels of aggregation units such as the census tract, the county or the state levels. When aggregated, the relationship between variables under investigation may change. For example, if we aggregated v1 and v2 using the uniform aggregation scheme highlighted earlier we get the following relationship. Figure 5.5: Data summarized using a uniform aggregation scheme. The resulting regression analysis is shown in the right-hand plot. Note the slight increase in slope and \\(R^2\\) values. If we aggregate the same point data using the non-homogeneous aggregation scheme, we get yet another characterization of the relationship between v1 and v2. Figure 5.6: Data summarized using a non-uniform aggregation scheme.The resulting regression analysis is shown in the right-hand plot. Note the high \\(R^2\\) value, yet the underlying v1 and v2 variables from which the aggregated values were computed were not at all correlated! It should be clear by now that different aggregation schemes can result in completely different analyses outcomes. In fact, it would not be impossible to come up with an aggregation scheme that would produce near perfect correlation between variables v1 and v2. This problem is often referred to as the modifiable aerial unit problem (MAUP) and has, as you can well imagine by now, some serious implications. Unfortunately, this problem is often overlooked in many analyses that involve aggregated data. 5.3 Ecological Fallacy But, as is often the case, our analysis is constrained by the data at hand. So when analyzing aggregated data, you must be careful in how you frame the results. For example, if your analysis was conducted with the data summarized using the non-uniform aggregation scheme, you might be tempted to state that there is a strong relationship between variables v1 and v2 at the individual level. But doing so leads to the ecological fallacy where the statistical relationship at one level of aggregation is (wrongly) assumed to hold at any other levels of aggregation (including at the individual level). In fact, all you can really say is that “at this level of aggregation, we observe a strong relationship between v1 and v2” and nothing more! 5.4 Mapping rates One of the first pitfalls you’ve been taught to avoid is the mapping of counts when the aerial units associated with these values are not uniform in size and shape. Two options in resolving this problem are: normalizing counts to area and normalizing counts to some underlying population count. An example of the latter is the mapping of infection rates or mortality rates. For example, the following map displays the distribution of kidney cancer death rates (by county) for the period 1980 to 1984. Figure 2.2: Kidney cancer death rates for the period spanning 1980-1984. Now let’s look at the top 10% of counties with the highest death rates. Figure 1.2: Top 10% of counties with the highest kidney cancer death rates. And now let’s look at the bottom 10% of counties with the lowest death rates. Figure 1.3: Bottom 10% of counties with the lowest kidney cancer death rates. A quick glance of these maps suggests clustering of high and low rates around the same parts of the country. In fact, if you were to explore these maps in a GIS, you would note that many of the bottom 10% counties are adjacent to the top 10% counties! If local environmental factors are to blame for kidney cancer deaths, why would they be present in one county and not in an adjacent county? Could differences in regulations between counties be the reason? These are hypotheses that one would probably want to explore, but before pursuing these hypotheses, it would behoove us to look a bit more closely at the batch of numbers we are working with. Let’s first look at a population count map (note that we are purposely not normalizing the count data). Figure 2.3: Population count for each county. Note that a quantile classification scheme is adopted force a large range of values to be assigned a single color swatch The central part of the states where we are observing both very high and very low cancer death rates seem to have low population counts. Could population count have something to do with this odd congruence of high and low cancer rates? Let’s explore the relationship between death rates and population counts outside of a GIS environment and focus solely on the two batches of numbers. The following plot is a scatterplot of death rates and population counts. Figure 2.4: Plot of rates vs population counts. Note the skewed nature of both data batches. Transforming both variables reveals much more about the relationship between them. Figure 2.5: Plot of rates vs population counts on log scales. One quickly notices a steady decrease in death rate variability about some central value of ~0.000045 (or 4.5e-5) as the population count increases. This is because lower population counts tend to generate the very high and very low rates observed in our data. This begs the question: does low population count cause very high and low cancer death rates, or is this simply a numerical artifact? To answer this question, let’s simulate some data. Let’s assume that the real death rate is 5 per 100,000 people . If a county has a population of 1000, then \\(1000 \\times 5e-5 = 0.05\\) persons would die of kidney cancer; when rounded to the next whole person, that translates to \\(0\\) deaths in that county. Now, there is still the possibility that a county of a 1000 could have one person succumb to the disease in which case the death rate for that county would be \\(1/1000=0.001\\) or 1 in a 1000, a rate much greater than the expected rate of 5 in 100,000! This little exercise reveals that you could never calculate a rate of 5 in 100,000 with a population count of just 1000. You either compute a rate of \\(0\\) or a rate of \\(0.001\\) (or more). In fact, you would need a large population count to accurately estimate the real death rate. Turning our attention back to our map, you will notice that a large majority of the counties have a small population count (about a quarter have a population count of 22,000 or less). This explains the wide range of rates observed for these smaller counties; the larger counties don’t have such a wide swing in values because they have a larger sample size which can more accurately reflect the true death rate. Rates that are computed using relatively small “at risk” population counts are deemed unstable. 5.5 Coping with Unstable Rates To compensate for the small population counts, we can minimize the influence those counties have on the representation of the spatial distribution of rates. One such technique, empirical Bayes (EB) method, does just that. Where county population counts are small, the “rates” are modified to match the overall expected rate (which is an average value of all rates in the map). This minimizes the counties’ influence on the range of rate values. EB techniques for rate smoothing aren’t available in ArcGIS (as of version 10.2) but are available in a couple of free and open source applications such as GeoDa and R. An EB smoothed representation of kidney cancer deaths gives us the following rate vs population plot: Figure 5.7: Plot of EB smoothed rates vs population counts on log scales. The variability in rates for smaller counties has decreased. The range of rate values has dropped from 0.00045 to 0.00023. Variability is still greater for smaller counties than larger ones, but not as pronounced as it was with the raw rates Maps of the top 10% and bottom 10% EB smoothed rates are shown in the next two figures. Figure 2.6: Top 10% of counties with the highest kidney cancer death rates using EB smoothing techniques. Figure 2.7: Bottom 10% of counties with the lowest kidney cancer death rates using EB smoothing technique. Note the differences in rate distribution. For example, higher rates now show up in Florida which would be expected given the large retirement population, and clusters are now contiguous which could suggest local effects. But it’s important to remember that EB smoothing does not reveal the true underlying rate; it only masks those that are unreliable. Also, EB smoothing does not completely eliminate unstable rates–note the slighlty higher rates for low population counts in Figure 5.7. Other solutions to the unstable rate problem include: Grouping small counties into larger ones–thus increasing population sample size. Increasing the study’s time interval. In this example, data were aggregated over the course of 5 years (1980-1984) but could be increased by adding 5 more years thus increasing sample sizes in each county. Grouping small counties AND increasing the study’s time interval. These solutions do have their downside in that they decrease the spatial and/or temporal resolutions. It should be clear by now that there is no single one-size-fits-all solution to the unstable rate problem. A sound analysis will usually require that one or more of the aforementioned solutions be explored. "],
["good-map-making-tips.html", "Chapter 6 Good Map Making Tips 6.1 Elements of a map 6.2 How to create a good map 6.3 Typefaces and Fonts", " Chapter 6 Good Map Making Tips 6.1 Elements of a map A map can be composed of many different map elements. They may include: Main map body, legend, title, scale indicator, orientation indicator, inset map and source and ancillary information. Not all elements need to be present in a map. In fact, in some cases they may not be appropriate at all. A scale bar, for instance, may not be appropriate if the coordinate system used does not preserve distance across the map’s extent. Knowing why and for whom a map is being made will dictate its layout. If it’s to be included in a paper as a figure, then parsimony should be the guiding principle. If it’s intended to be a standalone map, then additional map elements may be required. Knowing the intended audience should also dictate what you will convey and how. If it’s a general audience with little technical expertise then a simpler presentation may be in order. If the audience is well versed in the topic, then the map may be more complex. Map elements. Note that not all elements are needed, nor are they appropriate in some cases. Can you identify at least one element that does not belong in the map (hint, note the orientation of the longitudinal lines; are they parallel to one another? What implication does this have on the North direction and the placement of the North arrow?) 6.2 How to create a good map Here’s an example of a map layout that showcases several bad practices. Figure 6.1: Example of a bad map. Can you identify the many issues with this map? A good map establishes a visual hierarchy that ensures that the most important elements are at the top of this hierarchy and the least important are at the bottom. Typically, the top elements should consist of the main map body, the title (if this is a standalone map) and a legend (when appropriate). When showcasing Choropleth maps, it’s best to limit the color swatches to less than a dozen–it becomes difficult for the viewer to tie too many different colors in a map to a color swatch element in the legend. Also, classification breaks should not be chosen at random but should be chosen carefully; for example adopting a quantile classifications scheme to maximize the inclusion of the different color swatches in the map; or a classification system designed based on logical breaks (or easy to interpret breaks) when dictated by theory or cultural predisposition. Scale bars and north arrows should be used judiciously and need not be present in every map. These elements are used to measure orientation and distances. Such elements are critical in reference maps such as USGS Topo maps and navigation maps but serve little purpose in a thematic map where the goal is to highlight differences between aerial units. If, however, these elements are to be placed in a thematic map, reduce their visual prominence (see Figure @ref(fig:ScaleBar for examples of scale bars). The same principle applies to the selection of an orientation indicator (north arrow) element. Use a small north arrow design if it is to be placed low in the hierarchy, larger if it is to be used as a reference (such as a nautical chart). Figure 6.2: Scale bar designs from simplest (top) to more complex (bottom). Use the simpler design if it’s to be placed low in the visual hierarchy. Title and other text elements should be concise and to the point. If the map is to be embedded in a write-up such as a journal article, book or web page, title and text(s) elements should be omitted in favor of figure captions and written description in the accompanying text. Following the aforementioned guidelines can go a long way in producing an effective map. Figure 6.3: Example of a good map. 6.3 Typefaces and Fonts Maps may include text elements such as labels and ancillary text blocks. The choice of typeface (font family) and font (size, weight and style of a typeface) can impact the legibility of the map. A rule of thumb is to limit the number of fonts to two: a serif and a sans serif font. Figure 2.2: Serif fonts are characterized by brush strokes at the letter tips (highlighted in red in accompanying figure). Sans Serif fonts are devoid of brush strokes. Serif fonts are generally used to label natural features such as mountain ridges and water body names. Sans serif fonts are usually used to label anthropogenic features such as roads, cities and countries. Varying the typeset size across the map should be avoided unless a visual hierarchy of labels is desired. You also may want to stick with a single font color across the map unless the differences in categories need to be emphasized. In the following example, a snapshot of a map before (left) and after (right) highlight how manipulating typeset colors and styles (i.e. italic, bold) can have a desirable effect if done properly. Figure 1.2: The lack of typeset differences makes the map on the left difficult to differentiate county names from lake/river names. The judicious use of font colors and style on the right facilitate the separation of features. "],
["uncertainty-in-census-data.html", "Chapter 7 Uncertainty in Census Data 7.1 Introduction 7.2 Mapping uncertainty 7.3 Problems in mapping uncertainty 7.4 Class comparison maps 7.5 Problem when performing bivariate analysis", " Chapter 7 Uncertainty in Census Data 7.1 Introduction Many census datasets such as the U.S. Census Bureau’s American Community Survey (ACS) data1 are based on surveys from small samples. This entails that the variables provided by the Census Bureau are only estimates with a level of uncertainty often provided as a margin of error (MoE) or a standard error (SE). Note that the Bureau’s MoE encompasses a 90% confidence interval2 (i.e. there is a 90% chance that the MoE range covers the true value being estimated). This poses a challenge to both the visual exploration of the data as well as any statistical analyses of that data. 7.2 Mapping uncertainty One approach to mapping both estimates and SE’s is to display both as side-by-side maps. Figure 1.2: Maps of income estimates (left) and associated standard errors (right). While there is nothing inherently wrong in doing this, it can prove to be difficult to mentally process the two maps, particularly if the data consists of hundreds or thousands of small polygons. Another approach is to overlay the measure of uncertainty (SE or MoE) as a textured layer on top of the income layer. Figure 1.3: Map of estimated income (in shades of green) superimposed with different hash marks representing the ranges of income SE. Or, one could map both ends of the MoE range side by side. Figure 2.3: Maps of top end of 90 percent income estimate (left) and bottom end of 90 percent income estimate (right). 7.3 Problems in mapping uncertainty Attempting to convey uncertainty using the aforementioned maps fails to highlight the reason one chooses to map values in the first place: that is to compare values across a spatial domain. More specifically, we are interested in identifying spatial patterns of high or low values. What is implied in the above maps is that the estimates will always maintain their order across the polygons. In other words, if one polygon’s estimate is greater than all neighboring estimates, this order will always hold true if another sample was surveyed. But this assumption is incorrect. Each polygon (or county in the above example) can derive different estimates independently from its neighboring polygon. Let’s look at a bar plot of our estimates. Figure 2.4: Income estimates by county with 90 percent confidence interval. Note that many counties have overlapping estimate ranges. Note, for example, how Piscataquis county’s income estimate (grey point in the graphic) is lower than that of Oxford county. If another sample of the population was surveyed in each county, the new estimates could place Piscataquis above Oxford county in income rankings as shown in the following example: Figure 2.5: Example of income estimates one could expect to sample based on the 90 percent confidence interval shown in the previous plot. Note how, in this sample, Oxford’s income drops in ranking below that of Piscataquis. A similar change in ranking could occur for York county which drops down two counties (Hancock and Lincoln). How does the estimated income map compare with the simulated income map? Figure 2.6: Original income estimate (left) and realization of a simulated sample (right). A few more simulated samples (using the 90% confidence interval) are shown below: Figure 2.7: Original income estimate (left) and realizations from simulated samples (R1 through R5). 7.4 Class comparison maps There is no single solution to effectively convey both estimates and associated uncertainty in a map. Sun and Wong (Sun and Wong 2010) offer several suggestions dependent on the context of the problem. One approach adopts a class comparison method whereby a map displays both the estimate and a measure of whether the MoE surrounding that estimate extends beyond the assigned class. For example, if we adopt the classification breaks [0 , 20600 , 22800 , 25000 , 27000 , 34000 ], we will find that many of the estimates’ MoE extend beyond the classification breaks assigned to those estimates. Figure 7.1: Income estimates by county with 90 percent confidence interval. Note that many of the counties’ MoE have ranges that cross into an adjacent class. Take Piscataquis county, for example. Its estimate is assigned the second classification break (20600 to 22800 ), yet its lower confidence interval stretches into the first classification break indicating that we cannot be 90% confident that the estimate is assigned the proper class (i.e. its true value could fall into the first class). Other counties such as Cumberland and Penobscot don’t have that problem since their 90% confidence intervals fall inside the classification breaks. This information can be mapped as a hatch mark overlay. For example, income could be plotted using varying shades of green with hatch symbols indicating if the lower interval crosses into a lower class (135° hatch), if the upper interval crosses into an upper class (45° hatch), if both interval ends cross into a different class (90°-vertical-hatch) or if both interval ends remain inside the estimate’s class (no hatch). Figure 7.2: Plot of income with class comparison hatches. 7.5 Problem when performing bivariate analysis Data uncertainty issues do not only affect choropleth map presentations but also affect bivariate or multivariate analyses where two or more variables are statistically compared. One popular method in comparing variables is the regression analysis where a line is best fit to a bivariate scatterplot. For example, one can regress “percent not schooled”&quot; to “income”&quot; as follows: Figure 4.4: Regression between percent not having completed any school grade and median per capita income for each county. The \\(R^2\\) value associated with this regression analysis is 0.2 and the p-value is 0.081. But another realization of the survey could produce the following output: Figure 7.3: Example of what a regression line could look like had another sample been surveyed for each county. With this new (simulated) sample, the \\(R^2\\) value dropped to 0.07 and the p-value is now 0.322–a much less significant relationship then computed with the original estimate! In fact, if we were to survey 1000 different samples within each county we would get the following range of regression lines: Figure 7.4: A range of regression lines computed from different samples from each county. These overlapping lines define a type of confidence interval (aka confidence envelope). In other words, the true regression line between both variables lies somewhere within the dark region delineated by this interval. References "],
["spatial-operations-and-vector-overlays.html", "Chapter 8 Spatial Operations and Vector Overlays 8.1 Selection by Attribute 8.2 Selection by location 8.3 Vector Overlay", " Chapter 8 Spatial Operations and Vector Overlays 8.1 Selection by Attribute Features in a GIS layer can be selected graphically or by querying attribute values. For example, if a GIS layer represents land parcels, one could use the Area field to select all parcels having an area greater than 2.0 acres. Set algebra is used to define conditions that are to be satisfied while Boolean algebra is used to combine a set of conditions. 8.1.1 Set Algebra Set algebra consists of four basic operators: less than (&lt;), greater than (&gt;), equal to (=) not equal to (&lt;&gt;). Note that in some programming environments (such as R and Python), the equality condition is presented as two equal signs, ==, and not one. In such an environment x = 3 in interpreted as “pass the value 3 to x” and x == 3 is interpreted as “is x equal to 3?”. For example, if you have a GIS layer of major cities and you want to identify all census tracts having a population count greater than 50000, you would write the expression as &quot;POP&quot; &gt; 50000 (of course, this assumes that the attribute field name for population count is POP). Figure 1.2: An example of selection by attributes tool in ArcMap where it is accessed from the Selection pull-down menu. Figure 1.3: Selected cities meeting the criterion are shown in cyan color. The result of this operation is a selected subset of the Cities point layer. Note that in most GIS applications the selection process does not create a new layer. 8.1.2 Boolean Algebra You can combine conditions from set algebra operations using the following Boolean algebra operators: or (two conditions can be met), and (two conditions must be met), not (condition must not be met). Following up with the last example, let’s now select cities having a population greater than 50000 and that are in the US (and not Canada or Mexico). Assuming that the country field is labeled FIPS_CNTRY we could setup the expression as &quot;POP&quot; &gt; 50000 AND &quot;FIPS_CNTRY&quot; = US. Note that a value need not be numeric. In this example we are asking that an attribute value equal a specific string value (i.e. that it equal the string 'US'). Figure 2.3: Selected cities meeting both criteria are shown in cyan color. 8.2 Selection by location We can also select features from one GIS layer based on their spatial association with another GIS layer. This type of spatial association can be broken down into four categories: adjacency (whether features from one layer share a boundary with features of another), containment (whether features from one layer are inside features of another), intersection (whether features of one layer intersect features of another), and distance (whether one feature is within a certain distance from another). Continuing with our working example, we might be interested in cities that are within 100 miles of recent earthquakes. The earthquake points are from another GIS layer called Earthquakes (Sep 2013). Figure 2.4: An example of a selection by location tool in ArcMap where it is accessed from the Selection pull-down menu. Figure 2.5: Selected cities meeting the criterion are shown in cyan color. You can, of course, combine multiple selection criteria (both by attribute and location), but be careful when selecting from an existing selection in ArcMap: in both the Selection by Location window and the Selection by Attribute window, you must explicitly set the option to select from an existing selection. 8.3 Vector Overlay The concept of vector overlay is not new and goes back many decades–even before GIS became ubiquitous. It was once referred to as sieve mapping by land use planners who combined different layers–each mapped onto separate transparencies–to isolate or eliminate areas that did or did not meet a set of criteria. Map overlay refers to a group of procedures and techniques used in combining information from different data layers. This is an important capability of most GIS environments. Map overlays involve at least two input layers and result in at least one new output layer. A basic set of overlay tools include clipping, intersecting and unioning. 8.3.1 Clip Clipping takes one GIS layer (the clip feature) and another GIS layer (the to-be-clipped input feature). The output is a clipped version of the original input layer. The output attributes table is a subset of the original attributes table where only records for the clipped polygons are preserved. 8.3.2 Intersect Intersecting takes both layers as inputs then outputs the features from both layers that share the same spatial extent. Note that the output attribute table inherits attributes from both input layers (this differs from clipping where attributes from just one layer a carried through). 8.3.3 Union Unioning overlays both input layers and outputs all features from the two layers. Features that overlap are intersected creating new polygons. This overlay usually produces more polygons than are present in both input layers combined. The output attributes table contains attribute values from both input features (note that only a subset of the output attributes table is shown in the following figure). "],
["coordinate-systems.html", "Chapter 9 Coordinate Systems 9.1 Geographic Coordinate Systems 9.2 Projected Coordinate Systems 9.3 Spatial Properties", " Chapter 9 Coordinate Systems Implicit with any GIS data is a spatial reference system. It can consist of a simple arbitrary reference system such as a 10 m x 10 m sampling grid in a wood lot or, the boundaries of a soccer field or, it can consist of a geographic reference system, i.e. one where the spatial features are mapped to an earth based reference system. The focus of this topic is on earth reference systems which can be based on a Geographic Coordinate System (GCS) or a Project Coordinate System (PCS). 9.1 Geographic Coordinate Systems A geographic coordinate system is a reference system for identifying locations on the curved surface of the earth. Locations on the earth’s surface are measured in angular units from the center of the earth relative to two planes: the plane defined by the equator and the plane defined by the prime meridian (which crosses Greenwich England). A location is therefore defined by two values: a latitudinal value and a longitudinal value. Figure 2.2: Examples of latitudinal lines are shown on the left and examples of longitudinal lines are shown on the right. The 0° degree reference lines for each are shown in red (equator for latitudinal measurements and prime meridian for longitudinal measurements). A latitude measures the angle from the equatorial plane to the location on the earth’s surface. A longitude measurement measures the angle between the prime meridian plane to the location of the earth’s surface. For example Colby College is located at around 45.56° North and 69.66° West. In a GIS system, the North-South and East-West directions are encoded as signs. North and East are assigned a positive (\\(+\\)) sign and South and West are assigned a negative (\\(-\\)) sign. Colby College’s location is therefore encoded as +45.56° and -69.66°. A GCS is defined by an ellipsoid, geoid and datum. These elements are presented next. 9.1.1 Sphere and Ellipsoid Assuming that the earth is a perfect sphere greatly simplifies mathematical calculations and works well for small-scale maps (maps that show a large area of the earth). However, when working at larger scales, an ellipsoid representation of earth may be desired if accurate measurements are needed. An ellipsoid is defined by two radii: the semi-major axis (the polar radius) and the semi-minor axis (the equatorial radius). The reason the earth has a slightly ellipsoidal shape has to do with its rotation which induces a centripetal force along the equator. This results in an equatorial axis that is roughly 21 km longer than the polar axis. Figure 2.3: The earth can be mathematically modeled as a simple sphere (left) or an ellipsoid (right). Our estimate of these radii is quite precise thanks to satellite and computational capabilities. The semi-major axis is 6,378,137 meters and the semi-minor axis 6,356,752 meters. Differences in distance measurements on an ellipsoid vs. a sphere are small but measurable (the difference can be as high as 20 km) as illustrated in the following lattice plots. Figure 2.4: Differences in surface distance measurements between a sphere and an ellipsoid. Each graphic plots the differences in distance measurements made from a single point location along the 0° meridian identified by the green colored box (latitude value) to various latitudinal locations along a longitude (whose value is listed in the bisque colored box). For example, the second plot from the top-left corner plot shows the differences in distance measurements made from a location at 90° north (along the prime meridian) to a range of latitudinal locations along the 45° meridian. 9.1.2 Geoid Representing the earth’s surface as a mathematical model is crucial for a GIS environment. However, the earth’s shape is not a perfectly smooth surface. It has undulations resulting from changes in gravitational pull across its surface. These undulations may not be visible with the naked eye, but it is measurable and can influence locational measurements. Note that we are not including mountains and ocean bottoms in our discussion, instead we are focusing solely on the earth’s gravitational potential which can be best visualized by imagining the earth’s surface completely immersed in water and measuring the sea surface level over the entire earth surface. Figure 2.5: Earth’s geoid with gravitational field shown in rainbow colors. The ondulations depicted in the graphics are exaggerated for visual effects. (source: NASA) The earth’s gravitational field is dynamic and is tied to the flow of the earth’s hot and fluid core. Hence its geoid is constantly changing, albeit at a large temporal scale.The measurement and representation of the earth’s shape is at the heart of geodesy–a branch of applied mathematics. 9.1.3 Datum So how are we to reconcile our need to work with a (simple) mathematical model of the earth’s shape with the ondulating nature of the earth’s surface (i.e. its geoid)? The solution is to align the geoid with the ellipsoid (or sphere) representation of the earth and to map the earth’s surface features onto this ellipsoid. The alignment can be local where the ellipsoid surface is closely fit to the geoid at a particular location on the earth’s surface (such as the state of Kansas) or geocentric where the ellipsoid is aligned with the center of the earth. How one chooses to align the ellipsoid to the geoid defines a datum. 9.1.3.1 Local Datum There are many local datums to choose from, some are old while others are more recently defined. The choice of datum is largely driven by the location of interest. For example, when working in the US, a popular local datum to choose from is the North American Datum of 1927 (or NAD27 for short). NAD27 works well for the US but is not well suited for other parts of the world. For example, a far better local datum for Europe is the European Datum of 1950 (ED50 for short). Examples of common local datums are shown in the following table: Local datum Acronym Best for Comment North American Datum of 1927 NAD27 Continental US This is an old datum but still prevalent because of the wide use of older maps. European Datum of 1950 ED50 Western Europe Developed after World War II and still quite popular today. Not used in the UK. World Geodetic System 1972 WGS72 Global Developed by the Department of Defense. 9.1.3.2 Geocentric Datum Many of the modern datums use a geocentric alignment. These include the popular World Geodetic Survey for 1984 (WGS84) and North American Datums of 1983 (NAD83) datums. Most popular geocentric datums use the WGS84 ellipsoid or the GRS80 ellipsoid. These ellipsoids’ semi-major and semi-minor axes are nearly identical: 6,378,137 meters and 6,356,752 meters respectively. Examples of popular geocentric datums are shown in the following table: Geocentric datum Acronym Best for Comment North American Datum of 1983 NAD83 Continental US This is one of the most popular modern datums for the contiguous US. European Terrestrial Reference System 1989 ETRS89 Western Europe This is the most popular modern datum for much of Europe. World Geodetic System 1984 WGS84 Global Developed by the Department of Defense. 9.1.4 Building the Geographic Coordinate System A Geographic Coordinate System (GCS) is therefore defined by the ellipsoid model and by the way this ellipsoid is aligned with the geoid (defining the datum). It is important to know which GCS is associated with a GIS file or map document reference system. This is particularly true when overlapping layers are tied to different datums (and therefore GCS’). This is because a location on the earth’s surface can take on different coordinate values. For example, a location recorded in an NAD 1927 GCS having a coordinate pair of 44.56698° north and 69.65939° west will register a coordinate value of 44.56704° north and 69.65888° west in a NAD83 GCS and a coordinate value of 44.37465° north and -69.65888° west in a sphere based WGS84 GCS. If the coordinate systems for these point coordinate values were not properly defined, then they could be misplaced on a map. This is analogous to recording temperature using different units of measure (degrees Celsius, Fahrenheit and Kelvin)–each unit of measure will produce a different numeric value. Figure 4.1: Three points, each representing the same location but recorded in three different coordinate systems, are mapped in a same underlying coordinate system to highlight the different coordinate values a geographic location can acquire in differing coordinate systems. 9.2 Projected Coordinate Systems The surface of the earth is curved but maps are flat. A projected coordinate system (PCS) is a reference system for identifying locations and measuring features on a flat (map) surface. It consists of lines that intersect at right angles, forming a grid. Projected coordinate systems (which are based on Cartesian coordinates) have: an origin, a x and y axis, and a linear unit of measure. Going from a GCS to a PCS requires mathematical transformations. The myriad of projection types can be aggregated into three groups: planar, cylindrical and conical. 9.2.1 Planar Projections A planar projection (aka Azimuthal projection) maps the earth surface features to a flat surface that touches the earth’s surface at a point (tangent case), or along a line of tangency (a secant case). This projection is often used in mapping polar regions but can be used for any location on the earth’s surface (in which case they are called oblique planar projections). Figure 4.4: Examples of three planar projections: orthographic (left), gnomonic (center) and equidistant (right). Each covers a different spatial range (with the latter covering both northern and southern hemispheres) and each preserves a unique set of spatial properties. 9.2.2 Cylindrical Projection A cylindrical map projection maps the earth surface onto a map rolled into a cylinder (which can then be flattened into a plane). The cylinder can touch the surface of the earth along a single line of tangency (a tangent case), or along two lines of tangency (a secant case). The cylinder can be tangent to the equator or it can be oblique. A special case is the Transverse aspect which is tangent to lines of longitude. This is a popular projection used in defining the Universal Transverse Mercator (UTM) and State Plane coordinate systems. The UTM PCS covers the entire globe and is a popular coordinate system in the US. It’s important to note that the UTM PCS is broken down into zones and therefore limits its extent to these zones that are 6° wide. For example, the State of Maine uses the UTM coordinate system (Zone 19 North) for most of its statewide GIS maps. Most USGS quad maps are also presented in a UTM coordinate system. Popular datums tied to the UTM coordinate system include NAD27 and NAD83. There is also a WGS84 based UTM coordinate system. Distortion is minimized along the tangent or secant lines and increases as the distance from these lines increases. Figure 7.4: Examples of two cylindrical projections: Mercator (preserves shape but distortes area and distance) and equa-area (preserves area but distorts shape). 9.2.3 Conical Projection A conical map projection maps the earth surface onto a map rolled into a cone. Like the cylindrical projection, the cone can touch the surface of the earth along a single line of tangency (a tangent case), or along two lines of tangency (a secant case). Distortion is minimized along the tangent or secant lines and increases as the distance from these lines increases. When distance or area measurements are needed for the contiguous 48 states use one of the conical projections such as Equidistant Conic (distance preserving) or Albers Equal Area Conic (area preserving). Conical projections are also popular PCS’ in European maps such as Europe Albers Equal Area Conic and Europe Lambert Conformal Conic. Figure 9.1: Examples of three conical projections: Albers equal area (preserves area), equidistant (preserves distance) and conformal (preserves shape). 9.3 Spatial Properties All projections distort real-world geographic features to some degree. The four spatial properties on a map that are subject to distortion are: shape, area, distance and direction. A map that preserves shape is called conformal; one that preserves area is called equal-area; one that preserves distance is called equidistant; and one that preserves direction is called azimuthal. For most GIS applications (e.g. ArcGIS and QGIS), many of the built-in projections are named after the spatial properties they preserve. Each map projection is good at preserving only one or two of the four spatial properties. So when working with small-scale (large area) maps and multiple spatial properties are to be preserved, it is best to break the analyses across different projections to minimize error associated with spatial distortion. If you want to assess a projection’s spatial distortion across your study region, you can generate Tissot indicatrix (TI) ellipses. The idea is to project a small circle (i.e. small enough so that the distortion remains relatively uniform across the circle’s extent) and to measure its distorted shape on the projected map. For example, in assessing the type of distortion one could expect with a Mollweide projection across the continental US, a grid of circles could be generated at regular latitudinal and longitudinal intervals. Note the varying levels of distortion type and magnitude across the region. Let’s explore a Tissot circle at 44.5°N and 69.5°W (near Waterville Maine): The plot shows a perfect circle (displayed in a filled bisque color) that one would expect to see if no distortion was at play. The blue distorted ellipse (the indicatrix) is the transformed circle for this particular projection and location. The green and red lines show the magnitude and direction of the ellipse’s major and minor axes respectively. These lines can also be used to assess scale distortion (note that scale distortion can vary as a function of bearing). The green line shows maximum scale distortion and the red line shows minimum scale distortion–these are sometimes referred to as the principal directions. In this working example, the principal directions are 1.1293 and 0.8856. A scale value of 1 indicates no distortion. A value less than 1 indicates a smaller-than-true scale and a value greater than 1 indicates a greater-than-true scale. Projections can distort scale, but this does not necessarily mean that area is distorted. In fact, for this particular projection, area is relatively well preserved despite distortion in principal directions. Area distortion can easily be computed by taking the product of the two aforementioned principal directions. In this working example, area distortion is 1.0001 (i.e. negligible). The north-south dashed line in the graphic shows the orientation of the meridian. The east-west dotted line shows the orientation of the parallel. It’s important to recall that these distortions occur at the point where the TI is centered and not across the region covered by the TI plot. "],
["map-algebra.html", "Chapter 10 Map Algebra 10.1 Local operations and functions 10.2 Focal operations and functions 10.3 Zonal operations and functions 10.4 Global operations and functions 10.5 Operators and functions", " Chapter 10 Map Algebra Dana Tomlin is credited with defining a framework for the analysis of field data stored as gridded values (i.e. rasters). He coined this framework map algebra. Though gridded data can be stored in a vector format, map algebra is usually performed on raster data. Map algebra operations and functions are broken down into four groups: local, focal, zonal and global. Each is explored in the following sections. 10.1 Local operations and functions Local operations and functions are applied to each individual cell and only involve those cells sharing the same location. For example, if we start off with an original raster, then multiply it by 2 then add 1, we get a new raster whose cell values reflect the series of operations performed on the original raster cells. This is an example of a unary operation where just one single raster is involved in the operation. Figure 1.2: Example of a local operation where output=(2 * raster + 1). More than one raster can be involved in a local operation. For example, two rasters can be summed (i.e. each overlapping pixels are summed) to generate a new raster. Figure 1.3: Example of a local operation where output=(raster1+raster2). Note how each cell output only involves input raster cells that share the same exact location. Local operations also include reclassification of values. This is where a range of values from the input raster are assigned a new (common) value. For example, we might want to reclassify the input raster values as follows: Original values Reclassified values 0-25 25 26-50 50 51-75 75 76-100 100 Figure 2.3: Example of a local operation where the output results from the reclassification of input values. 10.2 Focal operations and functions Focal operations are also referred to as neighborhood operations. Focal operations assign to the output cells some summary value (such as the mean) of the neighboring cells from the input raster. For example, a cell output value can be the average of all 9 neighboring input cells (including the center cell); this acts as a smoothing function. Figure 2.4: Example of a focal operation where the output cell values take on the average value of neighboring cells from the input raster. If a focal cell is surrounded by non-existent cells, the output will be NA. Notice how, in the above example, the edge cells from the output raster have been assigned a value of NA (No Data). This is because cells outside of the extent have no value. Some GIS applications will ignore the missing surrounding values and just compute the average of the available cells as demonstrated in the next example. Figure 2.5: Example of a focal operation where the output cell values take on the average value of neighboring cells from the input raster. Surrounding non-existent cells are ignored. Focal (or neighbor) operations require that a window region (a kernel) be defined. In the above examples, a simple 3 by 3 kernel (or window) was used in the focal operations. The kernel can take on different dimensions and shape such as a 3 by 3 square where the central pixel is ignored (thus reducing the number of neighbors to 8) or a circular neighbor defined by a radius. Figure 2.6: Example of a focal operation where the kernel is defined by a 3 by 3 cell without the center cell and whose output cell takes on the average value of those neighboring cells. In addition to defining the neighborhood shape and dimension, a kernel also defines the weight each neighboring cell contributes to the summary statistic. For example, all cells in a 3 by 3 neighbor could each contribute 1/9th of their value to the summarized value (i.e. equal weight). But the weight can take on a more complex form defined by a function; such weights are defined by a kernel function. One popular function is a Gaussian weighted function which assigns greater weight to nearby cells than those further away. Figure 2.7: Example of a focal operation where the kernel is defined by a Gaussian function whereby the closest cells are assigned a greater weight. 10.3 Zonal operations and functions A zonal operation computes a new summary value (such as the mean) from cells aggregated for some zonal unit. In the following example, the cell values from the raster layer are aggregated into three zones whose boundaries are delineated in red. Each output zone shows the average cell value within that zone. Figure 4.1: Example of a zonal operation where the cell values are averaged for each of the three zones delineated in red. This technique is often used with rasters derived from remotely sensed imagery (e.g. NDVI) where areal units (such as counties or states) are used to compute the average cell values from the raster. 10.4 Global operations and functions Global operations and functions may make use of some or all input cells when computing an output cell value. An example of a global function is the Euclidean Distance tool which computes the shortest distance between a pixel and a source (or destination) location. In the following example, a new raster assigns to each cell a distance value to the closest cell having a value of 1 (there are just two such cells in the input raster). Figure 4.2: Example of a global function: the Euclidean distance. Each pixel is assigned its closest distance to one of the two source locations (defined in the input layer). Global operations and functions can also generate single value outputs such as the overall pixel mean or standard deviation. Another popular use of global functions is in the mapping of least-cost paths where a cost surface raster is used to identify the shortest path between two locations which minimizes cost (in time or money). 10.5 Operators and functions Operations and functions applied to gridded data can be broken down into three groups: mathematical, logical comparison and Boolean. 10.5.1 Mathematical operators and functions Two mathematical operators have already been demonstrated in earlier sections: the multiplier and the addition operators. Other operators include division and the modulo (aka the modulus) which is the remainder of a division. Mathematical functions can also be applied to gridded data manipulation. Examples are square root and sine functions. The following tables showcases a few examples with ArcGIS syntax. Operation Syntax Example Addition + R1 + R2 Subtraction - R1 - R2 Division / R1 / R2 Modulo Mod() Mod(R1, 100) Square root SquareRoot() SquareRoot(R1) 10.5.2 Logical comparison The logical comparison operators evaluate a condition then output a value of 1 if the condition is true and 0 if the condition is false. Logical comparison operators consist of greater than, less than, equal and not equal. Logical comparison Syntax Greater than &gt; Less than &lt; Equal == Not equal != For example, the following figure shows the output of the comparison between two rasters where we are assessing if cells in R1 are greater than those in R2 (on a cell-by-cell basis). Figure 4.3: Output of the operation R1&amp;R2. A value of 1 in the output raster indicates that the condition is true and a value of 0 indicates that the condition is false. When assessing whether two cells are equal, some programming environments such as R and ArcMap’s Raster Calculator require the use of the double equality syntax, ==, as in R1 == R2. In these programming environments, the single equality syntax is usually interpreted as an assignment operator so R1 = R2 would instruct the computer to assign the cell values in R2 to R1 (which is not what we want to do here). Some applications make use of special functions to test a condition. For example, ArcMap has a function called Con(condition, out1, out2) which assigns the value out1 if the condition is met and a value of out2 if it’s not. For example, ArcMap’s raster calculator expression Con( R1 &gt; R2, 1, 0) outputs a value of 1 if R1 is greater than R2 and 0 if not. It generates the same output as the one shown in the above figure. Note that in most programming environments (including ArcMap), the expression R1 &gt; R2 produces the same output because the value 1 is the numeric representation of TRUE and 0 of FALSE. 10.5.3 Boolean (or Logical) operators In map algebra, Boolean operators are used to compare conditional states of a cell (i.e. TRUE or FALSE). The three Boolean operators are AND, OR and NOT. Boolean ArcGIS R Example AND &amp; &amp; R1 &amp; R2 OR | | R1 | R2, R1 or R2 NOT ~ ! ~R2 A “TRUE” state is usually encoded as a 1 or any non-zero integer while a “FALSE” state is usually encoded as a 0. For example, if cell1=0 and cell2=1, the Boolean operation cell1 AND cell2 results in a FALSE (or 0) output cell value. This Boolean operation can be translated into plain English as “are the cells 1 and 2 both TRUE?” to which we answer “No they are not” (cell1 is FALSE). The OR operator can be interpreted as “is x or y TRUE?” so that cell1 OR cell2 would return TRUE. The NOT interpreter can be interpreted as “is x not TRUE?” so that NOT cell1 would return TRUE. Figure 4.4: Output of the operation R1 AND R2. A value of 1 in the output raster indicates that the condition is true and a value of 0 indicates that the condition is false. Figure 4.5: Output of the operation NOT R2. A value of 1 in the output raster indicates that the input cell is NOT TRUE (i.e. has a value of 0). 10.5.4 Combining operations Note that most software environments assign the AND Boolean operator the ampersand character, &amp;. Both comparison and Boolean operations can be combined into a single expression. For example, we may wish to find locations (cells) that satisfy requirements from two different raster layers: \\(0&lt;R1&lt;4\\) and \\(R2&gt;0\\). To satisfy the first requirement, we can write out the expression as (R1&gt;0) &amp; (R1&lt;4). Both comparisons (delimited by parentheses) return a 0 (FALSE) or a 1(TRUE). The ampersand, &amp;, is a Boolean operator that checks that both conditions are met and returns a 1 if yes or a 0 if not. This expression is then combined with another comparison using another ampersand operator that assesses if R2&gt;0 is TRUE or FALSE. The amalgamated expression is thus ((R1&gt;0) &amp; (R1&lt;4)) &amp; (R2&gt;0). Figure 7.3: Output of the operation ((R1&gt;0) &amp; (R1&lt;4)) &amp; (R2&gt;0). A value of 1 in the output raster indicates that the condition is true and a value of 0 indicates that the condition is false. "],
["point-pattern-analysis.html", "Chapter 11 Point Pattern Analysis 11.1 Centrography 11.2 Density based analysis 11.3 Distance based analysis 11.4 First and second order effects", " Chapter 11 Point Pattern Analysis 11.1 Centrography A very basic form of point pattern analysis involves summary statistics such as the mean center, standard distance and standard deviational ellipse. These point pattern analysis techniques were popular before computers were ubiquitous since hand calculations are not too involved, but these summary statistics are too concise and hide far more valuable information about the observed pattern. More powerful analysis methods can be used to explore point patterns. These methods can be classified into two groups: density based approach and distance based approach. 11.2 Density based analysis A first order property of a pattern concerns itself with the variation of the observations’ density across a study area. For example, the distribution of oaks will vary across a landscape based on underlying soil characteristics (resulting in areas having dense clusters of oaks and other areas not). Density based techniques characterize the pattern in terms of its distribution vis-a-vis the study area–a first-order property of the pattern. Here, we make a distinction between the intensity of a spatial process and the observed density of a pattern under study (which is often used to estimate the process’ intensity). A point pattern can be thought of as a “realization” of an underlying process whose intensity \\(\\lambda\\) is estimated from the observed point pattern’s density (which is sometimes denoted as \\(\\widehat{\\lambda}\\) where the caret \\(\\verb!^!\\) is referring to the fact that the observed density is an estimate of the underlying process’ intensity) . 11.2.1 Global density A basic measure of a pattern’s density \\(\\widehat{\\lambda}\\) is its overall, or global, density. This is simply the ratio of observed number of points, \\(n\\), to the study region’s surface area, \\(a\\), or: \\[ \\begin{equation} \\widehat{\\lambda} = \\frac{n}{a} \\label{eq:global-density} \\end{equation} \\] Figure 1.3: An example of a point pattern where n = 20 and the study area (defined by a square boundary) is 10 units squared. The point density is thus 20/100 = 0.2 points per unit area. 11.2.2 Local density A point pattern’s density can be measured at different locations within the study area. Such an approach helps us assess if the density–and, by extension, the underlying process’ local intensity \\(\\widehat{\\lambda}_i\\)–is constant across the study area. This can be an important property of the data since it may need to be mitigated for when using distance based analysis tools. Several techniques to measure local density are available, here we will focus on two such methods: quadrat density and kernel density. 11.2.2.1 Quadrat density This technique requires that the study area be divided into sub-regions (aka quadrats). Then, the point density is computed for each quadrat by dividing the number of points in each quadrat by the quadrat’s area. Quadrats can take on many different shapes such as hexagons and triangles, here we use square shaped quadrats to demonstrate the procedure. Figure 2.3: An example of a quadrat count where the study area is divided into four equally sized quadrats whose area is 25 square units each. The density in each quadrat can be computed by dividing the number of points in each quadrat by that quadrat’s area. The choice of quadrat numbers and quadrat shape can influence the measure of local density and must be chosen with care. If very small quadrat sizes are used you risk having many quadrats with no points which may prove uninformative. If very large quadrat sizes are used, you risk missing subtle changes in spatial density distributions such as the east-west gradient in density values in the above example. Converting a continuous field into discretized areas is sometimes referred to as tesselation. The end product is a tesselated surface. Quadrat regions do not have to take on a uniform pattern across the study area, they can also be defined based on a covariate. For example, if it’s believed that the underlying point pattern process is driven by elevation, quadrats can be defined by sub-regions such as different ranges of elevation values (labeled 1 through 4 on the right-hand plot in the following example). This can result in quadrats having non-uniform shape and area. Figure 2.5: Example of a covariate. Figure on the left shows the elevation map. Figure on the right shows elevation broken down into four sub-regions for which a local density will be computed. If the local intensity is the same across all regions of the tessellated covariate, then there is evidence of a dependence between the process that generated the point pattern and the covariate. In our example, sub-regions 1 through 4 have surface areas of 17.08, 50.45, 26.76, 5.71 respectively. To compute these regions’ point densities, we simply divide the number of points by the area. Figure 2.6: Figure on the left displays the number of points in each elevation sub-region (sub-regions are coded as values ranging from 1 to 4). Figure on the right shows the density of points (number of points divided by area of sub-region). The point density is not uniform across the sub-regions suggesting that elevation (as characterized by the sub-regions) may not fully explain the observed pattern. Note that how one chooses to tessellate a surface can have an influence on the resulting density values. For example, dividing the elevation into equal area sub-regions produces different density values. Figure 2.7: Same analysis as last figure using different sub-regions. Note the difference in density values. 11.2.2.2 Kernel density The kernel density approach is an extension of the quadrat method: Like the quadrat density, the kernel approach computes a localized density for subsets of the study area, but unlike its quadrat density counterpart, the sub-regions overlap one another providing a moving sub-region window. This moving window is defined by a kernel. Unlike its quadrat density counterpart, the kernel density approach generates a grid of density values whose cell size is smaller than that of the kernel window. Each cell is assigned the density value computed for the kernel window centered on that cell. Many software applications will usually generate a raster output to store the density values. A kernel not only defines the shape and size of the window, but it can also weight the points following a well defined kernel function. The simplest of functions is a basic kernel where each point in the kernel window is assigned equal weight. Figure 4.1: An example of a basic 3x3 kernel density map (ArcGIS calls this a point density map) where each point is assigned an equal weight. For example, the second cell from the top and left (i.e. centered at location x=1.5 and y =8.5) has one point within a 3x3 unit (pixel) region and thus has a local density of 1/9 = 0.11. Some of the most popular kernel functions assign weights to points that are inversely proportional to their distances to the kernel window center. A few such kernel functions follow a gaussian or quartic like distribution function. These functions tend to produce a smoother density map. Figure 4.2: An example of a kernel function is the 3x3 quartic kernel function where each point in the kernel window is weighted based on its proximity to the kernel’s center cell (typically, closer points are weighted more heavily). Kernel functions, like the quartic, tend to generate smoother surfaces. 11.2.2.3 Kernel Density Adjusted for Covariate In the previous section, we learned that we could use a covariate, like elevation, to define the sub-regions (quadrats) within which densities were computed. In essence a form of normalization was applied to the density calculations whereby each sub-region was assumed to represent a unique underlying process (if this were the case, then the density values would have been the same across each sub-region). The idea of normalizing the data to some underlying covariate can be extended to kernel density analysis. Here, instead of dividing the study region into discrete sub-regions (as was done with quadrat analysis), we normalize the computed density (from our observed point pattern) to a measure of density expected from the underlying covariate. This normalized density, which we’ll denote as \\(\\rho\\), can be estimated in one of three different ways– by ratio, re-weight and transform methods. We will not delve into the differences between these methods, but note that there is more than one way to estimate \\(\\rho\\). Figure 4.3: An estimate of \\(\\rho\\) using the ratio method. The figure on the left shows the estimated \\(\\rho\\) as a function of elevation. The envelope shows the 95% confidence interval. The figure on the right shows the spatial distribution of \\(\\rho\\). If the density, \\(\\rho\\), can be explained by the elevation, we would expect a constant \\(\\rho\\) value across all elevation values (figure on the left) and across the spatial extent (figure on the right). This appears to be the case with our working example except near the upper right-hand corner of the spatial extent where the elevation values are the highest. This can be explained by the small area covered by these high elevation locations which result in fewer observed points and thus higher uncertainty for that corner of the study extent. This is very apparent in the left figure where the 95% confidence interval envelope widens at higher elevation values (indicating the greater uncertainty in our estimated \\(\\rho\\) value at the higher elevation). 11.2.3 Modeling intensity as a function of a covariate So far, we have learned techniques that describe the distribution of points across a region of interest. But it is often more interesting to model the relationship between the distribution of points and some underlying covariate by defining that relationship mathematically. This can be done by exploring the changes in point density as a function of a covariate, however, unlike techniques explored thus far, this approach makes use of a statistical model. One such model is a Poisson point process model which can take on the form of: \\[ \\begin{equation} \\lambda(i) = e^{\\alpha + \\beta Z(i)} \\label{eq:density-covariate} \\end{equation} \\] The left-hand side of a logisitic regression model is often presented as the probability, P, of occurance and is related to λ as \\(\\lambda=P/(1-P)\\) which is the ratio of probability of occurence. Solving for P gives us \\(P = \\lambda/(1 + \\lambda)\\) which yields the following equation: \\[ P(i) = \\frac{e^{\\alpha + \\beta Z(i)}}{1 + e^{\\alpha + \\beta Z(i)}} \\] where \\(\\lambda(i)\\) is the modeled intensity at location \\(i\\), \\(e^{\\alpha}\\) (the exponent of \\(\\alpha\\)) is the base intensity when the covariate is zero and \\(e^{\\beta}\\) is the multiplier by which the intensity increases (or decreases) for each 1 unit increase in the covariate. This is a form of the logistic regression model–popular in the field of statistics. This equation implies that the relationship between the process that lead to the observed point pattern is a loglinear function of the underlying covariate (i.e. one where the process’ intensity is exponentially increasing or decreasing as a function of the covariate). Note that taking the log of both sides of the equation yields the more familiar linear regression model where \\(\\alpha + \\beta Z(i)\\) is the linear predictor. Let’s work with the point distribution of Starbucks cafes in the state of Massachusetts. The point pattern clearly exhibits a non-random distribution. It might be helpful to compare this distribution to some underlying covariate such as the population density distribution. Figure 4.5: Location of Starbucks relative to population density. Note that the classification scheme follows a log scale to more easily differentiate population density values. We can fit a poisson point process model to these data where the modeled intensity takes on the form: \\[ \\begin{equation} Starbucks\\ density(i) = e^{\\alpha + \\beta\\ population(i)} \\label{eq:walmart-model} \\end{equation} \\] The parameters \\(\\alpha\\) and \\(\\beta\\) are estimated from a method called maximum likelihood. Its implementation is not covered here but is widely covered in many statistics text books. And the term \\((i)\\) serves as a reminder that the point density and the population distribution both can vary as a function of location \\(i\\). The estimated value for \\(\\alpha\\) is -18.966. This is interpreted as stating that given a population density of zero, the base intensity of the point process is e-18.966 or 5.79657e-09–a number close to zero (as one would expect). The estimated value for \\(\\beta\\) is 0.00017. This is interpretated as stating that for every unit increase in population density (one person per square mile), the intensity of the point process increases by e0.00017 or 1.00017. If one were to plot the relationship between density and population, we would get: Figure 7.4: Poisson point process model fitted to the relationship between Starbucks store locations and population density. The model assumes a loglinear relationship. Note that the density is reported in number of stores per map unit area (the map units are in meters). 11.3 Distance based analysis A second order property of a pattern concerns itself with the observations’ influence on one another. For example, the distribution of oaks will be influenced by the location of parent trees: where parent oaks are present we would expect dense clusters of oaks to emerge. An alternative to density based methods is distance based methods for pattern analysis where the interest lies in how the points are distributed relative to one another–a second-order property of the pattern. 11.3.1 Average Nearest Neighbor An average nearest neighbor (ANN) analysis measures the average distance from each point in the study area to its nearest point. In the following example, the average nearest neighbor for all points is 1.52 units. Figure 11.1: Distance between each point and its closest point. For example, the point closest to point 1 is point 9 which is 2.32 units away. An extension of this idea is to plot the ANN values for different order neighbors, that is for the first closest point, then the second closest point, and so forth. Figure 9.1: ANN values for different neighbor order numbers. For example, the ANN for the first closest neighbor is 1.52 units; the ANN for the 2nd closest neighbor is 2.14 units; and so forth. The shape of the ANN curve as a function of neighbor order can provide insight into the spatial arrangement of points relative to one another. In the following example, three different point patterns of 20 points are presented. Figure 11.2: Three different point patterns: a single cluster, a dual cluster and a randomly scattered pattern. Each point pattern offers different ANN vs. neighbor order plots. Figure 11.3: Three different ANN vs. neighbor order plots. The black ANN line is for the first point pattern (single cluster); the blue line is for the second point pattern (double cluster) and the red line is for the third point pattern. The bottom line (black line) indicates that the cluster (left plot) is tight and that the distances between a point and all other points is very short. This is in stark contrast with the top line (red line) which indicates that the distances between points is much greater. Note that the way we describe these patterns is heavily influenced by the size and shape of the study region. If the region was defined as the smallest rectangle encompassing the cluster of points, the cluster of points would no longer look clustered. Figure 11.4: The same point pattern presented with two different study areas. How differently would you describe the point pattern in both cases? 11.3.2 K and L functions 11.3.2.1 K function The average nearest neighbor (ANN) statistic is one of many point pattern analysis statistics. Another statistic is the K-function which summarizes the distance between points for all distances. The calculation of K is fairly simple: it consists of dividing the mean sum of the number of points at different distance lags for each point by the area event density. For example, for point \\(S1\\) we draw circles, each of varying radius \\(d\\), centered on that point. We then count the number of points (events) inside each circle. We repeat this for point \\(S2\\) and all other points \\(Si\\). Next, we compute the average number of points in each circle then divide that number by the overall point density \\(\\hat{\\lambda}\\) (i.e. total number of events per study area). Distance band (km) # events from S1 # events from S2 # events from Si K 10 0 1 … 0.012 20 3 5 … 0.067 30 9 14 … 0.153 40 17 17 … 0.269 50 25 23 … 0.419 We can then plot K and compare that plot to a plot we would expect to get if an IRP/CSR process was at play (Kexpected). K-function calculated from the Walmart stores point distribution in MA (shown in black) compared toKexpected under the IRP/CSR assumption (shown in red). K values greater than \\(K_{expected}\\) indicates a greater number of points at that distance (more clustered); K values less than \\(K_{expected}\\) indicates fewer number of points at that distance (more dispersed). Note that like the ANN analysis, the \\(K\\)-function assumes stationarity in the underlying point process (i.e. that there is no overall drift or trend in the process’ intensity). 11.3.2.2 L function One problem with the \\(K_{expected}\\) is that the shape of the function tends to curve upward making it difficult to see small differences between K and \\(K_{expected}\\). A work-around is to transform the values in such a way that the expected values, \\(K_{expected}\\), lie horizontal. The transformation is calculated as follows: \\[ \\begin{equation} L=\\sqrt{\\dfrac{K(d)}{\\pi}}-d \\label{eq:L-function} \\end{equation} \\] The above \\(\\hat{K}\\) is transformed to the following plot (note how the \\(K_{expecetd}\\) red line is now perfectly horizontal) Figure 11.5: L-function (a simple transformation of the K-function). This graph makes it easier to compare K with Kexpected at lower distance values. It appears that Walmart locations are more clustered than expected under CSR/IRP up to a distance of 12 km but more dispersed at distances greater than 12 km. 11.3.3 The Pair Correlation Function \\(g\\) A shortcoming of the \\(K\\) function (and by extension the \\(L\\) function) is its cumulative nature which makes it difficult to know at exactly which distances a point pattern may stray from \\(K_{expected}\\) since all points up to distance \\(r\\) can contribute to \\(K(r)\\). The pair correlation function, \\(g\\), is a modified version of the \\(K\\) function where instead of summing all points within a distance \\(r\\), points falling within a narrow distance band are summed instead. Figure 11.6: Difference in how the \\(K\\) and \\(g\\) functions aggregate points at distance \\(r\\) (\\(r\\) = 30 km in this example). All points up to \\(r\\) contribute to \\(K\\) whereas just the points in the annulus band at \\(r\\) contribute to \\(g\\). The plot and interpretation of the \\(g\\) function differs from that of the \\(K\\) and \\(L\\) functions. Figure 11.7: \\(g\\)-function of the Massachusets Walmart point data. Its interpretation is similar to that of the \\(K\\) and \\(L\\) functions. Here, we observe distances between stores greater than expected under CSR up to about 5 km. Note that this cutoff is less than the 12 km cutoff observed with the \\(K\\) function; this can be explained by the point pattern at the shorter \\(r\\) distances contributing to the \\(K\\) values at higher \\(r\\) values–a problem avoided using the \\(g\\)-function. If \\(g(r)\\) = 1, then the inter-point distances (at and around distance \\(r\\)) are consistant with CSR. If \\(g(r)\\) &gt; 1, then the points are more clustered than expected under CSR. If \\(g(r)\\) &lt; 1, then the points are more dispersed than expected under CSR. Note that \\(g\\) can never be less than 0. Like its \\(K\\) counterpart, the \\(g\\)-function assumes stationarity in the underlying point process (i.e. that there is no overall drift or trend in the process’ intensity). 11.4 First and second order effects The concept of 1st order effects and 2nd order effects is an important one. It underlies the basic principles of spatial analysis. Figure 11.8: Tree distribution can be influenced by 1st order effects such as elevation gradient of spatial distribution of soil characteristics; this, in turn, changes the tree density distribution across the study area. Tree distribution can also be influenced by 2nd order effects such as seed dispersal processes where the process is independent of location and, instead, dependent on the presence of other trees. Density based measurements such as kernel density estimations look at the 1st order property of the underlying process. Distance based measurements such as ANN and K-functions focus on the 2nd order property of the underlying process. It’s important to note that it is seldom feasible to separate out the two effects when analyzing the data, thus the importance on relying on a priori knowledge of the phenomena being investigated before drawing any conclusions from the analyses results. "],
["hypothesis-testing.html", "Chapter 12 Hypothesis testing 12.1 IRP/CSR 12.2 Testing for CSR with the ANN tool 12.3 Alternatives to CSR/IRP 12.4 Monte Carlo test with K and L functions 12.5 Testing for a covariate effect", " Chapter 12 Hypothesis testing 12.1 IRP/CSR Figure 2.2: Could the distribution of Walmart stores in MA have been the result of a CSR/IRP process? Popular spatial analysis techniques compare observed point patterns to ones generated by an independent random process (IRP) also called complete spatial randomness (CSR). CSR/IRP satisfy two conditions: Any event has equal probability of being in any location, a 1st order effect. The location of one event is independent of the location of another event, a 2nd order effect. 12.2 Testing for CSR with the ANN tool 12.2.1 ArcGIS’ Average Nearest Neighbor Tool ArcMap offers a tool (ANN) that tests whether or not the observed first order nearest neighbor is consistent with a distribution of points one would expect to observe if the underlying process was completely random (i.e. IRP). But as we will learn very shortly, ArcMap’s ANN tool has its limitations. 12.2.1.1 A first attempt Figure 1.3: ArcGIS’ ANN tool. The size of the study area is not defined in this example. ArcGIS’ average nearest neighbor (ANN) tool computes the 1st nearest neighbor mean distance for all points. It also computes an expected mean distance (ANNexpected) under the assumption that the process that lead to the observed pattern is completely random. ArcGIS’ ANN tool offers the option to specify the study surface area. If the area is not explicitly defined, ArcGIS will assume that the area is defined by the smallest area encompassing the points. ArcGIS’ ANN analysis outputs the nearest neighbor ratio computed as: \\[ ANN_{ratio}=\\dfrac{ANN}{ANN_{expected}} \\] Figure 2.3: ANN results indicating that the pattern is consistent with a random process. Note the size of the study area which defaults to the point layer extent. If ANNratio is 1, the pattern results from a random process. If it’s greater than 1, it’s dispersed. If it’s less than 1, it’s clustered. In essence, ArcGIS is comparing the observed ANN value to ANNexpected one would compute if a complete spatial randomness (CSR) process was at play. ArcGIS’ tool also generates a p-value (telling us how confident we should be that our observed ANN value is consistent with a perfectly random process) along with a bell shaped curve in the output graphics window. The curve serves as an infographic that tells us if our point distribution is from a random process (CSR), or is more clustered/dispersed than one would expect under CSR. For example, if we were to run the Massachusetts Walmart point location layer through ArcGIS’ ANN tool, an ANNexpected value of 12,249 m would be computed along with an ANNratio of 1.085. The software would also indicate that the observed distribution is consistent with a CSR process (p-value of 0.28). But is it prudent to let the software define the study area for us? How does it know that the area we are interested in is the state of Massachusetts since this layer is not part of any input parameters? 12.2.1.2 A second attempt Figure 2.4: ArcGIS’ ANN tool. The size of the study is defined in this example. Here, we explicitly tell ArcGIS that the study area (Massachusetts) covers 21,089,917,382 m² (note that this is the MA shapefile’s surface area and not necessarily representative of MA’s actual surface area). ArcGIS’ ANN tool now returns a different output with a completely different conclusion. This time, the analysis suggests that the points are strongly dispersed across the state of Massachusetts and the very small p-value (p = 0.006) tells us that there is less than a 0.6% chance that an ANN value from a CSR point pattern would have been more extreme than the one we observed with the Walmart point pattern. Figure 2.5: ArcGIS’ ANN tool output. Note the different output result with the study area size defined. The output indicates that the points are more dispersed than expected under IRP. So how does ArcGIS estimate the ANNexpected value under CSR? It does so by taking the inverse of the square root of the number of points divided by the area, and multiplying this quotient by 0.5. \\[ ANN_{Expected}=\\dfrac{0.5}{\\sqrt{n/A}} \\] In other words, the expected ANN value under a CSR process is solely dependent on the number of points and the study extent’s surface area. Do you see a problem here? Could different shapes encompassing the same point pattern have the same surface area? If so, shouldn’t the shape of our study area be a parameter in our ANN analysis? Unfortunately, ArcGIS’ ANN tool cannot take into account the shape of the study area. An alternative work flow is outlined in the next section. 12.2.2 A better approach: a Monte Carlo test The Monte Carlo technique involves three steps: First, we postulate a process–our null hypothesis, \\(Ho\\). For example, we hypothesize that the distribution of Walmart stores is consistent with a completely random process (CSR). Next, we simulate many realizations of our postulated process and compute a statistic (e.g. ANN) for each realization. Finally, we compare our observed data to the patterns generated by our simulated processes and assess (via a measure of probability) if our pattern is a likely realization of the hypothesized process. Following our working example, we randomly re-position the location of our Walmart points 1000 times (or as many times computationally practical) following a completely random process–our hypothesized process, \\(Ho\\)–while making sure to keep the points confined to the study extent (the state of Massachusetts). Figure 2.6: Three different outcomes from simulated patterns following a CSR point process. These maps help answer the question how would Walmart stores be distributed if their locations were not influenced by the location of other stores and by any local factors (such as population density, population income, road locations, etc…) For each realization of our process, we compute an ANN value. Each simulated pattern results in a different ANNexpected value. We plot all ANNexpected values using a histogram (this is our \\(Ho\\) sample distribution), then compare our observed ANN value of 13,294 m to this distribution. Figure 2.7: Histogram of simulated ANN values (from 1000 simulations). This is the sample distribution of the null hypothesis, ANNexpected (under CSR). The red line shows our observed (Walmart) ANN value. About 32% of the simulated values are greater (more extreme) than our observed ANN value. Note that by using the same study region (the state of Massachusetts) in the simulations we take care of problems like study area boundary and shape issues since each simulated point pattern is confined to the exact same study area each and every time. 12.2.2.1 Extracting a \\(p\\)-value from a Monte Carlo test The p-value can be computed from a Monte Carlo test. The procedure is quite simple. It consists of counting the number of simulated test statistic values more extreme than the one observed (in our example, counting the number of simulated ANN values that were more extreme than ours). If we are interested in knowing the probability of having simulated values more extreme than ours, we identify the side of the distribution of simulated values closest to our observed statistic, count the number of simulated values more extreme than the observed statistic then compute \\(p\\) as follows: \\[ \\dfrac{N_{extreme}+1}{N+1} \\] where Nextreme is the number of simulated values more extreme than our observed statistic and N is the total number of simulations. Note that this is for a one-sided test. A practical and more generalized form of the equation looks like this: \\[ \\dfrac{min(N_{greater}+1 , N + 1 - N_{greater})}{N+1} \\] where \\(min(N_{greater}+1 , N + 1 - N_{greater})\\) is the smallest of the two values \\(N_{greater}+1\\) and \\(N + 1 - N_{greater}\\), and \\(N_{greater}\\) is the number of simulated values greater than the observed value. It’s best to implement this form of the equation in a scripting program thus avoiding the need to visually seek the side of the distribution closest to our observed statistic. For example, if we ran 1000 simulations in our ANN analysis and found that 319 of those were more extreme (on the right side of the simulated ANN distribution) than our observed ANN value, our p-value would be (319 + 1) / (1000 + 1) or p = 0.32. This is interpreted as “there is a 32% probability that we would be wrong in rejecting the null hypothesis Ho.” This suggests that we would be remiss in rejecting the null hypothesis that a CSR process could have generated our observed Walmart point distribution. But this is not to say that the Walmart stores were in fact placed across the state of Massachusetts randomly (it’s doubtful that Walmart executives make such an important decision purely by chance), all we are saying is that a CSR process could have been one of many processes that generated the observed point pattern. If a two-sided test is desired, then the equation for the \\(p\\) value takes on the following form: \\[ 2 \\times \\dfrac{min(N_{greater}+1 , N + 1 - N_{greater})}{N+1} \\] where we are simply multiplying the one-sided p-value by two. 12.3 Alternatives to CSR/IRP Figure 4.1: Walmart store distribution shown on top of a population density layer. The assumption of CSR is a good starting point, but it’s often unrealistic. Most real-world processes exhibit 1st and/or 2nd order effects. Figure 4.2: Examples of two randomly generated point patterns using population density as the underlying process. We can simulate the placement of Walmart stores using the population density layer as our inhomogeneous point process. We can test this hypothesis by generating random points that follow the population density distribution. Note that even though we are not referring to a CSR/IRP point process, we are still treating this as a random point process since the points are randomly located following the underlying population density distribution. Using the same Monte Carlo (MC) techniques used with IRP/CSR processes, we can simulate thousands of point patterns (following the population density) and compare our observed ANN value to those computed from our MC simulations. In this example, our observed ANN value falls far to the right of our simulated ANN values indicating that our points are more dispersed than would be expected had population density distribution been the sole driving process. The percentage of simulated values more extreme than our observed value is 0% (i.e. a p-value \\(\\backsimeq\\) 0.0). Another plausible hypothesis is that median household income could have been the sole factor in deciding where to place the Walmart stores. Figure 4.4: Walmart store distribution shown on top of a median income distribution layer. Running a MC simulation using median income distribution as the underlying density layer yields an ANN distribution where about 16% of the simulated values are more extreme than our observed ANN value (i.e. p-value = 0.16): Note that we now have two competing hypotheses: a CSR/IRP process and median income distribution. Both cannot be rejected. This serves as a reminder that a hypothesis test cannot tell us if a particular process is the process involved in the generation of our observed point pattern; instead, it tells us that the hypothesis is one of many plausible processes. It’s important to remember that the ANN tool is a distance based approach to point pattern analysis. Though we are randomly generating points following some underlying probability distribution map (think of this map as representing the intensity of the underlying process) we are still concerning ourselves with the repulsive/attractive forces that might dictate the placement of Walmarts relative to one another. 12.4 Monte Carlo test with K and L functions MC techniques are not unique to average nearest neighbor analysis. In fact, they can be implemented with many other statistical measure as with the K and L functions. However, unlike the ANN analysis, the K and L functions consist of multiple test statistics (one for each distance \\(r\\)). This results in not one but \\(r\\) number of simulated distributions. Typically, these distributions are presented as envelopes superimposed on the estimated \\(K\\) or \\(L\\) functions. However, since we cannot easily display the full distribution at each \\(r\\) interval, we usually limit the envelope to a pre-defined acceptance interval. For example, if we choose a two-sided significance level of 0.05, then we eliminate the smallest and largest 2.5% of the simulated K values computed for for each \\(r\\) intervals (hence the reason you might sometimes see such envelopes referred to as pointwise envelopes). This tends to generate a saw-tooth like envelope. Figure 7.3: Simulation results for the IRP/CSR hypothesized process. The gray envelope in the plot covers the 95% significance level. If the observed L lies outside of this envelope at distance \\(r\\), then there is less than a 5% chance that our observed point pattern resulted from the simulated process at that distance. The interpretation of these plots is straight forward: if \\(\\hat K\\) or \\(\\hat L\\) lies outside of the envelope at some distance \\(r\\), then this suggests that the point pattern may not be consistent with \\(H_o\\) (the hypothesized process) at distance \\(r\\) at the significance level defined for that envelope (0.05 in this example). One important assumption underlying the K and L functions is that the process is uniform across the region. If there is reason to believe this not to be the case, then the K function analysis needs to be controlled for inhomogeneity in the process. For example, we might hypothesize that population density dictates the density distribution of the Walmart stores across the region. We therefore run an MC test by randomly re-assigning Walmart point locations using the population distribution map as the underlying point density distribution (in other words, we expect the MC simulation to locate a greater proportion of the points where population density is the greatest). Figure 7.4: Simulation results for an inhomogeneous hypothesized process. When controlled for population density, the significance test suggests that the inter-distance of Walmarts is more dispersed than expected under the null up to a distance of 30 km. It may be tempting to scan across the plot looking for distances \\(r\\) for which deviation from the null is significant for a given significance value then report these findings as such. For example, given the results in the last figure, we might not be justified in stating that the patterns between \\(r\\) distances of 5 and 30 are more dispersed than expected at the 5% significance level but at a higher significance level instead. This problem is referred to as the multiple comparison problem–details of which are not covered here. 12.5 Testing for a covariate effect Any Poisson point process model can be fit to an observed point pattern, but just because we can fit a model does not imply that the model does a good job in explaining the observed pattern. To test how well a model can explain the observed point pattern, we need to compare it to a base model (such as one where we assume that the points are uniformly distributed across the study area–i.e. IRP). The latter is defined as the null hypothesis and the former is defined as the alternate hypothesis. For example, we may want to assess if the Poisson point process model that pits the placement of Walmarts as a function of population distribution (the alternate hypothesis) does a better job than the null model that assumes homogeneous intensity (i.e. a Walmart has no preference as to where it is to be placed). This requires that we first derive estimates for both models. A Poisson point process model implemented in a statisitcal software such as R produces the following output for the null model (using the Walmart data), PPM0 Stationary Poisson process Intensity: 2.1276e-09 Estimate S.E. CI95.lo CI95.hi Ztest Zval log(lambda) -19.96827 0.1507557 -20.26375 -19.6728 *** -132.4545 and the following output for the alternate model. PPM1 Nonstationary Poisson process Log intensity: ~pop Fitted trend coefficients: (Intercept) pop -2.007063e+01 1.043115e-04 Estimate S.E. CI95.lo CI95.hi Ztest (Intercept) -2.007063e+01 1.611991e-01 -2.038657e+01 -1.975468e+01 *** pop 1.043115e-04 3.851572e-05 2.882207e-05 1.798009e-04 ** Zval (Intercept) -124.508332 pop 2.708284 Problem: Values of the covariate &#39;pop&#39; were NA or undefined at 0.7% (4 out of 572) of the quadrature points Thus, the null model (homogeneous intensity) takes on the form: \\[ \\lambda(i) = e^{-19.96} \\] and the alternate model takes on the form: \\[ \\lambda(i) = e^{-20.1 + 1.04^{-4}population} \\] The models are then compared using the likelihood ratio test which produces the following output: Npar Df Deviance Pr(&gt;Chi) 5 NA NA NA 6 1 4.253072 0.0391794 The value under the heading PR(&gt;Chi) is the p-value which gives us the probability we would be wrong in rejecting the null. Here p=0.039 suggesting that there is an 3.9% chance that we would be remiss to reject the base model in favor of the alternate model–put another way, the alternate model may be an improvement over the null (note that what is deemed significant can be subjective and will vary based on circumstances). Note that if the p-value was very small (such as 0.001), then we would be far more confident in rejecting the null model in favor of the alternate model. "],
["spatial-autocorrelation.html", "Chapter 13 Spatial Autocorrelation 13.1 Global Moran’s I 13.2 Moran’s I at different lags 13.3 Local Moran’s I", " Chapter 13 Spatial Autocorrelation “The first law of geography: Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler 1970) Mapped events or entities can have non-spatial information attached to them (some GIS software tag these as attributes). A question often asked is whether or not features with similar values are clustered, randomly distributed or dispersed. In most cases, the distribution of attribute values will seldom show evidence of complete spatial randomness. Though our visual senses can, in some cases, discern clustered regions from non-clustered regions, the distinction may not always be so obvious. We must therefore come up with a quantitative and objective approach to quantifying the degree to which similar features cluster and where such clustering occurs. One popular test of spatial autocorrelation is the Moran’s I test. 13.1 Global Moran’s I 13.1.1 Computing the Moran’s I Let’s start with a working example: 2010 per capita income for the state of Maine. Figure 1.3: 2010 median per capita income aggregated at the county level. It may seem apparent that, when aggregated at the county level, the income distribution follows a north-south trend (i.e. high values appear clustered near the southern end of the state and low values seem clustered near the north and east). But a qualitative description may not be sufficient; we might want to quantify the degree to which similar (or dissimilar) counties are clustered. One measure of this type or relationship is the Moran’s I statistic. The Moran’s I index is the correlation coefficient for the relationship between a variable (like income) and its surrounding values. But before we go about computing this correlation, we need to come up with a way to define a neighbor. One approach is to define a neighbor as being any contiguous polygon. For example, the northern most county (Aroostook), has four contiguous neighbors while the southern most county (York) has two contiguous counties. Other neighborhood definitions include distance bands (e.g. counties within 100 km) and k nearest neighbors (e.g. the 2 closest neighbors). Note that distance bands and k nearest neighbors are usually measured using the polygon’s centroids and not their boundaries. Figure 2.3: Maps show the links between each polygon and their respective neighbor(s) based on the neighborhood definition. A contiguous neighbor is defined as one that shares a boundary or a vertex with the polygon of interest. Orange numbers indicate the number of neighbors for each polygon. Note that the top most county has no neighbors when a neighborhood definition of a 100 km distance band is used (i.e. no centroids are within a 100 km search radius) Once we define a neighborhood for our analysis we identify the neighbors for each polygon in our dataset then summaries the values for each neighborhood cluster (by computing their mean values, for example). This summarized neighbor value is sometimes referred to as a lagging value (Xlag). In our working example, we adopt a contiguity neighborhood and compute the average neighboring income value (Incomelag) for each county in our dataset. We then plot Incomelag vs. Income for each county. The Moran’s I coefficient between Incomelag and Income is nothing more than the slope of the least squares regression line that best fits the points after having equalized the spread between both sets of data. Figure 2.4: Scatter plot of Incomelag vs. Income. If we equalize the spread between both axes (i.e. convert to a z-score) the slope of the regression line represents the Moran’s I statistic. If there is no relationship between Income and Incomelag, the slope will be close to flat (resulting in a Moran’s I value near 0). In our working example, the Moran’s I value is 0.377. So this begs the question, how significant is this Moran’s I value (i.e. is the computed slope significantly different from 0)? There are two approaches to estimating the significance: an analytical solution and a Monte Carlo solution. The analytical solution makes some restrictive assumptions about the data and thus cannot always be reliable. Another approach (and the one favored here) is a Monte Carlo test which makes no assumptions about the dataset including the shape and layout of each polygon. 13.1.2 Monte Carlo approach to estimating significance In a Monte Carlo test (a permutation bootstrap test, to be exact), the attribute values are randomly assigned to polygons in the data set and for each permutation of the attribute values, a Moran’s I value is computed. The output is a sampling distribution of Moran’s I values under the (null) hypothesis that attribute values are randomly distributed across the study area. We then compare our observed Moran’s I value to this sampling distribution. Figure 2.5: Results from 199 permutations. Left plot shows Moran’s I slopes (in gray) from each random permutation of income values superimposed with the observed Moran’s I slope. Right plot shows the distribution of Moran’s I values for all 199 permutations; red vertical line shows our observed Moran’s I value of 0.377. In our working example, 199 simulations indicate that out observed Moran’s I value of 0.377 is not a value we would expect to compute if the income values were randomly distributed across each county. A (pseudo) P-value can easily be computed from the simulation results: \\[ \\dfrac{N_{extreme}+1}{N+1} \\] where \\(N_{extreme}\\) is the number of simulated Moran’s I values more extreme than our observed statistic and \\(N\\) is the total number of simulations. In our working example of 199 simulations where just one simulation result is more extreme than our observed statisic (i.e. \\(N_{extreme}\\) = 1), \\(p\\) = (1 + 1) / (199 + 1) = 0.01. This is interpreted as “there is a 1% probability that we would be wrong in rejecting the null hypothesis Ho.” 13.2 Moran’s I at different lags So far we have looked at spatial autocorrelation where we define neighbors as all polygons sharing a boundary with the polygon of interest. We may also be interested in studying the ranges of autocorrelation values as a function of distance. The steps for this type of analysis are straightforward: Compute lag values for a defined set of neighbors. Calculate the Moran’s I value for this set of neighbors. Repeat steps 1 and 2 for a different set of neighbors (at a greater distance for example) . For example, the Moran’s I values for income distribution in the state of Maine at distances of 25, 75, 125, up to 375 km are presented in the following plot: Figure 2.6: Moran’s I at different spatial lags defined by a 5 km width annulus at 50 km distance increments. Red dots indicate Moran I values for which a P-value was 0.05 or less. The plot suggests that there is significant spatial autocorrelation between counties within 25 km of one another, but as the distances between counties increases, autocorrelation shifts from being positive to being negative meaning that at greater distances, counties tend to be more dissimilar. 13.3 Local Moran’s I We can decompose the global Moran’s I down to its components thus constructing a localized measure of autocorrelation–i.e. a map of “hot spots” and “cold spots”. Figure 2.7: Red points and polygons highlight counties with high income values surrounded by high income counties. Blue points and polygons highlight counties with low income values surrounded by low income counties. Next, we can use Monte Carlo techniques to assess whether or not the High-High values and Low-Low values are significant at a confidence level of 0.05 (i.e. for P-value= 0.05). Figure 4.1: Significantly High-High and Low-Low clusters with P-values less than or equal to 0.5. References "],
["spatial-interpolation.html", "Chapter 14 Spatial Interpolation 14.1 Deterministic Approach to Interpolation 14.2 Statistical Approach to Interpolation", " Chapter 14 Spatial Interpolation Given a distribution of point meteorological stations showing precipitation values, how I can I estimate the precipitation values where data were not observed? Figure 2.2: Average yearly precipitation (reported in inches) for several meteorological sites in Texas. To help answer this question, we need to clearly define the nature of our point dataset. We’ve already encountered point data earlier in the course where our interest was in creating point density maps using different kernel windows. However, the point data used represented a complete enumeration of discrete events or observations–i.e. the entity of interest only occurred a discrete locations within a study area and therefore could only be measured at those locations. Here, our point data represents sampled observations of an entity that can be measured anywhere within our study area. So creating a point density raster from this data would only make sense if we were addressing the questions like “where are the meteorological stations concentrated within the state of Texas?”. Another class of techniques used with points that represent samples of a continuous field are interpolation methods. There are many interpolation tools available, but these tools can usually be grouped into two categories: deterministic and statistical interpolation methods. 14.1 Deterministic Approach to Interpolation We will explore two deterministic methods: proximity (aka Thiessen) techniques and inverse distance weighted techniques. (IDW for short). 14.1.1 Proximity interpolation This is probably the simplest (and possibly one of the oldest) interpolation method. It was introduced by Alfred H. Thiessen more than a century ago. The goal is simple: Assign to all unsampled locations the value of the closest sampled location. This generates a tessellated surface whereby lines that split the midpoint between each sampled location are connected thus enclosing an area. Each area ends up enclosing a sample point whose value it inherits. Figure 1.2: Tessellated surface generated from discrete point samples. This is also known as a Thiessen interpolation. One problem with this approach is that the surface values change abruptly across the tessellated boundaries. This is not representative of most surfaces in nature. Thiessen’s method was very practical in his days when computers did not exist. But today, computers afford us more advanced methods of interpolation as we will see next. 14.1.2 Inverse Distance Weighted (IDW) The IDW technique computes an average value for unsampled locations using values from nearby weighted locations. The weights are proportional to the proximity of the sampled points to the unsampled location and can be specified by the IDW power coefficient. In the following figure, the sampled points and values are superimposed on top of the (IDW) interpolated raster for reference. Figure 1.3: An IDW interpolation of the average yearly precipitation (reported in inches) for several meteorological sites in Texas. An IDW power coefficient of 2 was used in this example. IDW interpolation is probably one of the most widely used interpolators because of its simplicity. In many cases, it can do an adequate job. However, the choice of power can be quite subjective. There is another class of interpolators that makes use of the information provided to us by the sample points–more specifically, information pertaining to 1st and 2nd order behavior. 14.1.3 Fine tuning the interpolation parameters Finding the best set of input parameters to create an interpolated surface can be a subjective proposition. Other than eyeballing the results, how can you quantify the accuracy of the estimated values? One option is to split the points into two sets: the points used in the interpolation operation and the points used to validate the results. While this method is easily implemented (even via a pen and paper adoption) it does suffer from significant loss in power–i.e. we are using just half of the information to estimate the unsampled locations. A better approach (and one easily implemented in a computing environment) is to remove one data point from the dataset and interpolate its value using all other points in the dataset then repeating this process for each and every point the dataset (while making sure that the interpolator parameters remain constant across each interpolation). The interpolated values are then compared with the actual values from the omitted point. This method is sometimes referred to as jackknifing or leave-one-out cross-validation. The performance of the interpolator can be summarized by computing the root-mean of squared residuals (RMSE) from the errors as follows: \\[ RMSE = \\sqrt{\\frac{\\sum_{i=1}^n (\\hat {Z_{i}} - Z_i)^2}{n}} \\] where \\(\\hat {Z_{i}}\\) is the interpolated value at the unsampled location i (i.e. location where the sample point was removed), \\(Z_i\\) is the true value at location i and \\(n\\) is the number of points in the dataset. We can create a scatterplot of the predicted vs. expected precipitation values from our dataset. The solid diagonal line represents the one-to-one slope (i.e. of the predicted values match the true value exactly, then the points would fall on this line). The red dashed line is a linear fit to the points which is here to help guide our eyes along the pattern generated by these points. The computed RMSE from the above working example is 6.989. We can extend our exploration of the interpolator’s accuracy by creating a map of the confidence intervals. This involves layering all \\(n\\) interpolated surfaces from the aforementioned jackknife technique, then computing the confidence interval for each location ( pixel) in the output map (raster). If the range of interpolated values from the jackknife technique for an unsampled location \\(i\\) is high, then this implies that this location is highly sensitive to the presence or absence of a single point from the sample point locations thus producing a large confidence interval (i.e. we can’t be very confident of the predicted value). Conversely, if the range of values estimated for location \\(i\\) is low, then a small confidence interval is producing (providing us with greater confidence in the interpolated value). The following map shows the 95% confidence interval for each unsampled location (pixel) in the study extent. Figure 2.5: In this example an IDW power coefficient of 2 was used and the search parameters was confined to a minimum number of points of 10 and a maximum number of points of 15. The search window was isotropic. Each pixel represents the range of precipitation values (in inches) around the expected value given a 95% confidence interval. 14.2 Statistical Approach to Interpolation The statistical interpolation methods include surface trend and Kriging. 14.2.1 Trend Surfaces It may help to think of trend surface modeling as a regression on spatial coordinates where the coefficients apply to those coordinate values and (for more complicated surface trends) to the interplay of the coordinate values. We will explore a 0th order, 1st order and 2nd order surface trends in the following sub-sections. 14.2.1.1 0th Order Trend Surface The first model (and simplest model), is the 0th order model which takes on the following expression: Z = a where the intercept a is the mean value of all sample points (27.1 in our working example). This is simply a level (horizontal) surface whose cell values all equal 27.1. Figure 2.6: The simplest model where all interpolated surface values are equal to the mean precipitation. This makes for an uninformative map. A more interesting surface trend map is one where the surface trend has a slope other than 0 as highlighted in the next subsection. 14.2.1.2 1st Order Trend Surface The first order surface polynomial is a slanted flat plane whose formula is given by: Z = a + bX + cY where X and Y are the coordinate pairs. Figure 2.7: Result of a first order interpolation. The 1st order surface trend does a good job in highlighting the prominent east-west trend. But is the trend truly uniform along the X axis? Let’s explore a more complicated surface: the quadratic polynomial. 14.2.1.3 2nd Order Trend Surface The second order surface polynomial (aka quadratic polynomial) is a parabolic surface whose formula is given by: \\(Z = a + bX + cY + dX^2 + eY^2 + fXY\\) Figure 4.1: Result of a second order interpolation This interpolation picks up a slight curvature in the east-west trend. But it’s not a significant improvement on the 1st order trend. 14.2.2 Ordinary Kriging Several forms of kriging interpolators exist: ordinary, universal and simple just to name a few. This section will focus on ordinary kriging (OK) interpolation. This form of kriging usually involves four steps: Removing any spatial trend in the data (if present). Computing the experimental variogram, \\(\\gamma\\), which is a measure of spatial autocorrelation. Defining an experimental variogram model that best characterizes the spatial autocorrelation in the data. Interpolating the surface using the experimental variogram. Adding the kriged interpolated surface to the trend interpolated surface to produce the final output. These steps our outlined in the following subsections. 14.2.2.1 De-trending the data One assumption that needs to be met in ordinary kriging is that the mean and the variation in the entity being studied is constant across the study area. In other words, there should be no global trend in the data (the term drift is sometimes used to describe the trend in other texts). This assumption is clearly not met with our Texas precipitation dataset where a prominent east-west gradient is observed. This requires that we remove the trend from the data before proceeding with the kriging operations. Many pieces of software will accept a trend model (usually a first, second or third order polynomial). In the steps that follow, we will use the first order fit computed earlier to de-trend our point values (recall that the second order fit provided very little improvement over the first order fit). Removing the trend leaves us with the residuals that will be used in kriging interpolation. Note that the modeled trend will be added to the kriged interpolated surface at the end of the workflow. Figure 4.3: Map showing de-trended precipitation values (aka residuals). These detrended values are then passed to the ordinary kriging interpolation operations. You can think of these residuals as representing variability in the data not explained by the global trend. If variability is present in the residuals then it is best characterized as a distance based measure of variability (as opposed to a location based measure). 14.2.2.2 Experimental Variogram In Kriging interpolation, we focus on the spatial relationship between location attribute values. More specifically, we are interested in how these attribute values (precipitation residuals in our working example) vary as the distance between location point pairs increases. We can compute the difference, \\(\\gamma\\), in precipitation values by squaring their differences then dividing by 2. For example, if we take two meteorological stations (one whose de-trended precipitation value is -0.3 and the other whose value is -1.6), Figure 4.4: Locations of two sample sites used to demonstrate the the calculation of gamma. we can compute their difference (\\(\\gamma\\)) as follows: \\[ \\gamma = \\frac{(Z_2 - Z_1)^2}{2} = \\frac{(-0.3 - (-1.6))^2}{2} = 0.845 \\] We can compute \\(\\gamma\\) for all point pairs then plot these values as a function of the distance that separate these points: The red point in the plot is the value computed in the above example. The distance separating those two points is about km. This value is mapped in the above plots as a red dot. The above plot is called an experimental semivariogram cloud plot (also referred to as an experimental variogram cloud plot). The terms semivariogram and variogram are often used interchangeably in geostatistics (we’ll use the term variogram henceforth since this seems to be the term of choice in current literature). Also note that the word experimental is sometimes dropped when describing these plots, but its use in our terminology is an important reminder that the points we are working with are just samples of some continuous field whose spatial variation we are attempting to model. 14.2.2.3 Sample Experimental Variogram Cloud points can be difficult to interpret due to the sheer number of point pairs (we have 465 point pairs from just 50 sample points, and this just for 1/3 of the maximum distance lag!). A common approach to resolving this issue is to “bin” the cloud points into intervals called lags and to summarize the points within each interval. In the following plot, we split the data into 15 bins then compute the average point value for each bin (displayed as red points in the plot). The red points that summarize the cloud are the sample experimental variogram estimates for each of the 15 distance bands and the plot is referred to as the sample experimental variogram plot. 14.2.2.4 Experimental Variogram Model The next step is to fit a mathematical model to our sample experimental variogram. Different mathematical models can be used; their availability is software dependent. Examples of mathematical models are shown below: The goal is to apply the model that best fits our sample experimental variogram. This requires picking the proper model, then tweaking the partial sill, range, and nugget parameters (where appropriate). The following figure illustrates a nonzero intercept where the nugget is the distance between the \\(0\\) variance on the \\(y\\) axis and the variogram’s model intercept with the \\(y\\) axis. The partial sill is the vertical distance between the nugget and the part of the curve that levels off. If the variogram approaches \\(0\\) on the \\(y\\)-axis, then the nugget is \\(0\\) and the partial sill is simply referred to as the sill. The distance along the \\(x\\) axis where the curve levels off is referred to as the range. In our working example, we will try to fit the Spherical function to our sample experimental variogram. This is one of three popular models (the other two being linear and gaussian models.) 14.2.2.5 Kriging Interpolation The variogram model is used by the kriging interpolator to provide localized weighting parameters. Recall that with the IDW, the interpolated value at an unsampled site is determined by summarizing weighted neighboring points where the weighting parameter (the power parameter) is defined by the user and is applied uniformly to the entire study extent. Kriging use the variogram model to compute the weights of neighboring points based on the distribution of those values–in essence, kriging is letting the localized pattern produced by the sample points define the weights (in a systematic way). The exact mathematical implementation will not be covered here (it’s quite involved), but the resulting output is shown in the following figure: Figure 9.1: Krige interpolation of the residual (detrended) precipitation values. Recall that the kriging interpolation was performed on the de-trended data. In essence, we predicted the precipitation values based on localized factors. We now need to combine this interpolated surface with that produced from the trend interpolated surface to produce the following output: Figure 11.2: The final kriged surface. A valuable by-product of the kriging operation is the variance map which gives us a measure of uncertainty in the interpolated values. The smaller the variance, the better. Figure 11.3: Variance map resulting from the Kriging analysis. "],
["data-manipulation-in-r.html", "Data Manipulation in R Reading vector files Reading GIS raster files Dissecting the file summary Mapping spatial data Joining tables to spatial features Converting XY data to point objects Geocoding street addresses Exporting to a shapefile Exporting to a GeoPackage Exporting to a raster file format", " Data Manipulation in R Reading vector files First, we’ll load sample spatial data files into the current working directory (you might want to create a temporary folder for this exercise and set it as the working directory via the Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory pull-down menu from the RStudio interface). The following chunk will download and unzip a zipped shapefile into the current working directory. tmp &lt;- tempfile() download.file(&quot;http://colby.edu/~mgimond/Spatial/Data/Income_schooling.zip&quot;, destfile = tmp) unzip(tmp, exdir = &quot;.&quot;) The first line of code creates a temporary file on your machine to house the zip file. The second line of code downloads the zip file into the temporary file. The third line unzips the zip file into the current working directory. You should see the six files that constitute the shapefile: [1] &quot;Income_schooling.dbf&quot; &quot;Income_schooling.prj&quot; &quot;Income_schooling.sbn&quot; [4] &quot;Income_schooling.sbx&quot; &quot;Income_schooling.shp&quot; &quot;Income_schooling.shx&quot; [7] &quot;Income_schooling.zip&quot; The following chunk of code will download a GeoPackage file into the current working directory. Note that the file is not zipped thus eliminating any need to unzip (unlike a shapefile, a GeoPackage data file is self-contained). download.file(&quot;http://colby.edu/~mgimond/Spatial/Data/TX.gpkg&quot;, destfile = &quot;./TX.gpkg&quot;, mode=&quot;wb&quot;) 14.2.3 Reading a shapefile with rgdal There are two different ways one can load a shapefile into R: using the rgdal package and using the maptools package. The difference between both methods is that the rgdal option loads the coordinate system (CS) information (if present in the shapefile) while maptools does not. The function readOGR() from rgdal is used to load the shapefile. library(rgdal) s1 &lt;- readOGR(&quot;.&quot;, &quot;Income_schooling&quot;) OGR data source with driver: ESRI Shapefile Source: &quot;.&quot;, layer: &quot;Income_schooling&quot; with 16 features It has 5 fields The &quot;.&quot; parameter tells R to look for the shapefile in the current working directory, the second parameter tells R the name of the shapefile to open. Note that you must not add a file extension to the name as this will generate an error message. Let’s check the contents of this shapefile. summary(s1) Object of class SpatialPolygonsDataFrame Coordinates: min max x 336337.7 660529.1 y 4772272.3 5255569.2 Is projected: TRUE proj4string : [+proj=utm +zone=19 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0] Data attributes: NAME Income NoSchool NoSchoolSE Androscoggin: 1 Min. :20015 Min. :0.002390 Min. :0.0006832 Aroostook : 1 1st Qu.:21631 1st Qu.:0.004907 1st Qu.:0.0009130 Cumberland : 1 Median :23788 Median :0.005252 Median :0.0010281 Franklin : 1 Mean :24716 Mean :0.005778 Mean :0.0011658 Hancock : 1 3rd Qu.:27897 3rd Qu.:0.006603 3rd Qu.:0.0013404 Kennebec : 1 Max. :32549 Max. :0.013387 Max. :0.0021290 (Other) :10 IncomeSE Min. :242.4 1st Qu.:342.6 Median :455.8 Mean :458.8 3rd Qu.:551.5 Max. :724.2 Note the coordinate information under the proj4string : heading. It suggests that the CS is a UTM projection for zone 19 (north) and an NAD83 datum. 14.2.4 Reading a shapefile with maptools Here, we’ll use readShapeSpatial() from maptools to load the shapefile. library(maptools) s2 &lt;- readShapeSpatial(&quot;Income_schooling&quot;) Let’s check the contents of this shapefile. summary(s2) Object of class SpatialPolygonsDataFrame Coordinates: min max x 336337.7 660529.1 y 4772272.3 5255569.2 Is projected: NA proj4string : [NA] Data attributes: NAME Income NoSchool NoSchoolSE Androscoggin: 1 Min. :20015 Min. :0.002390 Min. :0.0006832 Aroostook : 1 1st Qu.:21631 1st Qu.:0.004907 1st Qu.:0.0009130 Cumberland : 1 Median :23788 Median :0.005252 Median :0.0010281 Franklin : 1 Mean :24716 Mean :0.005778 Mean :0.0011658 Hancock : 1 3rd Qu.:27897 3rd Qu.:0.006603 3rd Qu.:0.0013404 Kennebec : 1 Max. :32549 Max. :0.013387 Max. :0.0021290 (Other) :10 IncomeSE Min. :242.4 1st Qu.:342.6 Median :455.8 Mean :458.8 3rd Qu.:551.5 Max. :724.2 s2’s summary output is nearly identical to that of s1 except for the missing CS (the summary indicates that Is projected: NA, i.e. that there is no projection information). 14.2.5 Reading a GeoPackage A GeoPackage can be read with the rgdal package. A GeoPackage is a spatial database that can store more than one layer. This requires knowledge of the layer name which may be different from database filename. You can extract the layer name(s) from a GeoPackage as follows: library(rgdal) ogrListLayers(&quot;TX.gpkg&quot;)[1] [1] &quot;TX&quot; The TX.gpkg file has a single layer called TX. To extract the layer TX into a new spatial object called s3, type: s3 &lt;- readOGR(&quot;TX.gpkg&quot;, layer=&quot;TX&quot;) OGR data source with driver: GPKG Source: &quot;TX.gpkg&quot;, layer: &quot;TX&quot; with 1 features It has 49 fields Reading GIS raster files First, we’ll load a sample raster file into your current working directory (you might want to create a temporary folder for this exercise and set it as the working directory via the Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory pull-down menu from the RStudio interface). The raster file will be in an ERDAS Imagine format. download.file(&quot;http://colby.edu/~mgimond/Spatial/Data/elevation.img&quot;, destfile = &quot;./elevation.img&quot;, mode = &quot;wb&quot;) Raster files can be read with rgdal’s readGDAL() function or with raster’s raster() function. They differ in the output formats and in the way the data are managed in an R session–readGDAL will load the raster as a SpatialGridDataFrame object whereas raster will load the data as a raster* object and raster will not load the entire raster into memory whereas readGDAL will making raster’s approach attractive if large rasters are to be worked with. If you wish to load all data from a raster object in R memory, wrap the raster object withreadAll as in r1 &lt;- readAll(r). In the following example, we’ll load the raster as a raster* object. library(raster) r1 &lt;- raster(&quot;elevation.img&quot;) To view the contents of the newly created r1 raster object, type its name at the command line. r1 class : RasterLayer dimensions : 540, 1080, 583200 (nrow, ncol, ncell) resolution : 0.3333333, 0.3333333 (x, y) extent : -180, 180, -90, 90 (xmin, xmax, ymin, ymax) coord. ref. : +proj=longlat +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +no_defs data source : D:\\mgimond\\GitHub\\Spatial\\elevation.img names : elevation values : 0, 6821 (min, max) You’ll note that the object is of class RasterLayer. You’ll also note that the coordinate system information stored in the original raster has been carried over into the r1 object. The raster function will recognize many difference raster formats including GeoTIFF and jpg. Dissecting the file summary Let’s explore the contents of s1’s summary. summary(s1) Object of class SpatialPolygonsDataFrame Coordinates: min max x 336337.7 660529.1 y 4772272.3 5255569.2 Is projected: TRUE proj4string : [+proj=utm +zone=19 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0] Data attributes: NAME Income NoSchool NoSchoolSE Androscoggin: 1 Min. :20015 Min. :0.002390 Min. :0.0006832 Aroostook : 1 1st Qu.:21631 1st Qu.:0.004907 1st Qu.:0.0009130 Cumberland : 1 Median :23788 Median :0.005252 Median :0.0010281 Franklin : 1 Mean :24716 Mean :0.005778 Mean :0.0011658 Hancock : 1 3rd Qu.:27897 3rd Qu.:0.006603 3rd Qu.:0.0013404 Kennebec : 1 Max. :32549 Max. :0.013387 Max. :0.0021290 (Other) :10 IncomeSE Min. :242.4 1st Qu.:342.6 Median :455.8 Mean :458.8 3rd Qu.:551.5 Max. :724.2 The first line of output gives us the spatial object type, SpatialPolygonsDataFrame. It suggests that the shapefile is a polygon layer and that it has an attributes tables (hence the segment DataFrame in the object name). If this were a polyline layer the object type would be SpatialLinesDataFrame and if it were a point layer the object type would be SpatialPointsDataFrame. The next few lines of output give us the bounding extent of the layer in the layer’s coordinate system units. Coordinates: min max x 336337.7 660529.1 y 4772272.3 5255569.2 The three lines that follow give us the projection information. Note, once again, that this information is absent if you loaded the data via maptools. Is projected: TRUE proj4string : [+proj=utm +zone=19 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0] What remains of the output is a summary of each column in the attributes table. There are 5 columns named . You can also extract the column names via the names function: names(s1) [1] &quot;NAME&quot; &quot;Income&quot; &quot;NoSchool&quot; &quot;NoSchoolSE&quot; &quot;IncomeSE&quot; If you want to extract the contents of the attributes tables, use the data.frame function. data.frame(s1) NAME Income NoSchool NoSchoolSE IncomeSE 0 Aroostook 21024 0.01338720 0.001406960 250.909 1 Somerset 21025 0.00521153 0.001150020 390.909 2 Piscataquis 21292 0.00633830 0.002128960 724.242 3 Penobscot 23307 0.00684534 0.001025450 242.424 4 Washington 20015 0.00478188 0.000966036 327.273 5 Franklin 21744 0.00508507 0.001641740 530.909 6 Oxford 21885 0.00700822 0.001318160 536.970 7 Waldo 23020 0.00498141 0.000918837 450.909 8 Kennebec 25652 0.00570358 0.000917087 360.000 9 Androscoggin 24268 0.00830953 0.001178660 460.606 10 Hancock 28071 0.00238996 0.000784584 585.455 11 Knox 27141 0.00652269 0.001863920 684.849 12 Lincoln 27839 0.00278315 0.001030800 571.515 13 Cumberland 32549 0.00494917 0.000683236 346.061 14 Sagadahoc 28122 0.00285524 0.000900782 544.849 15 York 28496 0.00529228 0.000737195 332.121 Mapping spatial data There are many ways one can plot spatial features in R. We’ll explore two approaches: one using sp’s spplot, the second using the tmap package. 14.2.6 Mapping with spplot spplot is a base plotting function that is part of the sp package. Note that sp underlies many other packages including maptools and rgdal. To simply plot the Income attribute, type: spplot(s1, z=&quot;Income&quot;) The resulting map is referred to as a choropleth map (choro = area and pleth = value) where some value (an enumeration of population in this working example) is aggregated over a defined area (e.g.counties) and displayed using different colors. We can control how the attribute values are binned and which colors to assign to each bin. We will use the function classIntervals to generate the breaks following a quantile scheme. The classIntervals’s minimum and maximum values will be the range of values from the Income attribute. This poses a problem since the spplot function will interpret that maximum value as not being inclusive which will result in the polygon having the maximum attribute value not being assigned a color. To remedy this, we will pad the maximum value with the value of 0.1. We will also use the RColorBrewer package to generate the different color swatches: In our working example, we will generate four different shades of green. The code chunk and resulting map follow: library(classInt) library(RColorBrewer) # Generate breaks brks &lt;- classIntervals(s1$Income, n = 4, style = &quot;quantile&quot;)$brks brks[length(brks)] &lt;- brks[length(brks)] + 1 # Define color swatches pal &lt;- brewer.pal(4, &quot;Greens&quot;) # Generate the map spplot(s1, z=&quot;Income&quot;, at = brks, col.regions=pal) Note how we are able to extract the Income column via s1$Income which is passed to the classIntervals function (which, in turn, uses these values to compute the breaks). The breaks are stored in the object brks; its contents are 20015.0, 21631.0, 23787.5, 27897.0, 32550.0. A nice feature of spplot is its legend: instead of generating discrete color swatches (with associated value ranges) ssplot generates contiguous color swatches along a scale which provides us with not only information on the ranges covered by the colors but a visual on the distribution of these bins. 14.2.7 Mapping with tmap The tmap package is designed to map thematic data. The following chunk recreates the map generated with spplot. library(tmap) qtm(s1, fill=&quot;Income&quot;, fill.style=&quot;quantile&quot;, fill.n=8 ,fill.palette=&quot;Greens&quot;) The fill.style parameter defines the binning scheme. Here we choose a quantile scheme. Other options include equal (equal interval), jenks (same scheme ArcMap defaults to), sd (breaks the data into standard deviations), kmeans (based on the kmeans method), and pretty to name a few. If you want to manually define your breaks, you can pass the option fixed to the fill.style parameter then pass the desired breaks to the breaks parameter. For example, to break the data into 3 bins–under $23,000, $23,000 to $27,000 and $27,000 and greater– modify the above chunk as follows. library(tmap) qtm(s1, fill=&quot;Income&quot;, fill.style=&quot;fixed&quot;, fill.breaks=c(0,23000 ,27000,100000 ), fill.labels=c(&quot;under $23,000&quot;, &quot;$23,000 to $27,000&quot;, &quot;above $27,000&quot;), fill.palette=&quot;Greens&quot;, legend.text.size = 0.5, layout.legend.position = c(&quot;right&quot;, &quot;bottom&quot;)) If a more complex map is desired, the plotting functions can be broken down into elements (similar to ggplot2’s plotting elements). The last block of code can be modified as follows: tm_shape(s1) + tm_fill(&quot;Income&quot;, style=&quot;fixed&quot;, breaks=c(0,23000 ,27000,100000 ), labels=c(&quot;under $23,000&quot;, &quot;$23,000 to $27,000&quot;, &quot;above $27,000&quot;), palette=&quot;Greens&quot;) + tm_borders(&quot;grey&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE) Here, we move the legend element to the right of the map data frame and we suppress the drawing of the map border. 14.2.8 Mapping rasters with tmap To generate a quick map, simply use the qtm function. qtm(r1) The next chunk of code overlays the raster with a vector layer and limits the extent to the northeastern US. To maximize the use of colors, elevation breaks are manually defined. tm_shape(r1, ylim = c(30,50), xlim=c(-100,-50)) + tm_raster(breaks=c(0,100,200,300,400,500,750,1000,1500,2000,3000,4000)) + tm_shape(s1) + tm_fill(&quot;Income&quot;, style=&quot;fixed&quot;, breaks=c(0,23000 ,27000,100000 ), labels=c(&quot;under $23,000&quot;, &quot;$23,000 to $27,000&quot;, &quot;above $27,000&quot;), palette=&quot;Greens&quot;) + tm_borders(&quot;grey&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE) To learn more about the tmap package, check out the package vignette and the package author’s presentation. Joining tables to spatial features First, we’ll download a comma delimited table (aka a CSV file). The table consists of population count and land/water surface area for each county in the state of Maine. df &lt;- read.csv(&quot;http://colby.edu/~mgimond/Spatial/Data/ME2010_pop.csv&quot;) Next, we’ll make use of dplyr’s left_join function to join the table to the spatial object s1 by county name. library(dplyr) s1@data &lt;- left_join(s1@data, df, by=c(&quot;NAME&quot; = &quot;County&quot;)) The spatial object s1 consists of many components including the attributes table. Accessing each component requires appending the object name with @. In our case, we are accessing the attributes table (stored in the data component) via s1@data. In essence, the above code is joining the data table df to the attributes table s1@data and overwriting the existing attributes table (with the join) in the process. The function is also provided with a join key that dictates which columns from each table are to be used to match the records. Here, we are matching s1’s NAME column with df’s County column (both store the county names). For a join to be successful, all record elements must match letter for letter–this includes case! For example Kennebec and kennebec will not match since one has an upper case K while the other does not. Attribute tables can be manipulated just like any other regular table in R. For example, to compute population density from the population field and the land area field, type the following. s1$Pop_dens = s1$Population / s1$Land The $ symbol accesses the field names Population and Land. The new field Pop_dens is now added to the s1 attributes table. Next, we’ll map the population density values. tm_shape(s1) + tm_fill(&quot;Pop_dens&quot;, style=&quot;quantile&quot;, palette=&quot;Reds&quot;, title=&quot;Population density \\n(per square mile)&quot;) + tm_borders(&quot;white&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE) Converting XY data to point objects Let’s create a new data table with latitude and longitude values for nine major cities in the state of Maine. df2 &lt;- data.frame(lon = c(-68.783, -69.6458, -69.7653, -69.9536, -70.7733, -70.4462, -70.4449, -70.1924, -70.2691), lat = c(44.8109, 44.5521, 44.3235, 43.913, 43.4399, 43.4741, 43.5104, 44.0975, 43.6651 ), Name = c(&quot;Bangor&quot;, &quot;Waterville&quot;, &quot;Augusta&quot;, &quot;Brunswick&quot;, &quot;Sanford&quot;, &quot;Biddeford&quot;, &quot;Saco&quot;, &quot;Lewiston&quot;, &quot;Portland&quot;) ) Next, we’ll create a spatial object (a SpatialPointsDataFrame to be more precise) from the regular data table. pt &lt;- SpatialPointsDataFrame(coords = df2[,c(&quot;lon&quot;,&quot;lat&quot;)], data = df2, proj4string = CRS(&quot;+proj=longlat +datum=WGS84&quot;)) The coords parameter is given the latitude and longitude value columns–values used to locate the points associated with each record. The proj4string parameter defines the coordinate systems associated with the lat/long values–a WGS 1984 Geographic Coordinate System in our example. Let’s check that pt is indeed a spatial object. summary(pt) Object of class SpatialPointsDataFrame Coordinates: min max lon -70.7733 -68.7830 lat 43.4399 44.8109 Is projected: FALSE proj4string : [+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0] Number of points: 9 Data attributes: lon lat Name Min. :-70.77 Min. :43.44 Augusta :1 1st Qu.:-70.44 1st Qu.:43.51 Bangor :1 Median :-70.19 Median :43.91 Biddeford:1 Mean :-70.03 Mean :43.98 Brunswick:1 3rd Qu.:-69.77 3rd Qu.:44.32 Lewiston :1 Max. :-68.78 Max. :44.81 Portland :1 (Other) :3 Note the object class SpatialPointsDataFrame. Also note that the dataframe contents were also carried over into the attributes table (here we had just one attribute, Name, other than lon and lat). The next chunk of code plots the point layer on top of the population density thematic map. tm_shape(s1) + tm_fill(&quot;Pop_dens&quot;, style=&quot;quantile&quot;, palette=&quot;Reds&quot;, title=&quot;Population density \\n(per square mile)&quot;) + tm_borders(&quot;white&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE, inner.margins = c(0, 0.25, 0, 0.01)) + tm_shape(pt) + tm_dots(size=.3, col=&quot;yellow&quot;, border.col=&quot;black&quot;) + tm_text(&quot;Name&quot;, auto.placement = TRUE, shadow=TRUE, size=0.8) Geocoding street addresses The package ggmap offers a geocoding function called mutate_geocode which will take a table with physical addresses and create a new table with latitude and longitude values for those addresses. The new table also contains all other columns of the input table. The function makes use of Google Maps’ API. Let’s first create a new data table with addresses for a few liberal arts colleges in the state of Maine. df3 &lt;- data.frame(Address = c(&quot;4000 mayflower Hill, Waterville, Maine&quot;, &quot;2 Andrews Rd, Lewiston, ME 04240&quot;, &quot;255 Maine St, Brunswick, ME 04011&quot;, &quot;90 Quaker Hill Rd, Unity, ME 04988&quot;, &quot;105 Eden St, Bar Harbor, ME 04609&quot;), College = c(&quot;Colby&quot;, &quot;Bates&quot;, &quot;Bowdoin&quot;, &quot;Unity&quot;, &quot;CoA&quot;), stringsAsFactors = FALSE) Next, we’ll geocode the addresses and save the output to a new data table called df3.geo. library(ggmap) df3.geo &lt;- mutate_geocode(df3, location=Address, output=&quot;latlona&quot; ) The location parameter is assigned the column with the addresses. The output parameter is given instructions on what to output. Here, the option latlona instructs R to output both the lat/lon values as well as all other attributes. It also adds another column called address with Google’s nearest address match–not all addresses are necessarily matched exactly requiring Google to propose the closest match. df3.geo Address College lon lat 1 4000 mayflower Hill, Waterville, Maine Colby -69.65321 44.55605 2 2 Andrews Rd, Lewiston, ME 04240 Bates -70.20420 44.10675 3 255 Maine St, Brunswick, ME 04011 Bowdoin -69.96524 43.90701 4 90 Quaker Hill Rd, Unity, ME 04988 Unity -69.33084 44.60335 5 105 Eden St, Bar Harbor, ME 04609 CoA -68.22182 44.39536 address 1 mayflower hill dr, waterville, me 04901, usa 2 2 andrews rd, lewiston, me 04240, usa 3 255 maine st, brunswick, me 04011, usa 4 90 quaker hill rd, unity, me 04988, usa 5 105 eden st, bar harbor, me 04609, usa In our little example, all addresses are matched exactly except Colby which is assigned mayflower hill dr without a number. Note that the Google Maps API sets the limit to 2500 queries a day. To check how many queries you have left in your session, use geocodeQueryCheck. geocodeQueryCheck() 2495 geocoding queries remaining. Note that df2.geo is not a spatial object. It needs to be converted to one if it is to be used in generating a map output. The lat/lon values are assumed to be recorded in a WGS84 geographic coordinate system. pt2 &lt;- SpatialPointsDataFrame(coords = df3.geo[,c(&quot;lon&quot;,&quot;lat&quot;)], data = df3.geo, proj4string = CRS(&quot;+proj=longlat +datum=WGS84&quot;)) Finally, we’ll add the points to an existing thematic map. tm_shape(s1) + tm_fill(&quot;Pop_dens&quot;, style=&quot;quantile&quot;, palette=&quot;Reds&quot;, title=&quot;Population density \\n(per square mile)&quot;) + tm_borders(&quot;white&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_layout(frame = FALSE, inner.margins = c(0, 0.25, 0, 0.01)) + tm_shape(pt2) + tm_dots(size=.3, col=&quot;yellow&quot;, border.col=&quot;black&quot;) + tm_text(&quot;College&quot;, auto.placement = TRUE, shadow=TRUE, size=0.8) If you wish to use Google Maps as a backdrop, you can map the points with ggmap’s qmplot function. qmplot(lon, lat, data=df3.geo, source=&quot;osm&quot;) + geom_point(col=&quot;red&quot;, size=2, show.legend = FALSE) + geom_label(aes(label=College), hjust = &quot;inward&quot;, vjust=&quot;inward&quot;) Exporting to a shapefile Both rgdal and maptools have functions to export spatial objects to shapefiles, but only rgdal will export the coordinate systems (if present) with the shapefile. The following chunk of code demonstrates the exports of two spatial objects created earlier in this exercise: s1 (a SpatialPolygonsDataFrame object) and pt2 (a SpatialPointsDataFrame). library(rgdal) # Save polygon layer to shapefile writeOGR(s1, dsn=&quot;.&quot;, layer=&quot;Counties&quot;, driver=&quot;ESRI Shapefile&quot; ) # Save point layer to shapefile writeOGR(pt2, dsn=&quot;.&quot;, layer=&quot;Colleges&quot;, driver=&quot;ESRI Shapefile&quot; ) The layer=&quot;Colleges&quot; parameter defines the file names (note that the file extension is not specified since it will be automatically added based on the output format) and the driver parameter specifies the output file format. Exporting to a GeoPackage rgdal’s GPKG driver is used to export to a GeoPackage format. The syntax is slightly different from that used to export to a shapefile in that the database name must be specified as well as the layer name. For example, the following chunk of code exports the Counties layer to a database named geo.gpkg library(rgdal) # Save polygon layer to shapefile writeOGR(s1, dsn=&quot;geo.gpkg&quot;, layer=&quot;Counties&quot;, driver=&quot;GPKG&quot; ) Exporting to a raster file format You may use raster’s writeRaster function to export rasters to external raster formats. For example, to export to a GeoTIFF raster: library(rgdal) writeRaster(r1, filename=&quot;elev&quot;, format=&quot;GTiff&quot; ) To export to an ERDAS Imagine file format: library(rgdal) writeRaster(r1, filename=&quot;elev&quot;, format=&quot;HFA&quot; ) To list all available output raster file formats, type: writeFormats() "],
["coordinate-systems-in-r.html", "Coordinate Systems in R Checking for a coordinate system Understanding the coordinate syntax Assigning a coordinate system Modifying coordinate systems", " Coordinate Systems in R For this exercise, we’ll load a sample shapefile into our current working directory (you might want to create a temporary folder for this exercise and set it as the working directory via the Session &gt;&gt; Set Working Directory &gt;&gt; Choose Directory pull-down menu from the RStudio interface). tmp &lt;- tempfile() download.file(&quot;http://colby.edu/~mgimond/Spatial/Data/Income_schooling.zip&quot;, destfile = tmp) unzip(tmp, exdir = &quot;.&quot;) Next, we’ll convert the shapefile into an R spatial object using readOGR (note that maptools could also be used to create an R spatial object but unlike readOGR it will not load the GIS file’s coordinate system). library(rgdal) s1 &lt;- readOGR(&quot;.&quot;, &quot;Income_schooling&quot;) # CS is read and stored OGR data source with driver: ESRI Shapefile Source: &quot;.&quot;, layer: &quot;Income_schooling&quot; with 16 features It has 5 fields Checking for a coordinate system The summary function can be used to pull the coordinate system information (if present) but to extract just the coordinate system information, use proj4string instead–it’s part of the sp package that was automatically loaded with the rgdal package in an earlier step. proj4string(s1) [1] &quot;+proj=utm +zone=19 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0&quot; Object s1 has a CS. It’s made up of a multi-parameter string whose syntax is briefly discussed next. Understanding the coordinate syntax Coordinate systems are defined in R using a syntax created by the PROJ4 project. It consists of a list of parameters, each prefixed with the + character. For example, s1’s CS is in a UTM projection (+proj=utm) for zone 19 (+zone=19) and an NAD 1983 datum (+datum=NAD83). Other bits of information that can be gleaned from the projection string are the units (in meters) and the ellipsoid (GRS80). A list of a subset of PROJ4 parameters used in defining a CS follows. Click here for a full list of parameters. +a Semimajor radius of the ellipsoid axis +b Semiminor radius of the ellipsoid axis +datum Datum name (see `proj -ld`) +ellps Ellipsoid name (see `proj -le`) +lat_0 Latitude of origin +lat_1 Latitude of first standard parallel +lat_2 Latitude of second standard parallel +lat_ts Latitude of true scale +lon_0 Central meridian +over Allow longitude output outside -180 to 180 range, disables wrapping (see below) +proj Projection name (see `proj -l`) +south Denotes southern hemisphere UTM zone +units meters, US survey feet, etc. +x_0 False easting +y_0 False northing +zone UTM zone Assigning a coordinate system A CS definition can be passed to a spatial object. It can either fill a spatial object’s empty CS definition or it can overwrite and existing definition (the latter should only be executed if there is good reason to believe that the original definition is erroneous). Note that this step does not change an objects underlying coordinate system (this option will be discussed in the next section). We’ll pretend that a CS definition was not assigned to s1 and assign one manually. proj4string(s1) &lt;- CRS(&quot;+proj=utm +zone=19 +ellps=GRS80 +datum=NAD83&quot;) Note that we do not need to define all of the parameters so long as we know that the default values for these unused parameters are correct. Also note that we do not need to designate a hemisphere since the NAD83 datum applies only to North America. To recreate a CS defined in a piece of software such as ArcGIS, it is best to extract the CS’ WKID/EPSG code then use that number to lookup the PROJ4 syntax on http://spatialreference.org/ref/. For example, in ArcGIS, the WKID number can be extracted from the coordinate system properties output. Figure 2.5: An ArcGIS dataframe coordinate system properties window. Note the WKID/EPSG code of 26919 (highlighted in red) associated with the NAD 1983 UTM Zone 19 N CS. That number can then be entered in http://spatialreference.org/ref/’s search box to pull the Proj4 parameters (note that you must select Proj4 from the list of syntax options). Figure 2.6: Example of a search result for EPSG 26919 at http://spatialreference.org/ref/. Note that after clicking the EPSG:269191 link, you must then select the Proj4 syntax from a list of available syntaxes to view the projection parameters Here are examples of a few common projections: Projection WKID Authority Syntax UTM NAD 83 Zone 19N 26919 EPSG +proj=utm +zone=19 +ellps=GRS80 +datum=NAD83 +units=m +no_defs USA Contiguous albers equal area 102003 ESRI +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs Alaska albers equal area 3338 EPSG +proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs World Robinson 54030 ESRI +proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs Modifying coordinate systems The last step showed you how to define or modify the coordinate system definition. This section shows you how to transform the coordinate values associated with the spatial object to a different coordinate system. For example, to transform s1 to a geographic (lat/long) coordinate system use the spTransform function. s1.gcs &lt;- spTransform(s1, CRS(&quot;+proj=longlat +datum=WGS84&quot;)) Let’s explore s1.gcs’s summary. summary(s1.gcs) Object of class SpatialPolygonsDataFrame Coordinates: min max x -71.08751 -66.96927 y 43.09105 47.45333 Is projected: FALSE proj4string : [+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0] Data attributes: NAME Income NoSchool NoSchoolSE Androscoggin: 1 Min. :20015 Min. :0.002390 Min. :0.0006832 Aroostook : 1 1st Qu.:21631 1st Qu.:0.004907 1st Qu.:0.0009130 Cumberland : 1 Median :23788 Median :0.005252 Median :0.0010281 Franklin : 1 Mean :24716 Mean :0.005778 Mean :0.0011658 Hancock : 1 3rd Qu.:27897 3rd Qu.:0.006603 3rd Qu.:0.0013404 Kennebec : 1 Max. :32549 Max. :0.013387 Max. :0.0021290 (Other) :10 IncomeSE Min. :242.4 1st Qu.:342.6 Median :455.8 Mean :458.8 3rd Qu.:551.5 Max. :724.2 Note the new coordinate system definition, but also note the new min and max extent values have changed to reflect the new CS units (degrees latitude and longitude). You’ll also note the Is projected: FALSE output that reminds us that a geographic coordinate system is a not a projected coordinate system (i.e. projected onto a cartesian coordinate system). A geographic coordinate system is often desired when overlapping a web based mapping service such as Google, Bing or OpenStreetMap. To check that s1.gcs was properly transformed, we’ll overlay it on top of an OpenStreetMap using the leaflet package. library(leaflet) leaflet(s1.gcs) %&gt;% addPolygons() %&gt;% addTiles() It appears that s1.gcs was properly transformed. Next, we’ll explore other transformations using a built in dataset of the world from the maptools package. library(maptools) data(wrld_simpl) # Let&#39;s check it&#39;s current coordinate system proj4string(wrld_simpl) [1] &quot; +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0&quot; # Let&#39;s check its extent wrld_simpl@bbox min max x -180 180.00000 y -90 83.57027 The following chunk transforms the worl map to an Azimuthal equidistant projection centered on latitude 0 and longitude 0. wrld_simpl.ae &lt;- spTransform(wrld_simpl, CRS(&quot;+proj=aeqd +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;)) library(tmap) qtm(wrld_simpl.ae) + tm_layout(frame = FALSE) The following chunk transforms the worl map to an Azimuthal equidistant projection centered on Maine. wrld_simpl.aemaine &lt;- spTransform(wrld_simpl, CRS(&quot;+proj=aeqd +lat_0=44.5 +lon_0=-69.8 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;)) qtm(wrld_simpl.aemaine) + tm_layout(frame = FALSE) The following chunk transforms the worl map to a World Robinson projection. wrld_simpl.robin &lt;- spTransform(wrld_simpl, CRS(&quot;+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;)) qtm(wrld_simpl.robin) + tm_style_natural() + tm_layout(frame = FALSE) The following chunk transforms the worl map to a World sinusoidal projection. wrld_simpl.sin &lt;- spTransform(wrld_simpl, CRS(&quot;+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;)) qtm(wrld_simpl.sin) + tm_layout(frame = FALSE) Example of failed transformations As of maptools version 0.8-39, the following projection will generate an error. # Mercator world (54004) wrld_simpl.mercator &lt;- spTransform(wrld_simpl, CRS(&quot;+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;)) non finite transformation detected: [,1] [,2] [,3] [,4] Error in .spTransform_Polygon(input[[i]], to_args = to_args, from_args = from_args, : failure in Polygons 145 Polygon 1 points In addition: Warning message: In .spTransform_Polygon(input[[i]], to_args = to_args, from_args = from_args, : 2 projected point(s) not finite The error suggests that there is a problem with (at least) one of the polygons–polygon 145. We’ll create a function that will loop through each spatial element (polygon) and attempt a transformation to confirm that polygon 145 is indeed the sole problematic polygon. prob.poly &lt;- function(s){ n.poly &lt;- length(s@polygons) crd &lt;- &quot;+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot; for (i in 1:n.poly){ t1 &lt;- wrld_simpl[i, ] tryCatch(t1.merc &lt;- spTransform(t1, CRS(crd)), error = function(e){print(sprintf(&quot;Could not project polygon %i&quot;, i))}, warning = function(w){}) } } Now apply the function to the wrld_simpl object. # Run function with data prob.poly(wrld_simpl) Polygon 145 is indeed the lone problematic shape. Let’s plot it in its native coordinate system. qtm(wrld_simpl[145, ] ) The problematic polygon covers Antarctica. Let’s transform all polygons except polygon 145. wrld_simpl.mercator &lt;- spTransform(wrld_simpl[-145,], CRS(&quot;+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;)) qtm(wrld_simpl.mercator) + tm_style_natural() + tm_layout(frame = FALSE) This works, but we loose a large polygon. One possible solution is to clean the polygon’s geometry inside of R or in a GIS software such as QGIS or ArcMap. This step is not covered here but can be found via many online resources. Another issue that can come up when transforming spatial data is when the location of the tangent line(s) or point in the CS definition forces polygon features to split across the 180° meridian. For example, re-centering the mercator projection to -69° will create the following map. wrld_simpl.mercator &lt;- spTransform(wrld_simpl[-145,], CRS(&quot;+proj=merc +lon_0=-69 +k=1 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs&quot;)) qtm(wrld_simpl.mercator) The polygons are split and R does not know how to piece them together. Unfortunately, there are no easy workarounds to this other than converting the polygon to a polyline, splitting, and then rebuilding the polygon. Another solution is to convert the vector object to a raster then reproject the raster. "],
["vector-operations-in-r.html", "Vector operations in R Dissolve by contiguous shape Dissolve by attribute Calculate area Subsetting Intersecting Unioning", " Vector operations in R We’ll first load spatial objects used in this exercise from a remote website: a polygon object that delineates Maine counties; another polygon object that delineates distances to Augusta (Maine) as concentric circles; and a line object that shows the highway system that runs through Maine. These data are already stored as R data objects thus eliminating the need for any data conversion. z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/Income_schooling.rds&quot;)) s1 &lt;- readRDS(z) z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/dist.rds&quot;)) s2 &lt;- readRDS(z) z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/highway.rds&quot;)) l1 &lt;- readRDS(z) A map of the above layers is shown below. library(tmap) # Get min/max bounding box for both polygon shapes library(sp) b1 &lt;- bbox(s1) b2 &lt;- bbox(s2) b3 &lt;- pmax(b1,b2) b3[,1] &lt;- pmin(b1[,1],b2[,1]) # Plot all three features (note the bbox=b3 parameter that ensures that # the map extent encompasses all polygon extents) tm_shape(s1, bbox = b3) + tm_fill(col=&quot;grey&quot;) + tm_borders(col = &quot;white&quot;) + tm_shape(s2) + tm_fill(alpha = 0.2, col = &quot;red&quot;) + tm_borders(col = &quot;red&quot;) + tm_shape(l1) + tm_lines(col=&quot;yellow&quot;) The attributes table for both polygon objects are shown next. Note that each shape object has a unique set of attributes as well as a unique number of records Figure 1.3: Attribute tables for the Maine spatial object, s1, (left table) and the distance to Augusta spatial object, s2 (right table). Dissolve by contiguous shape To dissolve all polygons that share at least one line segment, simply pass the object name to raster’s aggregate function. In this example, we dissolve all polygons to create a single outline of the state of Maine. library(raster) ME &lt;- aggregate(s1) qtm(ME) Dissolve by attribute First, we’ll create a new column whose value is binary (TRUE/FALSE) depending on whether or not the county income is below the counties’ median income value. s1$med &lt;- s1$Income &gt; median(s1$Income) tm_shape(s1) + tm_fill(col=&quot;med&quot;) +tm_borders(col = &quot;white&quot;) Next, we’ll dissolve all polygons by the med value. Any polygons sharing at least one line segment having the same med value will be dissolved into a single polygon. ME.inc &lt;- aggregate(s1, by= &quot;med&quot;) tm_shape(ME.inc) + tm_fill(col=&quot;med&quot;) +tm_borders() The aggregation function will, by default, elliminate all other attribute values. If you wish to summarize other attribute values along with the attribute used for aggregation use dplyr’s piping operation. For example, to compute the median Income value for each of the below/above median income groups type the following: To summarize education by below/above median income library(dplyr) ME.inc$Income &lt;- s1@data %&gt;% group_by(med) %&gt;% summarize(medinc = median(Income)) %&gt;% .$medinc tm_shape(ME.inc) + tm_fill(col=&quot;Income&quot;) +tm_borders() To view the attributes table with both the aggregate variable, med, and the median income variable, Income, type: ME.inc@data med Income 1 FALSE 21518 2 TRUE 27955 Calculate area To calculate a polygon’s area, you can use rgeos’s gArea function. For example, to compute the area in km2, type the following: library(rgeos) ME.inc$Area &lt;- gArea(ME.inc, byid=TRUE) / 1000000 The gArea function computes the area in map units which happens to be in meters in our example. To convert the area from m2 to km2, we simply divide by 1,000,000 in the above chunk of code. This produces the following attributes table. ME.inc@data med Income Area 1 FALSE 21518 67743.58 2 TRUE 27955 15503.19 Subsetting You can use conventional R dataframe manipulation operations to subset by attribute values. For example, to subset by county name (e.g. Kennebec county), type: ME.ken &lt;- s1[s1$NAME == &quot;Kennebec&quot;,] qtm(ME.ken) To subset by a range of attribute values (e.g. subset by income values that are less than the median value), type: ME.inc2 &lt;- s1[s1$Income &lt; median(s1$Income), ] qtm(ME.inc2) Intersecting To intersect two polygon objects, you can use raster’s intersect function. library(raster) clp1 &lt;- intersect(s1,s2) tm_shape(clp1) + tm_fill(col=&quot;Income&quot;) +tm_borders() intersect keeps all features that overlap along with their combined attributes. Note that new polygons are created which can increase the size of the attributes table beyond the size of the combined input attributes table. Intersecting polygons with line and point objects This method will also intersect line and point objects with a polygon object, but the output will be either a line/point or a polygon depending on the order in which the objects are passed to intersect. If the line object is passed first, the output will be a line object that falls within the polygons of the input polygon object. If the polygon object is passed first, then all polygons that have a line segment within their boundaries will be returned. For example: To output all counties having at least one segment of the line feature running through them, type: library(raster) clp2 &lt;- intersect(s1,l1) tm_shape(clp2) + tm_fill(col=&quot;grey&quot;) +tm_borders() + tm_shape(l1) + tm_lines(col=&quot;red&quot;) To output all line segments that fall within the concentric distance circles of s2, type: library(raster) clp3 &lt;- intersect(l1,s2) tm_shape(s2) + tm_fill(col=&quot;grey&quot;) +tm_borders() + tm_shape(clp3) + tm_lines(col=&quot;red&quot;) In both cases, the output shape objects inherit both the original attribute values as well as the attributes from the intersecting object. Unioning To union two polygon objects, use raster’s union function. For example, library(raster) un1 &lt;- union(s1,s2) tm_shape(un1) + tm_fill(col=&quot;Income&quot;) + tm_borders() This produces the following attributes table. "],
["raster-operations-in-r.html", "Raster operations in R Local operations and functions Focal operations and functions Zonal operations and functions Global operations and functions Computing cumulative distances", " Raster operations in R We’ll first load spatial objects used in this exercise from a remote website: an elevation raster, a bathymetry raster and a continents polygon vector layer. These objects will be loaded as R spatial objects and will therefore not require conversion. library(raster) z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/elevation.rds&quot;)) r1 &lt;- readRDS(z) z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/bathymetry.rds&quot;)) r2 &lt;- readRDS(z) z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/continents.rds&quot;)) s1 &lt;- readRDS(z) Both rasters cover the entire globe. Elevation below mean sea level are encoded as 0 in the elevation raster. Likewise, bathymetrty values above mean sea level are encoded as 0. Note that most of the map algebra operations and functions covered in this tutorial are implemented using the raster package. Local operations and functions Unary operations and functions (applied to single raster) Most algebreic operations can be applied to rasters as they would with any vector element. For example, to convert all bathymetric values in r2 (currently recorded as postive values) to negative values simply multiply the raster by -1. library(raster) r2b &lt;- r2 * (-1) Another unary operation that can be applied to a raster is reclassification. In the following example, we will assign all values in r2b less than zero 1 and all zero values will remain unchanged. A simple way to do this is to apply a conditional statement. r2c &lt;- r2b &lt; 0 Let’s look at the output. Note that all 0 pixels are coded as FALSE and all 1 pixels are coded as TRUE. library(tmap) tm_shape(r2c) + tm_raster(palette = &quot;Greys&quot;) + tm_legend(outside = TRUE, text.size = .8) If a more elaborate form of reclassification is desired, you can use the reclassify function. In the following example, the raster object r2 is reclassified to 4 unique values: 100, 500, 1000 and 11000 as follows: Original depth values Reclassified values 0 - 100 100 101 - 500 500 501 - 1000 1000 1001 - 11000 11000 The first step is to create a plain matrix where the first and second columns list the starting and ending values of the range of r values that are to be reclassified, and where the third column lists the new raster cell values. m &lt;- c(0, 100, 100, 100, 500, 500, 500, 1000, 1000, 1000, 11000, 11000) m &lt;- matrix(m, ncol=3, byrow = T) m [,1] [,2] [,3] [1,] 0 100 100 [2,] 100 500 500 [3,] 500 1000 1000 [4,] 1000 11000 11000 r2c &lt;- reclassify(r2, m, right = T) The right=T parameter indicates that the intervals should be closed to the right (i.e. inclusive). tm_shape(r2c) + tm_raster(style=&quot;cat&quot;) + tm_legend(outside = TRUE, text.size = .8) You can also assign NA (missing) values to pixels. For example, to assign NA values to cells that are equal to 100, type r2c[r2c == 100] &lt;- NA The following chunk of code shows all pixels whose value is NA in a grey color. tm_shape(r2c) + tm_raster(showNA=TRUE, colorNA=&quot;grey&quot;) + tm_legend(outside = TRUE, text.size = .8) Binary operations and functions (where two layers are used) In the following example, r1 (elevation raster) is added to r2 (bathymetry raster) to create a single elevation raster for the globe. Note that the bathymetric raster will need to be muliplied by -1 to differentiate above mean sea level elevation from bellow mean sea level. elev &lt;- r1 - r2 tm_shape(elev) + tm_raster(palette=&quot;-RdBu&quot;,n=6) + tm_legend(outside = TRUE, text.size = .8) Focal operations and functions Operations or functions applied focally to rasters involve the cell whose value we are computing and all user defined neighboring cells. For example, a cell output value can be the average of all 121 cells–an 11x11 kernel–centered on the cell whose value is being estimated (this acts as a smoothing function). Next, we apply the focal function to r2. f1 &lt;- focal(elev, w=matrix(1,nrow=11,ncol=11) , fun=mean) tm_shape(f1) + tm_raster(palette=&quot;-RdBu&quot;,n=6) + tm_legend(outside = TRUE, text.size = .8) Notice that the edge cells have been assigned a value of NA. This is because cells outside of the extent have no value. You can remedy this by passing the parameters na.rm=TRUE and pad=TRUE . f1 &lt;- focal(elev, w=matrix(1,nrow=11,ncol=11) , fun=mean, pad=TRUE, na.rm = TRUE) tm_shape(f1) + tm_raster(palette=&quot;-RdBu&quot;,n=6) + tm_legend(outside = TRUE, text.size = .8) The neighbors matrix (or kernel) that defines the moving window can be customized. For example if we wanted to compute the average of all 8 neighboring cells excluding the central cell we could write the following: m &lt;- matrix(c(1,1,1,1,0,1,1,1,1)/8,nrow = 3) f2 &lt;- focal(elev, w=m, fun=sum) More complicated kernels can be defined. In the following example, a Sobel filter (used for edge detection in image processing) is defined then applied to the raster layer elev. Sobel &lt;- matrix(c(-1,0,1,-2,0,2,-1,0,1) / 4, nrow=3) f3 &lt;- focal(elev, w=Sobel, fun=sum) tm_shape(f3) + tm_raster(palette=&quot;Greys&quot;) + tm_legend(legend.show = FALSE) Zonal operations and functions A common zonal operation is the aggregation of cells. In the following example, raster layer elev is aggregated to a 5x5 raster layer. z1 &lt;- aggregate(elev, fact=2, fun=mean, expand=TRUE) tm_shape(z1) + tm_raster(palette=&quot;-RdBu&quot;,n=6) + tm_legend(outside = TRUE, text.size = .8) The image may not look much different from the original, but a look at the image properties will show a difference in pixel sizes. res(elev) [1] 0.3333333 0.3333333 res(z1) [1] 0.6666667 0.6666667 z1’s pixel dimensions are half of elev’s dimensions. You can reverse the process by using the disaggregate function which will split a cell into the desired number of subcells while assigning each one the same parent cell value. Zonal operations can often involve two layers, one with the values to be aggregated, the other with the defined zones. In the next example, elev’s cell values are averaged by zones defined by the raster layer l4. First, we generate a table with summed cell values r for each each zone l4, s1.elev &lt;- extract(elev, s1, fun=mean, sp=TRUE) The sp=TRUE parameter instructs the function to output a SpatialPolygonsDataFrame object (same as the input zonal object) instead of a standalone table (matrix). The output spatial object inherits the original attributes and adds a column called layer with the computed mean elevation values. s1.elev@data CONTINENT layer 0 Africa 630.6979 1 Antarctica 2370.8633 2 Asia 790.6696 3 Australia 276.5497 4 Europe 262.7611 5 North America 826.4162 6 South America 595.6067 We can map the average elevation by continent. tm_shape(s1.elev) + tm_polygons(col=&quot;layer&quot;) + tm_legend(outside = TRUE, text.size = .8) Many custom functions can be applied to extract. For example, to extract the maximum elevation value by continent, type: s1.elev &lt;- extract(elev, s1, fun=max, sp=TRUE) As another example, we may wish to extract the number of cells in each polygon. s1.elev &lt;- extract(elev, s1, fun=function(x,...){length(x)}, sp=TRUE) The extract function will also work with lines and points spatial objects. If you wish to compute the zonal statistics of a raster using a another raster as zones instead of a polygon, use the zonal function instead. Global operations and functions Global operations and functions may make use of all input cells of a grid in the computation of an output cell value. An example of a global function is the Euclidean distance tool, distance, which computes the shortest distance between a pixel and a source (or destination) location. To demonstrate the distance function, we’ll first create a new raster layer with two non-NA pixels. r1 &lt;- raster(ncol=100, nrow=100, xmn=0, xmx=100, ymn=0, ymx=100) r1[] &lt;- NA # Assign NoData values to all pixels r1[c(850, 5650)] &lt;- 1 # Change the pixels 20 and 85 to 1 tm_shape(r1) + tm_raster(palette=&quot;red&quot;) + tm_legend(outside = TRUE, text.size = .8) Next, we’ll compute a Euclidean distance raster from these two cells. The output extent will default to the input raster extent. r1.d &lt;- distance(r1) tm_shape(r1.d) + tm_raster(palette = &quot;Greens&quot;, style=&quot;order&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_shape(r1) + tm_raster(palette=&quot;red&quot;) You can also compute a distance raster using SpatialPoints objects or a simple x,y data table. In the following example, distances to points (25,30) and (87,80) are computed for each output cell. We will first need to create a blank raster (which will define the extent of the Euclidean raster output). We then create a new SpatialPoints object from a basic table. r2 &lt;- raster(ncol=100, nrow=100, xmn=0, xmx=100, ymn=0, ymx=100) xy &lt;- matrix(c(25,30,87,80),nrow=2, byrow=T) p1 &lt;- SpatialPoints(xy) Now let’s compute the Euclidean distance to these points. r2.d &lt;- distanceFromPoints(r2, p1) tm_shape(r2.d) + tm_raster(palette = &quot;Greens&quot;, style=&quot;order&quot;) + tm_legend(outside = TRUE, text.size = .8) + tm_shape(p1) + tm_bubbles(col=&quot;red&quot;) Computing cumulative distances This exercise demonstrates how to use functions from the gdistance package to generate a cumulative distance raster. One objective will be to demonstrate the influence “adjacency cells” wields in the final results. Load the gdistance package. library(gdistance) First, we’ll create a 100x100 raster and assign a value of 1 to each cell. If you were to include traveling costs other than distance (e.g. elevation) you would assign those values to each cell instead of a constant value of 1. r &lt;- raster(nrows=100,ncols=100,xmn=0,ymn=0,xmx=100,ymx=100) r[] &lt;- rep(1, ncell(r)) A translation matrix allows one to define a ‘traversing’ cost (other than distance) to go from one cell to an adjacent cell. For example, differences in height between two adjacent cell could be defined as a ‘cost’. In our example, we will assume that there are no ‘costs’ (other than distance) in traversing from one cell to any adjacent cell; we therefore assign a value of 1, (function(x){1}), to the translation between a cell and its adjacent cells (i.e. translation cost is uniform in all directions). There are four different ways in which ‘adjacency’ can be defined using the transition function. These are showcased in the next four blocks of code. In this example, adjacency is defined as a four node (vertical and horizontal) connection (i.e. a “rook” move). h4 &lt;- transition(r, transitionFunction = function(x){1}, directions = 4) In this example, adjacency is defined as an eight node connection (i.e. a single cell “queen” move). h8 &lt;- transition(r, transitionFunction = function(x){1}, directions = 8) In this example, adjacency is defined as a sixteen node connection (i.e. a single cell “queen” move combined with a “knight” move). h16 &lt;- transition(r, transitionFunction=function(x){1},16,symm=FALSE) In this example, adjacency is defined as a four node (diagonal) connection (i.e. a single cell “bishop” move). hb &lt;- transition(r, transitionFunction=function(x){1},&quot;bishop&quot;,symm=FALSE) The transition function treats all adjacent cells as being at an equal distance from the source cell across the entire raster. geoCorrection corrects for ‘true’ local distance. In essence, it’s adding an additional cost to traversing from one cell to an adjacent cell (the original cost being defined using the transition function). The importance of applying this correction will be shown later. Note: geoCorrection also corrects for distance distortions associated with data in a geographic coordinate system. To take advantage of this correction, make sure to define the raster layer’s coordinate system using the projection function. h4 &lt;- geoCorrection(h4, scl=FALSE) h8 &lt;- geoCorrection(h8, scl=FALSE) h16 &lt;- geoCorrection(h16, scl=FALSE) hb &lt;- geoCorrection(hb, scl=FALSE) In the “queen’s” case, the diagonal neighbors are \\(\\sqrt{2 x (CellWidth)^{2}}\\) times the cell width distance from the source cell. Next we will map the cumulative distance (accCost) from a central point (A) to all cells in the raster using the four different adjacency definitions. A &lt;- c(50,50) # Location of source cell h4.acc &lt;- accCost(h4,A) h8.acc &lt;- accCost(h8,A) h16.acc &lt;- accCost(h16,A) hb.acc &lt;- accCost(hb,A) If the geoCorrection function had not been applied in the previous steps, the cumulative distance between point location A and its neighboring adjacent cells would have been different. Note the difference in cumulative distance for the 16-direction case as shown in the next two figures. Uncorrected (i.e. geoCorrection not applied to h16): Corrected (i.e. geoCorrection applied to h16): The “bishop” case offers a unique problem: only cells in the diagonal direction are identified as being adjacent. This leaves many undefined cells (labeled as Inf). We will change the Inf cells to NA cells. hb.acc[hb.acc == Inf] &lt;- NA Now let’s compare a 7x7 subset (centered on point A) between the four different cumulative distance rasters. To highlight the differences between all four rasters, we will assign a red color to all cells that are within 20 cell units of point A. It’s obvious that the accuracy of the cumulative distance raster can be greatly influenced by how we define adjacent nodes. The number of red cells (i.e. area identified as being within a 20 units cumulative distance) ranges from 925 to 2749 cells. Working example In the following example, we will generate a raster layer with barriers (defined as NA cell values). The goal will be to identify all cells that fall within a 290 km traveling distance from the upper left-hand corner of the raster layer. Results between an 8-node and 16-node adjacency definition will be compared. # create an empty raster r &lt;- raster(nrows=300,ncols=150,xmn=0,ymn=0,xmx=150000, ymx=300000) # Define a UTM projection (this sets map units to meters) projection(r) = &quot;+proj=utm +zone=19 +datum=NAD83&quot; # Each cell is assigned a value of 1 r[] &lt;- rep(1, ncell(r)) # Generate &#39;baffles&#39; by assigning NA to cells. Cells are identified by # their index and not their coordinates. # Baffles need to be 2 cells thick to prevent the 16-node # case from &quot;jumping&quot; a one pixel thick NA cell. a &lt;- c(seq(3001,3100,1),seq(3151,3250,1)) a &lt;- c(a, a+6000, a+12000, a+18000, a+24000, a+30000, a+36000) a &lt;- c(a , a+3050) r[a] &lt;- NA # Let&#39;s check that the baffles are properly placed tm_shape(r) + tm_raster(col=&quot;Grey&quot;) + tm_legend(legend.show=FALSE) # Next, generate a transition matrix for the 8-node case and the 16-node case h8 &lt;- transition(r, transitionFunction = function(x){1}, directions = 8) h16 &lt;- transition(r, transitionFunction = function(x){1}, directions = 16) # Now assign distance cost to the matrices. h8 &lt;- geoCorrection(h8) h16 &lt;- geoCorrection(h16) # Define a point source. A &lt;- SpatialPoints(cbind(50,290000)) # Compute the cumulative cost raster h8.acc &lt;- accCost(h8, A) h16.acc &lt;- accCost(h16,A) # Replace Inf with NA h8.acc[h8.acc == Inf] &lt;- NA h16.acc[h16.acc == Inf] &lt;- NA Let’s plot the results. Yellow cells will identify cumulative distances within 290 km. tm_shape(h8.acc) + tm_raster(n=2, style=&quot;fixed&quot;, breaks=c(0,290000,Inf)) + tm_facets() + tm_shape(A) + tm_bubbles(col=&quot;red&quot;) + tm_legend(outside = TRUE, text.size = .8) tm_shape(h16.acc) + tm_raster(n=2, style=&quot;fixed&quot;, breaks=c(0,290000,Inf)) + tm_facets() + tm_shape(A) + tm_bubbles(col=&quot;red&quot;) + tm_legend(outside = TRUE, text.size = .8) We can compute the difference between the 8-node and 16-node cumulative distance rasters: table(h8.acc[] &lt;= 290000) FALSE TRUE 31458 10742 table(h16.acc[] &lt;= 290000) FALSE TRUE 30842 11358 The number of cells identified as being within a 290 km cumulative distance of point A for the 8-node case is 10742 whereas it’s 11358 for the 16-node case, a difference of 5.4%. "],
["point-pattern-analysis-in-r.html", "Point pattern analysis in R Density based analysis Distance based analysis Hypothesis tests", " Point pattern analysis in R Most point pattern analysis tools are available in the spatstat package. These tools are designed to work with points stored as ppp objects and not SpatialPointsDataFrame objects. A ppp may or may not have attribute information (also referred to as marks). Knowing whether or not a function requires that an attributes table be present in the ppp object matters if a the operation is to complete successfully. In the following chunk of code, a point feature of Starbucks stores in Massachusetts is downloaded from the website as a SpatialPointsDataFrame then converted to a ppp object. Converting a SpatialPointsDataFrame to a ppp requires the use of the maptools package. Note the intermediate step that strips the point object of a dataframe (attributes table) before being converted to a mark free ppp object. library(maptools) z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/starbucks.rds&quot;)) S1 &lt;- readRDS(z) SP &lt;- as(S1, &quot;SpatialPoints&quot;) p2 &lt;- as(SP, &quot;ppp&quot;) We will also load a polygon outline of the state of Massachusetts. It will be used to define the extent of the study area (this is needed for hypothesis testing later in this tutorial). It too must be converted to a spatstat readable format. We’ll convert it to an owin object. z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/ma.rds&quot;)) S2 &lt;- readRDS(z) W &lt;- as(S2, &quot;owin&quot;) Some spatstat applications make use of rasters which must be stored as an im object. The following chunk of code downloads a raster of Massachusetts population density. library(spatstat) # Needed for function as.im() z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/pop_sqmile.rds&quot;)) r &lt;- readRDS(z) pop &lt;- as.im(r) # Convert r object to an im object The above datasets will be used later in this tutorial. Note that im objects created by spatstat can be converted to a raster object using the raster() function from the raster package. This can be helpful if you wish to use a mapping application such as tmap to map the rasters instead of the base plotting function. Density based analysis Quadrat density You can compute the quadrat count and intensity using spatstat’s quadratcount() and intensity() functions. We’ll use a built-in dataset for this example. library(spatstat) P &lt;- bei Q &lt;- quadratcount(P, nx= 6, ny=3) The object Q stores the number of points inside each quadrat. You can plot the quadrats along with the counts as follows: plot(P, pch=20, cols=&quot;grey70&quot;, main=NULL) plot(Q, add=TRUE) You can compute the density of points within each quadrat as follows: # Compute the density for each quadrat Q.d &lt;- intensity(Q) # Plot the density plot( intensity(Q, image=TRUE), main=NULL) plot(P, pch=20, cex=0.6, col=rgb(0,0,0,.2), add=TRUE) Quadrat density on a tesselated surface We’ll work off of an internal dataset. library(spatstat) P &lt;- bei # Use built-in point dataset elev &lt;- bei.extra$elev # Use built-in covariate Next, we’ll divide the covariate into regions (aka tesselated surfaces). brk &lt;- c( 110, 135, 145, 155 , 160) Zcut &lt;- cut(elev, breaks=brk, labels=1:4) E &lt;- tess(image=Zcut) Next, compute the quadrat counts within each tesselated area then compute the density. Q &lt;- quadratcount(P, tess = E) Q.d &lt;- intensity(Q) Q.d tile 1 2 3 4 0.003219228 0.007573599 0.009870740 0.001859664 Plot the density values across each tesselated region. plot( intensity(Q, image=TRUE), main=NULL) plot(P, pch=20, cex=0.6, col=rgb(0,0,0,.2), add=TRUE) Let’s modify the color scheme. cl &lt;- interp.colours(c(&quot;lightyellow&quot;, &quot;orange&quot; ,&quot;red&quot;), E$n) plot( intensity(Q, image=TRUE), col=cl, main=NULL) plot(P, pch=20, cex=0.6, col=rgb(0,0,0,.2), add=TRUE) Kernel density raster The spatstat package has a function called density which implements a gaussian kernel function. Its bandwidth defines the kernel’s window extent. library(spatstat) P &lt;- bei # Using the default bandwidth K1 &lt;- density(P) plot(K1, main=NULL) contour(K1, add=TRUE) # Using a custom (smaller) bandwidth of 40 map units K2 &lt;- density(P, sigma=40) plot(K2, main=NULL) contour(K2, add=TRUE) # Using a custom (larger) bandwidth of 50 map units K3 &lt;- density(P, sigma=500) plot(K3, main=NULL) contour(K3, add=TRUE) Kernel Density Adjusted for Covariate In the following example, a kernel density map is generated using an elevation raster as a covariate. The outputs include a plot of \\(\\rho\\) vs. elevation and a raster map of \\(\\rho\\). library(spatstat) P &lt;- bei # Use built-in point dataset elev &lt;- bei.extra$elev # Use built-in covariate # Compute rho using the ratio method rho &lt;- rhohat(P, elev, method=&quot;ratio&quot;) # Generate rho vs covariate plot plot(rho, main=NULL) # Generate map of rho pred &lt;- predict(rho) cl &lt;- interp.colours(c(&quot;lightyellow&quot;, &quot;orange&quot; ,&quot;red&quot;), 100) # Create color scheme plot(pred, col=cl, las=2, main=NULL) Modeling intensity as a function of a covariate In this example, we make use of the Starbucks and population density datasets loaded at the beginning of this tutorial. First, we’ll map the data. The population density raster is mapped on a log scale. library(tmap) tm_shape(r) + tm_raster(style=&quot;quantile&quot;, palette = &quot;Greys&quot;) + tm_shape(S1) + tm_bubbles(col=&quot;red&quot;, alpha=0.5, border.col = &quot;yellow&quot;, border.lwd = 0.5) + tm_legend(outside = TRUE, text.size = .8) Next, we’ll generate the Poisson point process model and plot the results. Note that we are using the ppp representation of the Starbucks points (object p2) and not the SpatialPointsDataFrame representation (object S1). library(spatstat) # Create the Poisson point process model PPM1 &lt;- ppm(p2 ~ pop) # Plot the relationship plot(effectfun(PPM1, &quot;pop&quot;, se.fit=TRUE), main=NULL, cex.axis=0.6,cex.lab=0.6, legend=FALSE) Distance based analysis Average nearest neighbor analysis library(spatstat) P &lt;- bei To compute the average first nearest neighbor distance (1st order): mean(nndist(P, k=1)) [1] 4.329677 To compute the average second nearest neighbor distance (2nd order): mean(nndist(P, k=2)) [1] 6.473149 To plot average distance as a function of neighbor order for the first 100 closest neighbors: ANN &lt;- apply(nndist(P, k=1:100),2,FUN=mean) plot(ANN ~ eval(1:100), type=&quot;b&quot;, main=NULL ) The bottom axis shows the neighbor order number and the left axis shows the average distance. K and L functions library(spatstat) P &lt;- bei To compute the K function: K &lt;- Kest(P) plot(K, main=NULL) To compute the L function: L &lt;- Lest(P, main=NULL) plot(L) To plot the L function with the Lexpected line set horizontal: plot(L, . -r ~ r) The Pair Correlation Function g library(spatstat) P &lt;- swedishpines To compute the pair correlation function. g &lt;- pcf(P) plot(g, main=NULL) Hypothesis tests Test for clustering First, we’ll compute the ANN for Walmart locations assuming uniform density across the state (i.e. assuming that there are no underlying factors at play). Compute the observed data’s ANN value, library(spatstat) ann.p &lt;- mean(nndist(p2, k=1)) Now run the MC simulation using a homogeneous point pattern density process n &lt;- 599 # This tells the model how many times to run the simulation ann.r &lt;- vector() # Create an empty object to be used to store simulated values for (i in 1:n){ rand.p &lt;- rpoint(n=p2$n, win=W) # Generate the random point locations ann.r[i] &lt;- mean(nndist(rand.p, k=1)) # Tally the ANN values } In the above loop, the function rpoint is passed two parameters: n=p2k$n and win=W. The first tells the function how many points to randomly generate (p2$n extracts the number of points in object p2). The second tells the function to confine the points to the extent defined by w (the state of MA boundary). Next, let’s plot the histogram and add a blue line showing where our observed ANN value lies. hist(ann.r, breaks=40, col=&quot;bisque&quot;, xlim=range(ann.p, ann.r)) abline(v=ann.p, col=&quot;blue&quot;) It’s obvious from the test that the observed ANN is far smaller than the expected ANN values under the null hypothesis that the stores are randomly distributed across the state. A smaller value indicates that the stores are far more clustered than expected under the null. Next, we’ll run the same test but add the influence due to population distribution. This is a non-homogeneous test. n &lt;- 599 ann.r &lt;- vector() for (i in 1:n){ rand.p &lt;- rpoint(n=p2$n, f=pop) ann.r[i] &lt;- mean(nndist(rand.p, k=1)) } Here, we pass the parameter f=pop to the function rpoint telling it that the population density raster pop should be used to define where a point should be most likely placed (high population density) and least likely placed (low population density). Next, let’s plot the histogram and add a blue line showing where our observed ANN value lies. hist(ann.r, breaks=40, col=&quot;bisque&quot;, xlim=range(ann.p, ann.r)) abline(v=ann.p, col=&quot;blue&quot;) Population density alone cannot explain the clustering given that the observed ANN value is less than the expected ANN values. Computing a pseudo p-value We’ll work off of the last simulation. First, we need to find the number of simulated ANN values greater than our observed ANN value N.greater &lt;- sum(ann.r &gt; ann.p) To compute the p-value, find the end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-sided P-value. See lecture notes for more information. p &lt;- min(N.greater + 1, n + 1 - N.greater) / (n +1) p [1] 0.001666667 In our working example, you’ll note that or simulated ANN value was nowhere near the range of ANN values computed under the null yet we don’t have a p-value of zero. This is by design since the strength of our estimated p will be proportional to the number of simulations–there is always a chance that one realization of a point pattern could produce an ANN value more extreme than ours. Testing for a covariate effect It’s important that the p2 object have its extent defined so as to match the extent of the population raster pop. Plotting p2 with the base plot function shows that the extent is currently defined by the smallest rectangle enclosing the points. plot(p2, main=NULL) We can impose the Massachusetts boundary as the new extent as follows: Window(p2) &lt;- W plot(p2, main=NULL) Now, we’ll fit the model that assumes that point density is a function of population (the alternative hypothesis). library(spatstat) PPM1 &lt;- ppm(p2 ~ pop) PPM1 Nonstationary Poisson process Log intensity: ~pop Fitted trend coefficients: (Intercept) pop -1.905198e+01 1.836711e-04 Estimate S.E. CI95.lo CI95.hi Ztest (Intercept) -1.905198e+01 9.039783e-02 -1.922915e+01 -1.887480e+01 *** pop 1.836711e-04 6.060545e-06 1.717927e-04 1.955496e-04 *** Zval (Intercept) -210.75700 pop 30.30604 Problem: Values of the covariate &#39;pop&#39; were NA or undefined at 0.57% (4 out of 699) of the quadrature points Next, we’ll fit the model that assumes that density is not a function of population (the null hypothesis). PPM0 &lt;- ppm(p2 ~ 1) PPM0 Stationary Poisson process Intensity: 8.268627e-09 Estimate S.E. CI95.lo CI95.hi Ztest Zval log(lambda) -18.6108 0.07647191 -18.76068 -18.46092 *** -243.3678 In our working example, the null model (homogeneous intensity) takes on the form: \\[ \\lambda(i) = e^{-18.6} \\] The alternate model takes on the form: \\[ \\lambda(i) = e^{-19.1 + 1.8^{-4}population} \\] The models are then compared using the likelihood ratio test which produces the following output: anova(PPM0, PPM1, test=&quot;LRT&quot;) Npar Df Deviance Pr(&gt;Chi) 5 NA NA NA 6 1 347.4234 0 The value under the heading PR(&gt;Chi) is the p-value which gives us the probability we would be wrong in rejecting the null. Here p~0 suggesting that there is close to a 0% chance that we would be remiss to reject the base model in favor of the alternate model–put another way, the alternate model (that population density can help explain the distribution of Starbucks) is a significant improvement over the null. "],
["spatial-autocorrelation-in-r.html", "Spatial Autocorrelation in R Define neighboring polygons Computing the Moran’s I statistic: the hard way Computing the Moran’s I statistic: the easy way Moran’s I as a function of a distance band", " Spatial Autocorrelation in R The spdep functions used in this exercise make use of different spatial object classes than those used by the spatstat package. Here, we’ll load layers as SpatialPolygonsDataFrame objects. If we were to work with point data instead, we would store the point shapefiles as SpatialPointsDataFrame. We’ll first load the spatial object used in this exercise from a remote website–income and education data aggregated at the county level for the state of Maine. This object will be loaded as a SpatialPolygonsDataFrame and will therefore not require conversion. z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/Income_schooling.rds&quot;)) s1 &lt;- readRDS(z) The spatial object has five attributes. The one of interest for this exercise is Income (per capita, in dollars). Let’s map the income distribution using a quantile classification scheme. library(tmap) tm_shape(s1) + tm_polygons(style=&quot;quantile&quot;, col = &quot;Income&quot;) + tm_legend(outside = TRUE, text.size = .8) Define neighboring polygons The first step requires that we define “neighboring” polygons. This could refer to contiguous polygons, polygons within a certain distance band, or it could be non-spatial in nature and defined by social, political or cultural “neighbors”. Here, we’ll adopt a contiguous neighbor definition where we’ll accept any contiguous polygon that shares at least on vertex (this is the “queen” case and is defined by setting the parameter queen=TRUE). If we required that at least one edge be shared between polygons then we would set queen=FALSE. library(spdep) nb &lt;- poly2nb(s1, queen=TRUE) For each polygon in our polygon object, nb lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type: nb[1] [[1]] [1] 2 3 4 5 Polygon 1 has 4 neighbors. The first polygon is associated with the attribute name Aroostook: s1$NAME[1] [1] Aroostook 16 Levels: Androscoggin Aroostook Cumberland Franklin Hancock ... York The four neighboring polygons are associated with the names: s1$NAME[c(2,3,4,5)] [1] Somerset Piscataquis Penobscot Washington 16 Levels: Androscoggin Aroostook Cumberland Franklin Hancock ... York Next, we need to assign weights to each neighboring polygon. In our case, each polygon will be assigned equal weight when computing the neighboring mean values. lw &lt;- nb2listw(nb, style=&quot;W&quot;, zero.policy=TRUE) To see the weight of the first polygon’s four neighbors type: lw$weights[1] [[1]] [1] 0.25 0.25 0.25 0.25 Each neighbor is assigned a quarter of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.25 before being tallied. Finally, we’ll compute the average neighbor income value for each polygon. These values are often referred to as spatially lagged values. Inc.lag &lt;- lag.listw(lw, s1$Income) The following table shows the average neighboring income values (stored in the Inc.lag object) for each county. Computing the Moran’s I statistic: the hard way We can plot lagged income vs. income and fit a linear regression model to the data. # Create a regression model M &lt;- lm(Inc.lag ~ s1$Income) # Plot the data OP &lt;- par(pty=&quot;s&quot;) # Force a square plot plot( Inc.lag ~ s1$Income, pch=20, asp=1) abline(M, col=&quot;red&quot;) # Add the regression line from model M par(OP) # Revert back to default plot settings The slope of the regression line is the Moran’s I coefficient coef(M)[2] s1$Income 0.2828111 To assess if the slope is significantly different from zero, we can randomly permute the income values (i.e. we are not imposing any spatial autocorrelation) across all counties then fit a regression model to each permuted set of values. We then tally the slope values and compare our observed slope (i.e. the Moran’s I value) to the distribution of Moran’s I values from the simulation. n &lt;- 599 # Define the number of simulations I.r &lt;- vector() # Create an empty vector for (i in 1:n){ # Randomly shuffle income values x &lt;- sample(s1$Income, replace=FALSE) # Compute new set of lagged values x.lag &lt;- lag.listw(lw, x) # Compute the regression slope M.r &lt;- lm(x.lag ~ x) I.r[i] &lt;- coef(M.r)[2] } # Plot histogram of simulated Moran&#39;s I values # then add our observed Moran&#39;s I value to the plot hist(I.r, main=NULL) abline(v=coef(M)[2], col=&quot;red&quot;) par(OP) The simulation suggests that our observed Moran’s I value is not consistent with a Moran’s I value one would expect to get if the income values were not spatially correlated. In the next step, we’ll compute a pseudo p-value. Computing a pseudo p-value from an MC simulation First, we need to find the number of simulated ANN values greater than our observed ANN value N.greater &lt;- sum(coef(M)[2] &gt; I.r) To compute the p-value, find the end of the distribution closest to the observed ANN value, then divide that count by the total count. Note that this is a so-called one-side P-value. See lecture notes for more information. p &lt;- min(N.greater + 1, n + 1 - N.greater) / (n + 1) p [1] 0.025 In our working example, the p-value suggests that there is a small chance (~ 2%) of being wrong in stating that the income values are not clustered at the county level. Computing the Moran’s I statistic: the easy way To get the Moran’s I value, simply use the moran.test function. moran.test(s1$Income,lw) Moran I test under randomisation data: s1$Income weights: lw Moran I statistic standard deviate = 2.2472, p-value = 0.01231 alternative hypothesis: greater sample estimates: Moran I statistic Expectation Variance 0.28281108 -0.06666667 0.02418480 Note that the p-value computed from the moran.test function is not computed from an MC silumation but analytically instead. This may not always prove to be the most accurate measure of significance. To test for significance using the MC simulation method instead, use the moran.mc function. MC&lt;- moran.mc(s1$Income, lw, nsim=599) # View results (including p-value) MC Monte-Carlo simulation of Moran I data: s1$Income weights: lw number of simulations + 1: 600 statistic = 0.28281, observed rank = 584, p-value = 0.02667 alternative hypothesis: greater # Plot the distribution (note that this is a density plot instead of a histogram) plot(MC, main=NULL) Moran’s I as a function of a distance band In this section, we will explore spatial autocorrelation as a function of distance bands. Instead of defining neighbors as contiguous polygons, we will define neighbors based on distances to polygon centers. We therefore need to extract the center of each polygon coo &lt;- coordinates(s1) The object coo stores all sixteen pairs of coordinate values. Next, we will define the search radius to include all neighboring polygon centers within 50 km (or 50,000 meters) S.dist &lt;- dnearneigh(coo, 0, 50000) The dnearneigh function takes on three parameters: the coordinate values coo, the radius for the inner radius of the annulus band, and the radius for the outer annulus band. In our example, the inner annulus radius is 0 which implies that all polygon centers up to 50km are considered neighbors. Note that if we chose to restrict the neighbors to all polygon centers between 50 km and 100 km, then we would define a search annulus (instead of a circle) as dnearneigh(coo, 50000, 100000). Now that we defined our search circle, we need to identify all neighboring polygons for each polygon in the dataset. lw &lt;- nb2listw(S.dist, style=&quot;W&quot;,zero.policy=T) Run the MC simulation. MI &lt;- moran.mc(s1$Income, lw, nsim=599,zero.policy=T) Plot the results. plot(MI) Display p-value and other summary statistics. MI Monte-Carlo simulation of Moran I data: s1$Income weights: lw number of simulations + 1: 600 statistic = 0.31361, observed rank = 595, p-value = 0.008333 alternative hypothesis: greater "],
["interpolation-in-r.html", "Interpolation in R Thiessen polygons IDW 1st order polynomial fit 2nd order polynomial Kriging", " Interpolation in R First, let’s load the data from the website. The data are stored as SpatialPointsDataFrame and SpatialPointsDataFrame objects. Most of the functions used in this exercise work off of these classes. The one exception is the direchlet function which requires a conversion to a ppp object. library(rgdal) library(tmap) # Load precipitation data z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/precip.rds&quot;)) P &lt;- readRDS(z) # Load Texas boudary map z &lt;- gzcon(url(&quot;http://colby.edu/~mgimond/Spatial/Data/texas.rds&quot;)) W &lt;- readRDS(z) # Replace point boundary extent with that of Texas P@bbox &lt;- W@bbox tm_shape(W) + tm_polygons() + tm_shape(P) + tm_dots(col=&quot;Precip_in&quot;, palette = &quot;RdBu&quot;, auto.palette.mapping = FALSE, title=&quot;Sampled precipitation \\n(in inches)&quot;, size=0.7) + tm_text(&quot;Precip_in&quot;, just=&quot;left&quot;, xmod=.5, size = 0.7) + tm_legend(legend.outside=TRUE) You’ll note the line P@bbox &lt;- W@bbox which forces the rectangular extent of the Texas map onto the point data object. This is an important step if the interpolation of the points are to cover the entire extent of Texas. Had this step been omitted, most of the interpolated layers would have been limited to the smallest rectangular extent enclosing the point object. Thiessen polygons The Thiessen polygons (or proximity interpolation) can be created using spatstat’s dirichlet function. library(spatstat) # Used for the dirichlet tessellation function library(maptools) # Used for conversion from SPDF to ppp library(raster) # Used to clip out thiessen polygons # Create a tessellated surface th &lt;- as(dirichlet(as.ppp(P)), &quot;SpatialPolygons&quot;) # The dirichlet function does not carry over projection information # requiring that this information be added manually proj4string(th) &lt;- proj4string(P) # The tessellated surface does not store attribute information # from the point data layer. We&#39;ll use the over() function (from the sp # package) to join the point attributes to the tesselated surface via # a spatial join. The over() function creates a dataframe that will need to # be added to the `th` object thus creating a SpatialPolygonsDataFrame object th.z &lt;- over(th, P, fn=mean) th.spdf &lt;- SpatialPolygonsDataFrame(th, th.z) # Finally, we&#39;ll clip the tessellated surface to the Texas boundaries th.clp &lt;- raster::intersect(W,th.spdf) # Map the data tm_shape(th.clp) + tm_polygons(col=&quot;Precip_in&quot;, palette=&quot;RdBu&quot;, auto.palette.mapping=FALSE, title=&quot;Predicted precipitation \\n(in inches)&quot;) + tm_legend(legend.outside=TRUE) Many packages share the same function names. This can be a problem when these packages are loaded in a same R session. For example, the intersect function is available in the base, spatstat and raster packages–all of which are loaded in this current session. To ensure that the proper function is selected, it’s a good idea to preface the function name with the package name as in raster::intersect(). This tip will be used in the next chunk of code when calling the idw function which is available in both spatstat and gstat. Note that the dirichlet function (like most functions in the spatsat package) require that the point object be in a ppp format hence the inline as.ppp(P) syntax. IDW The IDW output is a raster. This requires that we first create an empty raster grid, then interpolate the precipitation values to each unsampled grid cell. An IDW power value of 2 (idp=2.0) will be used. library(gstat) # Use gstat&#39;s idw routine library(sp) # Used for the spsample function # Create an empty grid where n is the total number of cells grd &lt;- as.data.frame(spsample(P, &quot;regular&quot;, n=50000)) names(grd) &lt;- c(&quot;X&quot;, &quot;Y&quot;) coordinates(grd) &lt;- c(&quot;X&quot;, &quot;Y&quot;) gridded(grd) &lt;- TRUE # Create SpatialPixel object fullgrid(grd) &lt;- TRUE # Create SpatialGrid object # Add P&#39;s projection information to the empty grid proj4string(grd) &lt;- proj4string(P) # Interpolate the grid cells using a power value of 2 (idp=2.0) P.idw &lt;- gstat::idw(Precip_in ~ 1, P, newdata=grd, idp=2.0) # Convert to raster object then clip to Texas r &lt;- raster(P.idw) r.m &lt;- mask(r, W) # Plot tm_shape(r.m) + tm_raster(n=10,palette = &quot;RdBu&quot;, auto.palette.mapping = FALSE, title=&quot;Predicted precipitation \\n(in inches)&quot;) + tm_shape(P) + tm_dots(size=0.2) + tm_legend(legend.outside=TRUE) Fine-tuning the interpolation The choice of power function can be subjective. To fine-tune the choice of the power parameter, you can perform a leave-one-out validation routine to measure the error in the interpolated values. # Leave-one-out validation routine IDW.out &lt;- vector(length = length(P)) for (i in 1:length(P)) { IDW.out[i] &lt;- idw(Precip_in ~ 1, P[-i,], P[i,], idp=2.0)$var1.pred } # Plot the differences OP &lt;- par(pty=&quot;s&quot;, mar=c(4,3,0,0)) plot(IDW.out ~ P$Precip_in, asp=1, xlab=&quot;Observed&quot;, ylab=&quot;Predicted&quot;, pch=16, col=rgb(0,0,0,0.5)) abline(lm(IDW.out ~ P$Precip_in), col=&quot;red&quot;, lw=2,lty=2) abline(0,1) par(OP) The RMSE can be computed from IDW.out as follows: # Compute RMSE sqrt( sum((IDW.out - P$Precip_in)^2) / length(P)) [1] 6.989294 Cross-validation In addition to generating an interpolated surface, you can create a 95% confidence interval map of the interpolation model. Here we’ll create a 95% CI map from an IDW interpolation that uses a power parameter of 2 (idp=2.0). # Implementation of a jackknife technique to estimate # a confidence interval at each unsampled point. # Create the interpolated surface img &lt;- gstat::idw(Precip_in~1, P, newdata=grd, idp=2.0) n &lt;- length(P) Zi &lt;- matrix(nrow = length(img$var1.pred), ncol = n) # Remove a point then interpolate (do this n times for each point) st &lt;- stack() for (i in 1:n){ Z1 &lt;- gstat::idw(Precip_in~1, P[-i,], newdata=grd, idp=2.0) st &lt;- addLayer(st,raster(Z1,layer=1)) # Calculated pseudo-value Z at j Zi[,i] &lt;- n * img$var1.pred - (n-1) * Z1$var1.pred } # Jackknife estimator of parameter Z at location j Zj &lt;- as.matrix(apply(Zi, 1, sum, na.rm=T) / n ) # Compute (Zi* - Zj)^2 c1 &lt;- apply(Zi,2,&#39;-&#39;,Zj) # Compute the difference c1 &lt;- apply(c1^2, 1, sum, na.rm=T ) # Sum the square of the difference # Compute the confidence interval CI &lt;- sqrt( 1/(n*(n-1)) * c1) # Create (CI / interpolated value) raster img.sig &lt;- img img.sig$v &lt;- CI /img$var1.pred # Clip the confidence raster to Texas r &lt;- raster(img.sig, layer=&quot;v&quot;) r.m &lt;- mask(r, W) # Plot the map tm_shape(r.m) + tm_raster(n=7,title=&quot;95% confidence interval \\n(in inches)&quot;) + tm_shape(P) + tm_dots(size=0.2) + tm_legend(legend.outside=TRUE) 1st order polynomial fit To fit a first order polynomial model of the form \\(precip = intercept + aX + bY\\) to the data, # Define the 1st order polynomial equation f.1 &lt;- as.formula(Precip_in ~ X + Y) # Add X and Y to P P$X &lt;- coordinates(P)[,1] P$Y &lt;- coordinates(P)[,2] # Run the regression model lm.1 &lt;- lm( f.1, data=P) # Use the regression model output to interpolate the surface dat.1st &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.1, newdata=grd))) # Clip the interpolated raster to Texas r &lt;- raster(dat.1st) r.m &lt;- mask(r, W) # Plot the map tm_shape(r.m) + tm_raster(n=10, palette=&quot;RdBu&quot;, auto.palette.mapping=FALSE, title=&quot;Predicted precipitation \\n(in inches)&quot;) + tm_shape(P) + tm_dots(size=0.2) + tm_legend(legend.outside=TRUE) 2nd order polynomial To fit a second order polynomial model of the form \\(precip = intercept + aX + bY + dX^2 + eY^2 +fXY\\) to the data, # Define the 2nd order polynomial equation f.2 &lt;- as.formula(Precip_in ~ X + Y + I(X*X)+I(Y*Y) + I(X*Y)) # Add X and Y to P P$X &lt;- coordinates(P)[,1] P$Y &lt;- coordinates(P)[,2] # Run the regression model lm.2 &lt;- lm( f.2, data=P) # Use the regression model output to interpolate the surface dat.2nd &lt;- SpatialGridDataFrame(grd, data.frame(var1.pred = predict(lm.2, newdata=grd))) # Clip the interpolated raster to Texas r &lt;- raster(dat.2nd) r.m &lt;- mask(r, W) # Plot the map tm_shape(r.m) + tm_raster(n=10, palette=&quot;RdBu&quot;, auto.palette.mapping=FALSE, title=&quot;Predicted precipitation \\n(in inches)&quot;) + tm_shape(P) + tm_dots(size=0.2) + tm_legend(legend.outside=TRUE) Kriging Fit the variogram model First, we need to create a variogram model. Note that the variogram model is computed on the de-trended data. This is implemented in the following chunk of code by passing the 1st order trend model (defined in an earlier code chunk as formula object f.1) to the variogram function. # Define the 1st order polynomial equation f.1 &lt;- as.formula(Precip_in ~ X + Y) # Compute the sample variogram; note that the f.1 trend model is one of the # parameters passed to variogram(). This tells the function to create the # variogram on the de-trended data. var.smpl &lt;- variogram(f.1, P, cloud = FALSE, cutoff=1000000, width=89900) # Compute the variogram model by passing the nugget, sill and range values # to fit.variogram() via the vgm() function. dat.fit &lt;- fit.variogram(var.smpl, fit.ranges = FALSE, fit.sills = FALSE, vgm(psill=14, model=&quot;Sph&quot;, range=590000, nugget=0)) # The following plot allows us to assess the fit plot(var.smpl, dat.fit, xlim=c(0,1000000)) Generate Kriged surface Next, use the variogram model dat.fit to generate a kriged interpolated surface. The krige function allows us to include the trend model thus saving us from having to de-trend the data, krige the residuals, then combine the two rasters. Instead, all we need to do is pass krige the trend formula f.1. # Define the trend model f.1 &lt;- as.formula(Precip_in ~ X + Y) # Perform the krige interpolation (note the use of the variogram model # created in the earlier step) dat.krg &lt;- krige( f.1, P, grd, dat.fit) # Convert kriged surface to a raster object for clipping r &lt;- raster(dat.krg) r.m &lt;- mask(r, W) # Plot the map tm_shape(r.m) + tm_raster(n=10, palette=&quot;RdBu&quot;, auto.palette.mapping=FALSE, title=&quot;Predicted precipitation \\n(in inches)&quot;) + tm_shape(P) + tm_dots(size=0.2) + tm_legend(legend.outside=TRUE) Generate the variance map The dat.krg object stores not just the interpolated values, but the variance values as well. These can be passed to the raster object for mapping as follows: r &lt;- raster(dat.krg, layer=&quot;var1.var&quot;) r.m &lt;- mask(r, W) tm_shape(r.m) + tm_raster(n=7, palette =&quot;Reds&quot;, title=&quot;Variance map \\n(in inches)&quot;) +tm_shape(P) + tm_dots(size=0.2) + tm_legend(legend.outside=TRUE) "],
["mapping-rates-in-r.html", "Mapping rates in R Raw Rates Standardized mortality ratios (relative risk). Dykes and Unwin’s chi-square statistic Unstable ratios", " Mapping rates in R The following libraries are used in the examples that follow. library(spdep) library(classInt) library(RColorBrewer) library(maptools) Next, we’ll initialize some color palettes. pal1 &lt;- brewer.pal(6,&quot;Greys&quot;) pal2 &lt;- brewer.pal(8,&quot;RdYlGn&quot;) pal3 &lt;- c(brewer.pal(9,&quot;Greys&quot;), &quot;#FF0000&quot;) The Auckland dataset from the spdep package will be used throughout this exercise. Some of the graphics that follow are R reproductions of Bailey and Gatrell’s book, Interactive Spatial Data Analysis (Bailey and Gatrell 1995). auckland &lt;- readShapePoly(system.file(&quot;etc/shapes/auckland.shp&quot;, package=&quot;spdep&quot;)[1]) The Auckland data represents total infant deaths (under the age of five) for Auckland, New Zealand, spanning the years 1977 through 1985 for different census area units. The following block of code maps these counts by area. Both equal interval and quantile classification schemes of the same data are mapped. brks1 &lt;- classIntervals(auckland$M77_85, n = 6, style = &quot;equal&quot;) brks2 &lt;- classIntervals(auckland$M77_85, n = 6, style = &quot;quantile&quot;) print(spplot(auckland, &quot;M77_85&quot;, at = brks1$brks, col.regions = pal1) ,position=c(0,0,.5,1),more=T) print(spplot(auckland, &quot;M77_85&quot;, at = brks2$brks, col.regions = pal1) ,position=c(0.5,0,1,1),more=T) These are examples of choropleth maps (choro = area and pleth = value) where some value (an enumeration of child deaths in this working example) is aggregated over a defined area (e.g. census area units) and displayed using different colors. Since the area units used to map death counts are not uniform in shape and area across Auckland, there is a tendency to assign more “visual weight” to polygons having larger areas than those having smaller areas. In our example, census units in the southern end of Auckland appear to have an “abnormally” large infant death count. Another perceptual interpretation of the map is one that flags those southern units as being “problematic” or of “great concern”. However, as we shall see in the following sections, this perception may not reflect reality. We therefore seek to produce perceptually tenable maps. Dykes and Unwin (Dykes and Unwin 2001) define a similar concept called map stability which seeks to produce maps that convey real effects. Raw Rates A popular approach for correcting for biased visual weights (due, for instance, to different unit area sizes) is to normalize the count data by area thus giving a count per unit area. Though this may make sense for population count data, it does not make a whole lot sense when applied to mortality counts; we are usually interested in the number of deaths per population count and not in the number of deaths per unit area. In the next chunk of code we extract population count under the age of 5 from the Auckland data set and assign this value to the variable pop. Likewise, we extract the under 5 mortality count and assign this value to the variable mor. Bear in mind that the mortality count spans a 9 year period. Since mortality rates are usually presented in rates per year, we need to multiply the population value (which is for the year 1981) by nine. This will be important in the subsequent code when we compute mortality rates. pop &lt;- auckland$Und5_81 * 9 mor &lt;- auckland$M77_85 Next, we will compute the raw rates (infant deaths per 1000 individuals per year) and map this rate by census unit area. Both quantile and equal interval classification schemes of the same data are mapped. auckland$raw.rate &lt;- mor / pop * 1000 brks1 &lt;- classIntervals(auckland$raw.rate, n = 6, style = &quot;equal&quot;) brks2 &lt;- classIntervals(auckland$raw.rate, n = 6, style = &quot;quantile&quot;) print(spplot(auckland, &quot;raw.rate&quot;, at = brks1$brks, col.regions = pal1) ,position=c(0,0,.5,1),more=T) print(spplot(auckland, &quot;raw.rate&quot;, at = brks2$brks, col.regions = pal1) ,position=c(0.5,0,1,1),more=T) Note how our perception of the distribution of infant deaths changes when looking at mapped raw rates vs. counts. A north-south trend in perceived “abnormal” infant deaths is no longer apparent in this map. Standardized mortality ratios (relative risk). Another way to re-express the data is to map the Standardized Mortality Ratios (SMR)-a very popular form of representation in the field of epidemiology. Such maps map the ratios of the number of deaths to an expected death count. There are many ways to define an expected death count, many of which can be externally specified. In the following example, the expected death count \\(E_i\\) is estimated by multiplying the under 5 population count for each area by the overall death rate for Auckland: \\[E_i = \\frac{n_i}{mortality_{Auckland} } \\] where \\(n_i\\) is the population count within census unit area \\(i\\) and \\(mortality_{Auckland}\\) is the overall death rate computed from \\(mortality_{Auckland} = \\sum_{i=1}^j O_i / \\sum_{i=1}^j n_i\\) where \\(O_i\\) is the observed death count for census unit \\(i\\). This chunk of code replicates Bailey and Gatrell’s figure 8.1 with the one exception that the color scheme is reversed (Bailey and Gatrell assign lighter hues to higher numbers). auck.rate &lt;- sum(mor) / sum(pop) mor.exp &lt;- pop * auck.rate # Expected count over a nine year period auckland$rel.rate &lt;- 100 * mor / mor.exp brks &lt;- classIntervals(auckland$rel.rate, n = 6, style = &quot;fixed&quot;, fixedBreaks = c(0,47, 83, 118, 154, 190, 704)) spplot(auckland, &quot;rel.rate&quot;, at = brks$brks,col.regions=pal1) Dykes and Unwin’s chi-square statistic Dykes and Unwin (Dykes and Unwin 2001) propose a similar technique whereby the rates are standardized following: \\[\\frac{O_i - E_i}{\\sqrt{E_i}} \\] This has the effect of creating a distribution of values closer to normal (as opposed to a Poisson distribution of rates and counts encountered thus far). We can therefore apply a diverging color scheme where green hues represent less than expected rates and red hues represent greater than expected rates. auckland$chi.squ = (mor - mor.exp) / sqrt(mor.exp) brks &lt;- classIntervals(auckland$chi.squ, n = 6, style = &quot;fixed&quot;, fixedBreaks = c(-5,-3, -1, -2, 0, 1, 2, 3, 5)) spplot(auckland, &quot;chi.squ&quot;, at = brks$brks,col.regions=rev(pal2)) Unstable ratios One problem with the various techniques used thus far is their sensitivity (hence instability) to small underlying population counts (i.e. unstable ratios). This next chunk of code maps the under 5 population count by census area unit. brks &lt;- classIntervals(auckland$Und5_81, n = 6, style = &quot;equal&quot;) spplot(auckland, &quot;Und5_81&quot;, at = brks$brks,col.regions=pal1) Note the variability in population count with some areas encompassing fewer than 50 infants. If there is just one death in that census unit, the death rate would be reported as \\(1/50 * 1000\\) or 20 per thousand infants–far more than then the 2.63 per thousand rate for our Auckland data set. Interestingly, the three highest raw rates in Auckland (14.2450142, 18.5185185, 10.5820106 deaths per 1000) are associated with some of the smallest underlying population counts (39, 6, 21 infants under 5). One approach to circumventing this issue is to generate a probability map of the data. The next section highlights such an example. Global Empirical Bayes (EB) rate estimate The idea behind Bayesian approach is to compare the value in some area \\(i\\) to some a priori estimate of the value and to “stabilize” the values due to unstable ratios (e.g. where area populations are small). The a priori estimate can be based on some global mean. An example of the use on a global EB infant mortality rate map is shown below. The EB map is shown side-by-side with the raw rates map for comparison. aka Global moment estimator of infant mortality per 1000 per year EB.est &lt;- EBest(auckland$M77_85, auckland$Und5_81 * 9 ) auckland$EBest &lt;- EB.est$estmm * 1000 brks1 &lt;- classIntervals(auckland$EBest, n = 10, style = &quot;quantile&quot;) brks2 &lt;- classIntervals(auckland$raw.rate, n = 10, style = &quot;quantile&quot;) print(spplot(auckland, &quot;EBest&quot;, at = brks1$brks, col.regions = pal3, main=&quot;EB rates&quot;) ,position=c(0,0,.5,1),more=T) print(spplot(auckland, &quot;raw.rate&quot;, at = brks2$brks, col.regions = pal3, main=&quot;Raw Rates&quot;) ,position=c(0.5,0,1,1),more=T) The census units with the top 10% rates are highlighted in red. Unstable rates (i.e. those associated with smaller population counts) are assigned lower weights to reduce their “prominence” in the mapped data. Notice how the three high raw rates highlighted in the last section are reduced from 14.2450142, 18.5185185, 10.5820106 counts per thousand to 3.6610133, 2.8672132, 3.0283279 counts per thousand. The “remapping” of these values along with others can be shown on the following plot: Local Empirical Bayes (EB) rate estimate The a priori mean and variance need not be aspatial (i.e. the prior distribution being the same for the entire Auckland study area). The adjusted estimated rates can be shrunk towards a local mean instead. Such technique is referred to as local empirical Bayes rate estimates. In the following example, we define local as consisting of all first order adjacent census unit areas. nb &lt;- poly2nb(auckland) EBL.est &lt;- EBlocal(auckland$M77_85, 9*auckland$Und5_81, nb) auckland$EBLest &lt;- EBL.est$est * 1000 brks1 &lt;- classIntervals(auckland$EBLest, n = 10, style = &quot;quantile&quot;) brks2 &lt;- classIntervals(auckland$raw.rate, n = 10, style = &quot;quantile&quot;) print(spplot(auckland, &quot;EBLest&quot;, at = brks1$brks, col.regions = pal3, main=&quot;Local EB rates&quot;) ,position=c(0,0,.5,1),more=T) print(spplot(auckland, &quot;raw.rate&quot;, at = brks2$brks, col.regions = pal3, main=&quot;Raw Rates&quot;) ,position=c(0.5,0,1,1),more=T) The census units with the top 10% rates are highlighted in red. References "],
["references.html", "References", " References "]
]
